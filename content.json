{"meta":{"title":"Fish","subtitle":"Fishの博客","description":"书中自有黄金屋","author":"Fish","url":"http://example.com","root":"/"},"pages":[{"title":"友情链接","date":"2018-06-07T14:17:49.000Z","updated":"2021-07-27T07:31:12.000Z","comments":true,"path":"link/index.html","permalink":"http://example.com/link/index.html","excerpt":"","text":""},{"title":"关于本站","date":"2020-04-19T04:58:56.000Z","updated":"2021-08-02T02:38:16.000Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"仙气の云彩“ 认识自我、超越自我 “ 那么你离成功就又进了一步！ 当你在项目中感觉所要学习的人和事越来越多时，说明你在 成长 。 当你感觉要责怪的人和事越来越少时，说明你在 成熟 。 当你在项目中不断获得了友谊和朋友时，说明你将取得项目的 成功 。 联系我B站：仙气のxianqi（萌新up） QQ：2287853439 （嘀嘀~，麻烦让一下，我要开车了！） 座右铭：既然选择了远方 便只顾风雨兼程"},{"title":"分类","date":"2020-11-24T07:12:19.000Z","updated":"2021-07-26T14:44:30.000Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"我的歌单","date":"2019-05-17T08:14:00.000Z","updated":"2021-07-26T13:06:54.000Z","comments":true,"path":"music/index.html","permalink":"http://example.com/music/index.html","excerpt":"","text":""},{"title":"留言板","date":"2020-10-31T02:11:28.000Z","updated":"2023-05-29T15:23:24.000Z","comments":true,"path":"comments/index.html","permalink":"http://example.com/comments/index.html","excerpt":"","text":""},{"title":"archives","date":"2019-10-24T16:00:00.000Z","updated":"2021-06-27T12:40:26.000Z","comments":true,"path":"archives/index.html","permalink":"http://example.com/archives/index.html","excerpt":"","text":""},{"title":"相册","date":"2023-07-10T00:59:11.839Z","updated":"2021-10-14T02:13:26.000Z","comments":false,"path":"List/gallery/index.html","permalink":"http://example.com/List/gallery/index.html","excerpt":"","text":"壁紙 世俗的欲望 OH MY GIRL 關於OH MY GIRL的圖片"},{"title":"标签","date":"2020-11-24T07:14:39.000Z","updated":"2021-07-26T14:44:44.000Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-10T08:41:10.000Z","updated":"2021-07-26T07:32:30.000Z","comments":false,"path":"List/movies/index.html","permalink":"http://example.com/List/movies/index.html","excerpt":"","text":"励志视频"},{"title":"Music-BBOX","date":"2020-04-23T04:58:56.000Z","updated":"2021-07-26T07:32:22.000Z","comments":false,"path":"List/music/index.html","permalink":"http://example.com/List/music/index.html","excerpt":"","text":""},{"title":"","date":"2021-09-28T14:56:14.000Z","updated":"2021-09-28T16:44:44.000Z","comments":false,"path":"List/gallery/ohmygirl/index.html","permalink":"http://example.com/List/gallery/ohmygirl/index.html","excerpt":"","text":""},{"title":"","date":"2021-09-28T14:56:13.000Z","updated":"2021-10-14T02:11:36.000Z","comments":false,"path":"List/gallery/wallpaper/index.html","permalink":"http://example.com/List/gallery/wallpaper/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQL基础","slug":"32-MySQL基础","date":"2023-09-11T06:59:03.000Z","updated":"2023-09-11T10:03:01.995Z","comments":true,"path":"posts/32.html","link":"","permalink":"http://example.com/posts/32.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 1.什么是BufferPool？Buffer Pool基本概念 Buffer Pool：缓冲池，简称BP。其作用是用来缓存表数据与索引数据，减少磁盘IO操作，提升效率。 Buffer Pool由缓存数据页(Page) 和 对缓存数据页进行描述的控制块 组成, 控制块中存储着对应缓存页的所属的 表空间、数据页的编号、以及对应缓存页在Buffer Pool中的地址等信息. Buffer Pool默认大小是128M, 以Page页为单位，Page页默认大小16K，而控制块的大小约为数据页的5%，大 概是800字节。 注: Buffer Pool大小为128M指的就是缓存页的大小，控制块则一般占5%，所以每次会多申请6M的内存空间用于存放控制块 如何判断一个页是否在BufferPool中缓存 ? MySQl中有一个哈希表数据结构，它使用表空间号+数据页号，作为一个key，然后缓冲页对应的控制块作为value。 当需要访问某个页的数据时，先从哈希表中根据表空间号+页号看看是否存在对应的缓冲页。 如果有，则直接使用；如果没有，就从free链表中选出一个空闲的缓冲页，然后把磁盘中对应的页加载到该缓冲页的位置 2.InnoDB如何管理Page页？Page页分类 BP的底层采用链表数据结构管理Page。在InnoDB访问表记录和索引时会在Page页中缓存，以后使用可以减少磁盘IO操作，提升效率。 Page根据状态可以分为三种类型： free page ： 空闲page，未被使用 clean page：被使用page，数据没有被修改过 dirty page：脏页，被使用page，数据被修改过，Page页中数据和磁盘的数据产生了不一致 Page页如何管理 针对上面所说的三种page类型，InnoDB通过三种链表结构来维护和管理 free list：表示空闲缓冲区，管理free page free链表是把所有空闲的缓冲页对应的控制块作为一个个的节点放到一个链表中，这个链表便称之为free链表 基节点: free链表中只有一个基节点是不记录缓存页信息(单独申请空间)，它里面就存放了free链表的头节点的地址，尾节点的地址，还有free链表里当前有多少个节点。 2.flush list： 表示需要刷新到磁盘的缓冲区，管理dirty page，内部page按修改时间排序。 InnoDB引擎为了提高处理效率，在每次修改缓冲页后，并不是立刻把修改刷新到磁盘上，而是在未来的某个时间点进行刷新操作. 所以需要使用到flush链表存储脏页，凡是被修改过的缓冲页对应的控制块都会作为节点加入到flush链表. flush链表的结构与free链表的结构相似 3.lru list：表示正在使用的缓冲区，管理clean page和dirty page，缓冲区以midpoint为基点，前面链表称为new列表区，存放经常访问的数据，占63%；后面的链表称为old列表区，存放使用较少数据，占37% 3.为什么写缓冲区，仅适用于非唯一普通索引页？change Buffer基本概念 Change Buffer：写缓冲区,是针对二级索引(辅助索引) 页的更新优化措施。 作用: 在进行DML操作时，如果请求的辅助索引（二级索引）没有在缓冲池中时，并不会立刻将磁盘页加载到缓冲池，而是在CB记录缓冲变更，等未来数据被读取时，再将数据合并恢复到BP中。 ChangeBuffer用于存储SQL变更操作，比如Insert/Update/Delete等SQL语句 ChangeBuffer中的每个变更操作都有其对应的数据页，并且该数据页未加载到缓存中； 当ChangeBuffer中变更操作对应的数据页加载到缓存中后，InnoDB会把变更操作Merge到数据页上； InnoDB会定期加载ChangeBuffer中操作对应的数据页到缓存中，并Merge变更操作； change buffer更新流程 写缓冲区，仅适用于非唯一普通索引页，为什么？ 如果在索引设置唯一性，在进行修改时，InnoDB必须要做唯一性校验，因此必须查询磁盘，做一次IO操 作。会直接将记录查询到BufferPool中，然后在缓冲池修改，不会在ChangeBuffer操作。 4.MySQL为什么改进LRU算法？普通LRU算法 LRU = Least Recently Used（最近最少使用）: 就是末尾淘汰法，新数据从链表头部加入，释放空间时从末尾淘汰. 当要访问某个页时，如果不在Buffer Pool，需要把该页加载到缓冲池,并且把该缓冲页对应的控制块作为节点添加到LRU链表的头部。 当要访问某个页时，如果在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部 当需要释放空间时,从最末尾淘汰 普通LRU链表的优缺点 优点 所有最近使用的数据都在链表表头，最近未使用的数据都在链表表尾,保证热数据能最快被获取到。 缺点 如果发生全表扫描（比如：没有建立合适的索引 or 查询时使用select * 等），则有很大可能将真正的热数据淘汰掉. 由于MySQL中存在预读机制，很多预读的页都会被放到LRU链表的表头。如果这些预读的页都没有用到的话，这样，会导致很多尾部的缓冲页很快就会被淘汰。 改进型LRU算法 改进型LRU：将链表分为new和old两个部分，加入元素时并不是从表头插入，而是从中间midpoint位置插入(就是说从磁盘中新读出的数据会放在冷数据区的头部)，如果数据很快被访问，那么page就会向new列表头部移动，如果数据没有被访问，会逐步向old尾部移动，等待淘汰。 冷数据区的数据页什么时候会被转到到热数据区呢 ? 如果该数据页在LRU链表中存在时间超过1s，就将其移动到链表头部 ( 链表指的是整个LRU链表) 如果该数据页在LRU链表中存在的时间短于1s，其位置不变(由于全表扫描有一个特点，就是它对某个页的频繁访问总耗时会很短) 1s这个时间是由参数 innodb_old_blocks_time 控制的 5.使用索引一定可以提升效率吗？索引就是排好序的,帮助我们进行快速查找的数据结构. 简单来讲，索引就是一种将数据库中的记录按照特殊形式存储的数据结构。通过索引，能够显著地提高数据查询的效率，从而提升服务器的性能. 索引的优势与劣势 优点 提高数据检索的效率,降低数据库的IO成本 通过索引列对数据进行排序,降低数据排序的成本,降低了CPU的消耗 缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，降低了数据的维护速度 创建索引的原则 在经常需要搜索的列上创建索引，可以加快搜索的速度； 在作为主键的列上创建索引，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 6.介绍一下Page页的结构？Page是整个InnoDB存储的最基本构件，也是InnoDB磁盘管理的最小单位，与数据库相关的所有内容都存储在这种Page结构里。 Page分为几种类型，常见的页类型有数据页（B+tree Node）Undo页（Undo Log Page）系统页（System Page） 事务数据页（Transaction System Page）等 Page 各部分说明 名称 占用大小 说明 File Header 38字节 文件头, 描述页信息 Page Header 56字节 页头,页的状态 Infimum + Supremum 26字节 最大和最小记录,这是两个虚拟的行记录 User Records 不确定 用户记录,存储数据行记录 Free Space 不确定 空闲空间,页中还没有被使用的空间 Page Directory 不确定 页目录,存储用户记录的相对位置 File Trailer 8字节 文件尾,校验页是否完整 File Header 字段用于记录 Page 的头信息，其中比较重要的是 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 字段，通过这两个字段，我们可以找到该页的上一页和下一页，实际上所有页通过两个字段可以形成一条双向链表 Page Header 字段用于记录 Page 的状态信息。 Infimum 和 Supremum 是两个伪行记录，Infimum（下确界）记录比该页中任何主键值都要小的值，Supremum （上确界）记录比该页中任何主键值都要大的值，这个伪记录分别构成了页中记录的边界。 User Records 中存放的是实际的数据行记录 Free Space 中存放的是空闲空间，被删除的行记录会被记录成空闲空间 Page Directory 记录着与二叉查找相关的信息 File Trailer 存储用于检测数据完整性的校验和等数据。 页结构整体上可以分为三大部分，分别为通用部分(文件头、文件尾)、存储记录空间、索引部分。 通用部分 (File Header&amp;File Trailer ) 通用部分 : 主要指文件头和文件尾，将页的内容进行封装，通过文件头和文件尾校验的CheckSum方式来确保页的传输是完整的。 其中比较重要的是在文件头中的 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 字段，通过这两个字段，我们可以找到该页的上一页和下一页，实际上所有页通过两个字段可以形成一条双向链表 记录部分(User Records&amp;Free Space) 页的主要作用是存储记录，所以“最小和最大记录”和“用户记录”部分占了页结构的主要空间。另外空闲空间是个灵活的部分，当有新的记录插入时，会从空闲空间中进行分配用于存储新记录 3)数据目录部分 (Page Directory) 数据页中行记录按照主键值由小到大顺序串联成一个单链表(页中记录是以单向链表的形式进行存储的)，且单链表的链表头为最小记录，链表尾为最大记录。并且为了更快速地定位到指定的行记录，通过 Page Directory实现目录的功能，借助 Page Directory使用二分法快速找到需要查找的行记录。 7.说一下聚簇索引与非聚簇索引？聚集索引与非聚集索引的区别是：叶节点是否存放一整行记录 聚簇索引: 将数据存储与索引放到了一块,索引结构的叶子节点保存了行数据. 非聚簇索引：将数据与索引分开存储，索引结构的叶子节点指向了数据对应的位置. InnoDB 主键使用的是聚簇索引，MyISAM 不管是主键索引，还是二级索引使用的都是非聚簇索引。 在InnoDB引擎中，主键索引采用的就是聚簇索引结构存储。 聚簇索引（聚集索引） 聚簇索引是一种数据存储方式，InnoDB的聚簇索引就是按照主键顺序构建 B+Tree结构。B+Tree 的叶子节点就是行记录，行记录和主键值紧凑地存储在一起。 这也意味着 InnoDB 的主键索引就是数据表本身，它按主键顺序存放了整张表的数据，占用的空间就是整个表数据量的大小。通常说的主键索引就是聚集索引。 InnoDB的表要求必须要有聚簇索引： 如果表定义了主键，则主键索引就是聚簇索引 如果表没有定义主键，则第一个非空unique列作为聚簇索引 否则InnoDB会从建一个隐藏的row-id作为聚簇索引 辅助索引InnoDB辅助索引，也叫作二级索引，是根据索引列构建 B+Tree结构。但在 B+Tree 的叶子节点中只存了索引列和主键的信息。二级索引占用的空间会比聚簇索引小很多， 通常创建辅助索引就是为了提升查询效率。一个表InnoDB只能创建一个聚簇索引，但可以创建多个辅助索引。 非聚簇索引 与InnoDB表存储不同，MyISM使用的是非聚簇索引， 非聚簇索引的两棵B+树看上去没什么不同 ，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。 表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于 索引树是独立的，通过辅助键检索无需访问主键的索引树 。 聚簇索引的优点 当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。 当通过聚簇索引查找目标数据时理论上比非聚簇索引要快，因为非聚簇索引定位到对应主键时还要多一次目标记录寻址,即多一次I/O。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 聚簇索引的缺点 插入速度严重依赖于插入顺序 。 更新主键的代价很高，因为将会导致被更新的行移动 。 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。 8.索引有哪几种类型？1）普通索引 这是最基本的索引类型，基于普通字段建立的索引，没有任何限制。 CREATE INDEX &lt;索引的名字&gt; ON tablename (字段名); ALTER TABLE tablename ADD INDEX [索引的名字] (字段名); CREATE TABLE tablename ( [...], INDEX [索引的名字] (字段名) ); 2）唯一索引 与”普通索引”类似，不同的就是：索引字段的值必须唯一，但允许有空值 。 CREATE UNIQUE INDEX &lt;索引的名字&gt; ON tablename (字段名); ALTER TABLE tablename ADD UNIQUE INDEX [索引的名字] (字段名); CREATE TABLE tablename ( [...], UNIQUE [索引的名字] (字段名) ; 3）主键索引 它是一种特殊的唯一索引，不允许有空值。在创建或修改表时追加主键约束即可，每个表只能有一个主键。 CREATE TABLE tablename ( [...], PRIMARY KEY (字段名) ); ALTER TABLE tablename ADD PRIMARY KEY (字段名); 4）复合索引 用户可以在多个列上建立索引，这种索引叫做组复合索引（组合索引）。复合索引可以代替多个单一索引，相比多个单一索引复合索引所需的开销更小。 CREATE INDEX &lt;索引的名字&gt; ON tablename (字段名1，字段名2...); ALTER TABLE tablename ADD INDEX [索引的名字] (字段名1，字段名2...); CREATE TABLE tablename ( [...], INDEX [索引的名字] (字段名1，字段名2...) ); 复合索引使用注意事项： 何时使用复合索引，要根据where条件建索引，注意不要过多使用索引，过多使用会对更新操作效率有很大影响。 如果表已经建立了(col1，col2)，就没有必要再单独建立（col1）；如果现在有(col1)索引，如果查询需要col1和col2条件，可以建立(col1,col2)复合索引，对于查询有一定提高。 5) 全文索引 查询操作在数据量比较少时，可以使用like模糊查询，但是对于大量的文本数据检索，效率很低。如果使用全文索引，查询速度会比like快很多倍。 在MySQL 5.6 以前的版本，只有MyISAM存储引擎支持全文索引，从MySQL 5.6开始MyISAM和InnoDB存储引擎均支持。 CREATE FULLTEXT INDEX &lt;索引的名字&gt; ON tablename (字段名); ALTER TABLE tablename ADD FULLTEXT [索引的名字] (字段名); CREATE TABLE tablename ( [...], FULLTEXT KEY [索引的名字] (字段名) ; 全文索引方式有自然语言检索 IN NATURAL LANGUAGE MODE和布尔检索 IN BOOLEAN MODE两种 和常用的like模糊查询不同，全文索引有自己的语法格式，使用 match 和 against 关键字，比如 SELECT * FROM users3 WHERE MATCH(NAME) AGAINST('aabb'); -- * 表示通配符,只能在词的后面 SELECT * FROM users3 WHERE MATCH(NAME) AGAINST('aa*' &nbsp;IN BOOLEAN MODE); 全文索引使用注意事项： 全文索引必须在字符串、文本字段上建立。 全文索引字段值必须在最小字符和最大字符之间的才会有效。（innodb：3-84；myisam：4-84） 9.介绍一下最佳左前缀法则？1)最佳左前缀法则 最佳左前缀法则: 如果创建的是联合索引,就要遵循该法则. 使用索引时，where后面的条件需要从索引的最左前列开始使用,并且不能跳过索引中的列使用。 场景1: 按照索引字段顺序使用，三个字段都使用了索引,没有问题。 EXPLAIN SELECT * FROM users WHERE user_name = 'tom' AND user_age = 17 AND user_level = 'A'; 场景2: 直接跳过user_name使用索引字段，索引无效，未使用到索引。 EXPLAIN SELECT * FROM users WHERE user_age = 17 AND user_level = 'A'; 场景3: 不按照创建联合索引的顺序,使用索引 EXPLAIN SELECT * FROM users WHERE user_age = 17 AND user_name = 'tom' AND user_level = 'A'; where后面查询条件顺序是 user_age、user_level、user_name与我们创建的索引顺序 user_name、user_age、user_level不一致，为什么还是使用了索引，原因是因为MySql底层优化器对其进行了优化。 最佳左前缀底层原理MySQL创建联合索引的规则是: 首先会对联合索引最左边的字段进行排序( 例子中是 user_name ), 在第一个字段的基础之上 再对第二个字段进行排序 ( 例子中是 user_age ) . 最佳左前缀原则其实是和B+树的结构有关系, 最左字段肯定是有序的, 第二个字段则是无序的(联合索引的排序方式是: 先按照第一个字段进行排序,如果第一个字段相等再根据第二个字段排序). 所以如果直接使用第二个字段 user_age 通常是使用不到索引的. 10.什么是索引下推？索引下推（index condition pushdown ）简称ICP，在Mysql5.6的版本上推出，用于优化查询。 需求: 查询users表中 “名字第一个字是张，年龄为10岁的所有记录”。 SELECT * FROM users WHERE user_name LIKE '张%' AND user_age = 10; 根据最左前缀法则，该语句在搜索索引树的时候，只能匹配到名字第一个字是‘张’的记录，接下来是怎么处理的呢？当然就是从该记录开始，逐个回表，到主键索引上找出相应的记录，再比对 age 这个字段的值是否符合。 图1: 在 (name,age) 索引里面特意去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次 MySQL 5.6引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数。 图2: InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过,减少回表次数. 总结 如果没有索引下推优化（或称ICP优化），当进行索引查询时，首先根据索引来查找记录，然后再根据where条件来过滤记录； 在支持ICP优化后，MySQL会在取出索引的同时，判断是否可以进行where条件过滤再进行索引查询，也就是说提前执行where的部分过滤操作，在某些场景下，可以大大减少回表次数，从而提升整体性能。 11.什么是自适应哈希索引？自适应Hash索引（Adatptive Hash Index，内部简称AHI）是InnoDB的三大特性之一，还有两个是 Buffer Pool简称BP、双写缓冲区（Doublewrite Buffer）。 1、自适应即我们不需要自己处理，当InnoDB引擎根据查询统计发现某一查询满足hash索引的数据结构特点，就会给其建立一个hash索引； 2、hash索引底层的数据结构是散列表（Hash表），其数据特点就是比较适合在内存中使用，自适应Hash索引存在于InnoDB架构中的缓存中（不存在于磁盘架构中），见下面的InnoDB架构图。 3、自适应hash索引只适合搜索等值的查询，如select * from table where index_col=’xxx’，而对于其他查找类型，如范围查找，是不能使用的； Adaptive Hash Index是针对B+树Search Path的优化，因此所有会涉及到Search Path的操作，均可使用此Hash索引进行优化. 根据索引键值(前缀)快速定位到叶子节点满足条件记录的Offset，减少了B+树Search Path的代价，将B+树从Root节点至Leaf节点的路径定位，优化为Hash Index的快速查询。 InnoDB的自适应Hash索引是默认开启的，可以通过配置下面的参数设置进行关闭。 innodb_adaptive_hash_index = off 自适应Hash索引使用分片进行实现的，分片数可以使用配置参数设置： innodb_adaptive_hash_index_parts = 8 12.为什么LIKE以%开头索引会失效？like查询为范围查询，%出现在左边，则索引失效。%出现在右边索引未失效. 场景1: 两边都有% 或者 字段左边有%,索引都会失效。 EXPLAIN SELECT * FROM users WHERE user_name LIKE '%tom%'; EXPLAIN SELECT * FROM users WHERE user_name LIKE '%tom'; 场景2: 字段右边有%,索引生效 EXPLAIN SELECT * FROM users WHERE user_name LIKE 'tom%'; 解决%出现在左边索引失效的方法，使用覆盖索引 EXPLAIN SELECT user_name FROM users WHERE user_name LIKE '%jack%'; EXPLAIN SELECT user_name,user_age,user_level FROM users WHERE user_name LIKE '%jack%'; 对比场景1可以知道, 通过使用覆盖索引 type = index,并且 extra = Using index,从全表扫描变成了全索引扫描. like 失效的原因 %号在右: 由于B+树的索引顺序，是按照首字母的大小进行排序，%号在右的匹配又是匹配首字母。所以可以在B+树上进行有序的查找，查找首字母符合要求的数据。所以有些时候可以用到索引. %号在左: 是匹配字符串尾部的数据，我们上面说了排序规则，尾部的字母是没有顺序的，所以不能按照索引顺序查询，就用不到索引. 两个%%号: 这个是查询任意位置的字母满足条件即可，只有首字母是进行索引排序的，其他位置的字母都是相对无序的，所以查找任意位置的字母是用不上索引的. 13.自增还是UUID？数据库主键的类型该如何选择？auto_increment的优点： 字段长度较uuid小很多，可以是bigint甚至是int类型，这对检索的性能会有所影响。 在写的方面，因为是自增的，所以主键是趋势自增的，也就是说新增的数据永远在后面，这点对于性能有很大的提升。 数据库自动编号，速度快，而且是增量增长，按顺序存放，对于检索非常有利。 数字型，占用空间小，易排序，在程序中传递也方便。 auto_increment的缺点： 由于是自增，很容易通过网络爬虫知晓当前系统的业务量。 高并发的情况下，竞争自增锁会降低数据库的吞吐能力。 数据迁移或分库分表场景下，自增方式不再适用。 UUID的优点： 不会冲突。进行数据拆分、合并存储的时候，能保证主键全局的唯一性 可以在应用层生成，提高数据库吞吐能力 UUID的缺点： 影响插入速度， 并且造成硬盘使用率低。与自增相比，最大的缺陷就是随机io，下面我们会去具体解释 字符串类型相比整数类型肯定更消耗空间，而且会比整数类型操作慢。 uuid 和自增 id 的索引结构对比 1、使用自增 id 的内部结构 自增的主键的值是顺序的，所以 InnoDB 把每一条记录都存储在一条记录的后面。 当达到页面的最大填充因子时候（InnoDB 默认的最大填充因子是页大小的 15/16，会留出 1/16 的空间留作以后的修改）。 下一条记录就会写入新的页中，一旦数据按照这种顺序的方式加载，主键页就会近乎于顺序的记录填满，提升了页面的最大填充率，不会有页的浪费。 新插入的行一定会在原有的最大数据行下一行，MySQL 定位和寻址很快，不会为计算新行的位置而做出额外的消耗。减少了页分裂和碎片的产生。 2、使用 uuid 的索引内部结构 插入UUID： 新的记录可能会插入之前记录的中间，因此需要移动之前的记录 被写满已经刷新到磁盘上的页可能会被重新读取 因为 uuid 相对顺序的自增 id 来说是毫无规律可言的，新行的值不一定要比之前的主键的值要大，所以 innodb 无法做到总是把新行插入到索引的最后，而是需要为新行寻找新的合适的位置从而来分配新的空间。 这个过程需要做很多额外的操作，数据的毫无顺序会导致数据分布散乱，将会导致以下的问题： 写入的目标页很可能已经刷新到磁盘上并且从缓存上移除，或者还没有被加载到缓存中，innodb 在插入之前不得不先找到并从磁盘读取目标页到内存中，这将导致大量的随机 IO。 因为写入是乱序的，innodb 不得不频繁的做页分裂操作，以便为新的行分配空间，页分裂导致移动大量的数据，一次插入最少需要修改三个页以上。 由于频繁的页分裂，页会变得稀疏并被不规则的填充，最终会导致数据会有碎片。 在把随机值（uuid 和雪花 id）载入到聚簇索引（InnoDB 默认的索引类型）以后，有时候会需要做一次 OPTIMEIZE TABLE 来重建表并优化页的填充，这将又需要一定的时间消耗。 结论：使用 InnoDB 应该尽可能的按主键的自增顺序插入，并且尽可能使用单调的增加的聚簇键的值来插入新行。如果是分库分表场景下，分布式主键ID的生成方案 优先选择雪花算法生成全局唯一主键（雪花算法生成的主键在一定程度上是有序的）。 14.InnoDB与MyISAM的区别？InnoDB和MyISAM是使用MySQL时最常用的两种引擎类型，我们重点来看下两者区别。 事务和外键InnoDB支持事务和外键，具有安全性和完整性，适合大量insert或update操作MyISAM不支持事务和外键，它提供高速存储和检索，适合大量的select查询操作 锁机制InnoDB支持行级锁，锁定指定记录。基于索引来加锁实现。MyISAM支持表级锁，锁定整张表。 索引结构InnoDB使用聚集索引（聚簇索引），索引和记录在一起存储，既缓存索引，也缓存记录。MyISAM使用非聚集索引（非聚簇索引），索引和记录分开。 并发处理能力MyISAM使用表锁，会导致写操作并发率低，读之间并不阻塞，读写阻塞。InnoDB读写阻塞可以与隔离级别有关，可以采用多版本并发控制（MVCC）来支持高并发 存储文件InnoDB表对应两个文件，一个.frm表结构文件，一个.ibd数据文件。InnoDB表最大支持64TB；MyISAM表对应三个文件，一个.frm表结构文件，一个MYD表数据文件，一个.MYI索引文件。从MySQL5.0开始默认限制是256TB。 MyISAM 适用场景 不需要事务支持（不支持） 并发相对较低（锁定机制问题） 数据修改相对较少，以读为主 数据一致性要求不高 InnoDB 适用场景 需要事务支持（具有较好的事务特性） 行级锁定对高并发有很好的适应能力 数据更新较为频繁的场景 数据一致性要求较高 硬件设备内存较大，可以利用InnoDB较好的缓存能力来提高内存利用率，减少磁盘IO 两种引擎该如何选择？ 是否需要事务？有，InnoDB 是否存在并发修改？有，InnoDB 是否追求快速查询，且数据修改少？是，MyISAM 在绝大多数情况下，推荐使用InnoDB 扩展资料：各个存储引擎特性对比 15.B树和B+树的区别是什么？1）B-Tree介绍 B-Tree是一种平衡的多路查找树,B树允许一个节点存放多个数据. 这样可以在尽可能减少树的深度的同时,存放更多的数据(把瘦高的树变的矮胖). B-Tree中所有节点的子树个数的最大值称为B-Tree的阶,用m表示.一颗m阶的B树,如果不为空,就必须满足以下条件. m阶的B-Tree满足以下条件: 每个节点最多拥有m-1个关键字(根节点除外),也就是m个子树 根节点至少有两个子树(可以没有子树,有就必须是两个) 分支节点至少有(m/2)颗子树 (除去根节点和叶子节点其他都是分支节点) 所有叶子节点都在同一层,并且以升序排序 什么是B-Tree的阶 ?所有节点中，节点【60,70,90】拥有的子节点数目最多，四个子节点（灰色节点），所以上面的B-Tree为4阶B树。 B-Tree结构存储索引的特点 为了描述B-Tree首先定义一条记录为一个键值对[key, data] ，key为记录的键值，对应表中的主键值(聚簇索引)，data为一行记录中除主键外的数据。对于不同的记录，key值互不相同 索引值和data数据分布在整棵树结构中 白色块部分是指针,存储着子节点的地址信息。 每个节点可以存放多个索引值及对应的data数据 树节点中的多个索引值从左到右升序排列 B-Tree的查找操作 B-Tree的每个节点的元素可以视为一次I/O读取，树的高度表示最多的I/O次数，在相同数量的总元素个数下，每个节点的元素个数越多，高度越低，查询所需的I/O次数越少. B-Tree总结 优点: B树可以在内部节点存储键值和相关记录数据，因此把频繁访问的数据放在靠近根节点的位置将大大提高热点数据的查询效率。 缺点: B树中每个节点不仅包含数据的key值,还有data数据. 所以当data数据较大时,会导致每个节点存储的key值减少,并且导致B树的层数变高.增加查询时的IO次数. 使用场景: B树主要应用于文件系统以及部分数据库索引，如MongoDB，大部分关系型数据库索引则是使用B+树实现 2）B+Tree B+Tree是在B-Tree基础上的一种优化，使其更适合实现存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 B+Tree的特征 非叶子节点只存储键值信息. 所有叶子节点之间都有一个链指针. 数据记录都存放在叶子节点中. B+Tree的优势 B+Tree是B Tree的变种，B Tree能解决的问题，B+Tree也能够解决（降低树的高度，增大节点存储数据量） B+Tree扫库和扫表能力更强，如果我们要根据索引去进行数据表的扫描，对B Tree进行扫描，需要把整棵树遍历一遍，而B+Tree只需要遍历他的所有叶子节点即可（叶子节点之间有引用）。 B+Tree磁盘读写能力更强，他的根节点和支节点不保存数据区，所有根节点和支节点同样大小的情况下，保存的关键字要比B Tree要多。而叶子节点不保存子节点引用。所以，B+Tree读写一次磁盘加载的关键字比B Tree更多。 B+Tree排序能力更强，如上面的图中可以看出，B+Tree天然具有排序功能。 B+Tree查询效率更加稳定，每次查询数据，查询IO次数一定是稳定的。当然这个每个人的理解都不同，因为在B Tree如果根节点命中直接返回，确实效率更高。 16.一个B+树中大概能存放多少条索引记录？MySQL设计者将一个B+Tree的节点的大小设置为等于一个页. (这样做的目的是每个节点只需要一次I/O就可以完全载入), InnoDB的一个页的大小是16KB,所以每个节点的大小也是16KB, 并且B+Tree的根节点是保存在内存中的,子节点才是存储在磁盘上. 假设一个B+树高为2，即存在一个根节点和若干个叶子节点，那么这棵B+树的存放总记录数为： *根节点指针数单个叶子节点记录行数.** 计算根节点指针数: 假设表的主键为INT类型,占用的就是4个字节,或者是BIGINT占用8个字节, 指针大小为6个字节,那么一个页(就是B+Tree中的一个节点) ,大概可以存储: 16384B / (4B+6B) = 1638 ,一个节点最多可以存储1638个索引指针. 计算每个叶子节点的记录数:我们假设一行记录的数据大小为1k,那么一页就可以存储16行数据,16KB / 1KB = 16. 一颗高度为2的B+Tree可以存放的记录数为: 1638 * 16=26208 条数据记录, 同样的原理可以推算出一个高度3的B+Tree可以存放: 1638 * 1638 * 16 = 42928704条这样的记录. 所以InnoDB中的B+Tree高度一般为1-3层,就可以满足千万级别的数据存储,在查找数据时一次页的查找代表一次 IO，所以通过主键索引查询通常只需要 1-3 次 IO 操作即可查找到数据。 17.explain 用过吗，有哪些主要字段？使用 EXPLAIN 关键字可以模拟优化器来执行SQL查询语句，从而知道MySQL是如何处理我们的SQL语句的。分析出查询语句或是表结构的性能瓶颈。 MySQL查询过程 通过explain我们可以获得以下信息： 表的读取顺序 数据读取操作的操作类型 哪些索引可以被使用 哪些索引真正被使用 表的直接引用 每张表的有多少行被优化器查询了 Explain使用方式: explain+sql语句, 通过执行explain可以获得sql语句执行的相关信息 explain select * from users; 18.type字段中有哪些常见的值？type字段在 MySQL 官网文档描述如下： The join type. For descriptions of the different types. type字段显示的是连接类型 ( join type表示的是用什么样的方式来获取数据)，它描述了找到所需数据所使用的扫描方式, 是较为重要的一个指标。 下面给出各种连接类型,按照从最佳类型到最坏类型进行排序: -- 完整的连接类型比较多 system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL -- 简化之后,我们可以只关注一下几种 system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说,需要保证查询至少达到 range级别,最好能到ref,否则就要就行SQL的优化调整 下面介绍type字段不同值表示的含义: type类型 解释 system 不进行磁盘IO,查询系统表,仅仅返回一条数据 const 查找主键索引,最多返回1条或0条数据. 属于精确查找 eq_ref 查找唯一性索引,返回数据最多一条, 属于精确查找 ref 查找非唯一性索引,返回匹配某一条件的多条数据,属于精确查找,数据返回可能是多条. range 查找某个索引的部分索引,只检索给定范围的行,属于范围查找. 比如: &gt; 、 &lt; 、in 、between index 查找所有索引树,比ALL快一些,因为索引文件要比数据文件小. ALL 不使用任何索引,直接进行全表扫描 19.Extra有哪些主要指标，各自的含义是什么？Extra 是 EXPLAIN 输出中另外一个很重要的列，该列显示MySQL在查询过程中的一些详细信息 extra类型 解释 Using filesort MySQL中无法利用索引完成的排序操作称为 “文件排序” Using index 表示直接访问索引就能够获取到所需要的数据（覆盖索引），不需要通过索引回表 Using index condition 搜索条件中虽然出现了索引列，但是有部分条件无法使用索引， 会根据能用索引的条件先搜索一遍再匹配无法使用索引的条件。 Using join buffer 使用了连接缓存, 会显示join连接查询时,MySQL选择的查询算法 Using temporary 表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 Using where 意味着全表扫描或者在查找使用索引的情况下，但是还有查询条件不在索引字段当中 20.如何进行分页查询优化？ 一般性分页 一般的分页查询使用简单的 limit 子句就可以实现。limit格式如下： SELECT * FROM 表名 LIMIT [offset,] rows 第一个参数指定第一个返回记录行的偏移量，注意从0开始； 第二个参数指定返回记录行的最大数目； 如果只给定一个参数，它表示返回最大的记录行数目； 思考1：如果偏移量固定，返回记录量对执行时间有什么影响？ select * from user limit 10000,1; select * from user limit 10000,10; select * from user limit 10000,100; select * from user limit 10000,1000; select * from user limit 10000,10000; 结果：在查询记录时，返回记录量低于100条，查询时间基本没有变化，差距不大。随着查询记录量越大，所花费的时间也会越来越多。 思考2：如果查询偏移量变化，返回记录数固定对执行时间有什么影响？ select * from user limit 1,100; select * from user limit 10,100; select * from user limit 100,100; select * from user limit 1000,100; select * from user limit 10000,100; 结果：在查询记录时，如果查询记录量相同，偏移量超过100后就开始随着偏移量增大，查询时间急剧的增加。（这种分页查询机制，每次都会从数据库第一条记录开始扫描，越往后查询越慢，而且查询的数据越多，也会拖慢总查询速度。） 分页优化方案 优化1: 通过索引进行分页 直接进行limit操作 会产生全表扫描,速度很慢. Limit限制的是从结果集的M位置处取出N条输出,其余抛弃. 假设ID是连续递增的,我们根据查询的页数和查询的记录数可以算出查询的id的范围，然后配合 limit使用 EXPLAIN SELECT * FROM user WHERE id &gt;= 100001 LIMIT 100; 优化2：利用子查询优化 -- 首先定位偏移位置的id SELECT id FROM user_contacts LIMIT 100000,1; -- 根据获取到的id值向后查询. EXPLAIN SELECT * FROM user_contacts WHERE id &gt;= (SELECT id FROM user_contacts LIMIT 100000,1) LIMIT 100; 原因：使用了id做主键比较(id&gt;=)，并且子查询使用了覆盖索引进行优化。 21.如何做慢查询优化？MySQL 慢查询的相关参数解释： slow_query_log：是否开启慢查询日志，ON(1)表示开启,OFF(0) 表示关闭。 slow-query-log-file：新版（5.6及以上版本）MySQL数据库慢查询日志存储路径。 long_query_time： 慢查询阈值，当查询时间多于设定的阈值时，记录日志。 慢查询配置方式 默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的 mysql&gt; show variables like '%slow_query_log%'; +---------------------+------------------------------+ | Variable_name &nbsp; &nbsp; &nbsp; | Value &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| +---------------------+------------------------------+ | slow_query_log &nbsp; &nbsp; &nbsp;| ON &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | | slow_query_log_file | /var/lib/mysql/test-slow.log | +---------------------+------------------------------+ 可以通过设置slow_query_log的值来开启 mysql&gt; set global slow_query_log=1; 使用 set global slow_query_log=1 开启了慢查询日志只对当前数据库生效，MySQL重启后则会失效。如果要永久生效，就必须修改配置文件my.cnf（其它系统变量也是如此） -- 编辑配置 vim /etc/my.cnf -- 添加如下内容 slow_query_log =1 slow_query_log_file=/var/lib/mysql/ruyuan-slow.log -- 重启MySQL service mysqld restart mysql&gt; show variables like '%slow_query%'; +---------------------+--------------------------------+ | Variable_name &nbsp; &nbsp; &nbsp; | Value &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| +---------------------+--------------------------------+ | slow_query_log &nbsp; &nbsp; &nbsp;| ON &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | | slow_query_log_file | /var/lib/mysql/ruyuan-slow.log | +---------------------+--------------------------------+ 那么开启了慢查询日志后，什么样的SQL才会记录到慢查询日志里面呢？ 这个是由参数 long_query_time控制，默认情况下long_query_time的值为10秒. mysql&gt; show variables like 'long_query_time'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ mysql&gt; set global long_query_time=1; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like 'long_query_time'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 修改了变量long_query_time，但是查询变量long_query_time的值还是10，难道没有修改到呢？注意：使用命令 set global long_query_time=1 修改后，需要重新连接或新开一个会话才能看到修改值。 mysql&gt; show variables like 'long_query_time'; +-----------------+----------+ | Variable_name &nbsp; | Value &nbsp; &nbsp;| +-----------------+----------+ | long_query_time | 1.000000 | +-----------------+----------+ log_output 参数是指定日志的存储方式。log_output='FILE' 表示将日志存入文件，默认值是’FILE’。log_output='TABLE' 表示将日志存入数据库，这样日志信息就会被写入到 mysql.slow_log 表中。 mysql&gt; SHOW VARIABLES LIKE '%log_output%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_output &nbsp; &nbsp;| FILE &nbsp;| +---------------+-------+ MySQL数据库支持同时两种日志存储方式，配置的时候以逗号隔开即可，如：log_output=’FILE,TABLE’。日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源，因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件. 系统变量 log-queries-not-using-indexes：未使用索引的查询也被记录到慢查询日志中（可选项）。如果调优的话，建议开启这个选项。 mysql&gt; show variables like 'log_queries_not_using_indexes'; +-------------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | OFF &nbsp; | +-------------------------------+-------+ mysql&gt; set global log_queries_not_using_indexes=1; Query OK, 0 rows affected (0.00 sec) mysql&gt; show variables like 'log_queries_not_using_indexes'; +-------------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value | +-------------------------------+-------+ | log_queries_not_using_indexes | ON &nbsp; &nbsp;| +-------------------------------+-------+ 3) 慢查询测试 执行 test_index.sql 脚本,监控慢查询日志内容 [root@localhost mysql]# tail -f /var/lib/mysql/ruyuan-slow.log /usr/sbin/mysqld, Version: 5.7.30-log (MySQL Community Server (GPL)). started with: Tcp port: 0 Unix socket: /var/lib/mysql/mysql.sock Time &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Id Command &nbsp; Argument 执行下面的SQL,执行超时 (超过1秒) 我们去查看慢查询日志 SELECT * FROM test_index WHERE &nbsp; hobby = '20009951' OR hobby = '10009931' OR hobby = '30009931' OR dname = 'name4000' OR dname = 'name6600' ; 日志内容 我们得到慢查询日志后，最重要的一步就是去分析这个日志。我们先来看下慢日志里到底记录了哪些内容。 如下图是慢日志里其中一条SQL的记录内容，可以看到有时间戳，用户，查询时长及具体的SQL等信息. # Time: 2022-02-23T13:50:45.005959Z # User@Host: root[root] @ localhost [] Id: &nbsp; &nbsp; 3 # Query_time: 3.724273 Lock_time: 0.000371 Rows_sent: 5 Rows_examined: 5000000 SET timestamp=1645624245; select * from test_index where hobby = '20009951' or hobby = '10009931' or hobby = '30009931' or dname = 'name4000' or dname = 'name6600'; Time: 执行时间 User: 用户信息 ,Id信息 Query_time: 查询时长 Lock_time: 等待锁的时长 Rows_sent:查询结果的行数 Rows_examined: 查询扫描的行数 SET timestamp: 时间戳 SQL的具体信息 慢查询SQL优化思路 1) SQL性能下降的原因 在日常的运维过程中，经常会遇到DBA将一些执行效率较低的SQL发过来找开发人员分析，当我们拿到这个SQL语句之后，在对这些SQL进行分析之前，需要明确可能导致SQL执行性能下降的原因进行分析，执行性能下降可以体现在以下两个方面： 等待时间长 锁表导致查询一直处于等待状态，后续我们从MySQL锁的机制去分析SQL执行的原理 执行时间长 1.查询语句写的烂 2.索引失效 3.关联查询太多join 4.服务器调优及各个参数的设置 2) 慢查询优化思路 优先选择优化高并发执行的SQL,因为高并发的SQL发生问题带来后果更严重. 比如下面两种情况: SQL1: 每小时执行10000次, 每次20个IO 优化后每次18个IO,每小时节省2万次IO SQL2: 每小时10次,每次20000个IO,每次优化减少2000个IO,每小时节省2万次IO SQL2更难优化,SQL1更好优化.但是第一种属于高并发SQL,更急需优化 成本更低 定位优化对象的性能瓶颈(在优化之前了解性能瓶颈在哪) 在去优化SQL时,选择优化分方向有三个: 1.IO(数据访问消耗的了太多的时间,查看是否正确使用了索引) , 2.CPU(数据运算花费了太多时间, 数据的运算分组 排序是不是有问题) 3.网络带宽(加大网络带宽) 明确优化目标 需要根据数据库当前的状态 数据库中与该条SQL的关系 当前SQL的具体功能 最好的情况消耗的资源,最差情况下消耗的资源,优化的结果只有一个给用户一个好的体验 从explain执行计划入手 只有explain能告诉你当前SQL的执行状态 永远用小的结果集驱动大的结果集 小的数据集驱动大的数据集,减少内层表读取的次数 类似于嵌套循环 for(int i = 0; i &lt; 5; i++){ for(int i = 0; i &lt; 1000; i++){ } } 如果小的循环在外层,对于数据库连接来说就只连接5次,进行5000次操作,如果1000在外,则需要进行1000次数据库连接,从而浪费资源，增加消耗.这就是为什么要小表驱动大表。 尽可能在索引中完成排序 排序操作用的比较多,order by 后面的字段如果在索引中,索引本来就是排好序的,所以速度很快,没有索引的话,就需要从表中拿数据,在内存中进行排序,如果内存空间不够还会发生落盘操作 只获取自己需要的列 不要使用select * ,select * 很可能不走索引,而且数据量过大 只使用最有效的过滤条件 误区 where后面的条件越多越好,但实际上是应该用最短的路径访问到数据 尽可能避免复杂的join和子查询 每条SQL的JOIN操作 建议不要超过三张表 将复杂的SQL, 拆分成多个小的SQL 单个表执行,获取的结果 在程序中进行封装 如果join占用的资源比较多,会导致其他进程等待时间变长 合理设计并利用索引 如何判定是否需要创建索引? 1.较为频繁的作为查询条件的字段应该创建索引. 2.唯一性太差的字段不适合单独创建索引，即使频繁作为查询条件.（唯一性太差的字段主要是指哪些呢？如状态字段，类型字段等等这些字段中的数据可能总共就是那么几个几十个数值重复使用）（当一条Query所返回的数据超过了全表的15%的时候，就不应该再使用索引扫描来完成这个Query了）. 3.更新非常频繁的字段不适合创建索引.（因为索引中的字段被更新的时候，不仅仅需要更新表中的数据，同时还要更新索引数据，以确保索引信息是准确的）. 4.不会出现在WHERE子句中的字段不该创建索引. 如何选择合适索引? 1.对于单键索引，尽量选择针对当前Query过滤性更好的索引. 2.选择联合索引时,当前Query中过滤性最好的字段在索引字段顺序中排列要靠前. 3.选择联合索引时,尽量索引字段出现在w中比较多的索引. 22.Hash索引有哪些优缺点？MySQL中索引的常用数据结构有两种: 一种是B+Tree,另一种则是Hash. Hash底层实现是由Hash表来实现的，是根据键值 &lt;key,value&gt; 存储数据的结构。非常适合根据key查找value值，也就是单个key查询，或者说等值查询。 对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值,如果出现哈希码值相同的情况会拉出一条链表. Hsah索引的优点 因为索引自身只需要存储对应的Hash值,所以索引结构非常紧凑, 只需要做等值比较查询，而不包含排序或范围查询的需求，都适合使用哈希索引 . 没有哈希冲突的情况下,等值查询访问哈希索引的数据非常快.(如果发生Hash冲突,存储引擎必须遍历链表中的所有行指针,逐行进行比较,直到找到所有符合条件的行). Hash索引的缺点 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。 哈希索引只支持等值比较查询。不支持任何范围查询和部分索引列匹配查找。 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。 23.说一下InnoDB内存相关的参数优化？Buffer Pool参数优化 1.1 缓冲池内存大小配置 一个大的日志缓冲区允许大量的事务在提交之前不写日志到磁盘。因此，如果你有很多事务的更新，插入或删除操作，通过设置这个参数会大量的减少磁盘I/O的次数数。建议: 在专用数据库服务器上，可以将缓冲池大小设置为服务器物理内存的60% - 80%. 查看缓冲池大小 mysql&gt; show variables like '%innodb_buffer_pool_size%'; +-------------------------+-----------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value &nbsp; &nbsp; | +-------------------------+-----------+ | innodb_buffer_pool_size | 134217728 | +-------------------------+-----------+ mysql&gt; select 134217728 / 1024 / 1024; +-------------------------+ | 134217728 / 1024 / 1024 | +-------------------------+ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128.00000000 | +-------------------------+ 在线调整InnoDB缓冲池大小innodb_buffer_pool_size可以动态设置，允许在不重新启动服务器的情况下调整缓冲池的大小. mysql&gt; SET GLOBAL innodb_buffer_pool_size = 268435456; -- 512 Query OK, 0 rows affected (0.10 sec) mysql&gt; show variables like '%innodb_buffer_pool_size%'; +-------------------------+-----------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value &nbsp; &nbsp; | +-------------------------+-----------+ | innodb_buffer_pool_size | 268435456 | +-------------------------+-----------+ 监控在线调整缓冲池的进度 mysql&gt; SHOW STATUS WHERE Variable_name='InnoDB_buffer_pool_resize_status'; +----------------------------------+----------------------------------------------------------------------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Value &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| +----------------------------------+----------------------------------------------------------------------+ | Innodb_buffer_pool_resize_status | Size did not change (old size = new size = 268435456. Nothing to do. | +----------------------------------+----------------------------------------------------------------------+ 1.3 InnoDB 缓存性能评估 当前配置的innodb_buffer_pool_size是否合适，可以通过分析InnoDB缓冲池的缓存命中率来验证。 以下公式计算InnoDB buffer pool 命中率: 命中率 = innodb_buffer_pool_read_requests / (innodb_buffer_pool_read_requests+innodb_buffer_pool_reads)* 100 参数1: innodb_buffer_pool_reads：表示InnoDB缓冲池无法满足的请求数。需要从磁盘中读取。 参数2: innodb_buffer_pool_read_requests：表示从内存中读取页的请求数。 mysql&gt; show status like 'innodb_buffer_pool_read%'; +---------------------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value | +---------------------------------------+-------+ | Innodb_buffer_pool_read_ahead_rnd &nbsp; &nbsp; | 0 &nbsp; &nbsp; | | Innodb_buffer_pool_read_ahead &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; | | Innodb_buffer_pool_read_ahead_evicted | 0 &nbsp; &nbsp; | | Innodb_buffer_pool_read_requests &nbsp; &nbsp; &nbsp;| 12701 | | Innodb_buffer_pool_reads &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 455 &nbsp; | +---------------------------------------+-------+ -- 此值低于90%，则可以考虑增加innodb_buffer_pool_size。 mysql&gt; select 12701 / (455 + 12701) * 100 ; +-----------------------------+ | 12701 / (455 + 12701) * 100 | +-----------------------------+ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 96.5415 | +-----------------------------+ 1.4 Page管理相关参数 查看Page页的大小(默认16KB),innodb_page_size只能在初始化MySQL实例之前配置，不能在之后修改。如果没有指定值，则使用默认页面大小初始化实例。 mysql&gt; show variables like '%innodb_page_size%'; +------------------+-------+ | Variable_name &nbsp; &nbsp;| Value | +------------------+-------+ | innodb_page_size | 16384 | +------------------+-------+ Page页管理状态相关参数 mysql&gt; show global status like '%innodb_buffer_pool_pages%'; +----------------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Value | +----------------------------------+-------+ | Innodb_buffer_pool_pages_data &nbsp; &nbsp;| 515 &nbsp; | | Innodb_buffer_pool_pages_dirty &nbsp; | 0 &nbsp; &nbsp; | | Innodb_buffer_pool_pages_flushed | 334 &nbsp; | | Innodb_buffer_pool_pages_free &nbsp; &nbsp;| 15868 | | Innodb_buffer_pool_pages_misc &nbsp; &nbsp;| 0 &nbsp; &nbsp; | | Innodb_buffer_pool_pages_total &nbsp; | 16383 | +----------------------------------+-------+ pages_data: InnoDB缓冲池中包含数据的页数。 该数字包括脏页面和干净页面。 pages_dirty: 显示在内存中修改但尚未写入数据文件的InnoDB缓冲池数据页的数量（脏页刷新）。 pages_flushed: 表示从InnoDB缓冲池中刷新脏页的请求数。 pages_free: 显示InnoDB缓冲池中的空闲页面 pages_misc: 缓存池中当前已经被用作管理用途或hash index而不能用作为普通数据页的数目 pages_total: 缓存池的页总数目。单位是page。 24.InnoDB日志相关的参数优化了解过吗？1.日志缓冲区相关参数配置 日志缓冲区的大小。一般默认值16MB是够用的，但如果事务之中含有blog/text等大字段，这个缓冲区会被很快填满会引起额外的IO负载。配置更大的日志缓冲区,可以有效的提高MySQL的效率. innodb_log_buffer_size 缓冲区大小 mysql&gt; show variables like 'innodb_log_buffer_size'; +------------------------+----------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Value &nbsp; &nbsp;| +------------------------+----------+ | innodb_log_buffer_size | 16777216 | +------------------------+----------+ innodb_log_files_in_group 日志组文件个数日志组根据需要来创建。而日志组的成员则需要至少2个，实现循环写入并作为冗余策略。 mysql&gt; show variables like 'innodb_log_files_in_group'; +---------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Value | +---------------------------+-------+ | innodb_log_files_in_group | 2 &nbsp; &nbsp; | +---------------------------+-------+ innodb_log_file_size 日志文件大小参数innodb_log_file_size用于设定MySQL日志组中每个日志文件的大小(默认48M)。此参数是一个全局的静态参数，不能动态修改。*参数innodb_log_file_size的最大值，二进制日志文件大小（innodb_log_file_size * innodb_log_files_in_group）不能超过512GB.所以单个日志文件的大小不能超过256G.* mysql&gt; show variables like 'innodb_log_file_size'; +----------------------+----------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp;| Value &nbsp; &nbsp;| +----------------------+----------+ | innodb_log_file_size | 50331648 | +----------------------+----------+ 2.日志文件参数优化 首先我们先来看一下日志文件大小设置对性能的影响 设置过小 参数 innodb_log_file_size设置太小，就会导致MySQL的日志文件( redo log）频繁切换，频繁的触发数据库的检查点（Checkpoint），导致刷新脏页到磁盘的次数增加。从而影响IO性能。 处理大事务时，将所有的日志文件写满了，事务内容还没有写完，这样就会导致日志不能切换. 设置过大参数 innodb_log_file_size如果设置太大，虽然可以提升IO性能，但是当MySQL由于意外宕机时，二进制日志很大，那么恢复的时间必然很长。而且这个恢复时间往往不可控，受多方面因素影响。 优化建议: 如何设置合适的日志文件大小 ? 根据实际生产场景的优化经验,一般是计算一段时间内生成的事务日志（redo log）的大小， 而MySQL的日志文件的大小最少应该承载一个小时的业务日志量(官网文档中有说明)。 想要估计一下InnoDB redo log的大小，需要抓取一段时间内Log SequenceNumber（日志顺序号）的数据,来计算一小时内产生的日志大小. Log sequence number 自系统修改开始，就不断的生成redo日志。为了记录一共生成了多少日志，于是mysql设计了全局变量log sequence number，简称lsn，但不是从0开始，是从8704字节开始。 -- pager分页工具, 只获取 sequence的信息 mysql&gt; pager grep sequence; PAGER set to 'grep sequence' -- 查询状态,并倒计时一分钟 mysql&gt; show engine innodb status\\G select sleep(60); Log sequence number 5399154 1 row in set (0.00 sec) 1 row in set (1 min 0.00 sec) -- 一分时间内所生成的数据量 5406150 mysql&gt; show engine innodb status\\G; Log sequence number 5406150 -- 关闭pager mysql&gt; nopager; PAGER set to stdout 有了一分钟的日志量,据此推算一小时内的日志量 mysql&gt; select (5406150 - 5399154) / 1024 as kb_per_min; +------------+ | kb_per_min | +------------+ | &nbsp; &nbsp; 6.8320 | +------------+ mysql&gt; select (5406150 - 5399154) / 1024 * 60 as kb_per_min; +------------+ | kb_per_min | +------------+ | &nbsp; 409.9219 | +------------+ 太大的缓冲池或非常不正常的业务负载可能会计算出非常大(或非常小)的日志大小。这也是公式不足之处，需要根据判断和经验。但这个计算方法是一个很好的参考标准。 25.InnoDB IO线程相关参数优化了解过吗？数据库属于 IO 密集型的应用程序，其主要职责就是数据的管理及存储工作。从内存中读取一个数据库数据的时间是微秒级别，而从一块普通硬盘上读取一个IO是在毫秒级别。要优化数据库，IO操作是必须要优化的，尽可能将磁盘IO转化为内存IO。 1) 参数: query_cache_size&amp;have_query_cacheMySQL查询缓存会保存查询返回的完整结果。当查询命中该缓存，会立刻返回结果，跳过了解析，优化和执行阶段。查询缓存会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有缓存都将失效。 查看查询缓存是否开启 -- 查询是否支持查询缓存 mysql&gt; show variables like 'have_query_cache'; +------------------+-------+ | Variable_name &nbsp; &nbsp;| Value | +------------------+-------+ | have_query_cache | YES &nbsp; | +------------------+-------+ -- 查询是否开启查询缓存 默认关闭 mysql&gt; show variables like '%query_cache_type%'; +------------------+-------+ | Variable_name &nbsp; &nbsp;| Value | +------------------+-------+ | query_cache_type | OFF &nbsp; | +------------------+-------+ 开启缓存,在my.ini中添加下面一行参数 query_cache_size=128M query_cache_type=1 query_cache_type: 设置为0，OFF,缓存禁用 设置为1，ON,缓存所有的结果 设置为2，DENAND,只缓存在select语句中通过SQL_CACHE指定需要缓存的查询 测试能否缓存查询 mysql&gt; show status like '%Qcache%'; +-------------------------+---------+ | Variable_name | Value | +-------------------------+---------+ | Qcache_free_blocks | 1 | | Qcache_free_memory | 1031832 | | Qcache_hits | 0 | | Qcache_inserts | 0 | | Qcache_lowmem_prunes | 0 | | Qcache_not_cached | 1 | | Qcache_queries_in_cache | 0 | | Qcache_total_blocks | 1 | +-------------------------+---------+ Qcache_free_blocks:缓存中目前剩余的blocks数量（如果值较大，则查询缓存中的内存碎片过多） Qcache_free_memory:空闲缓存的内存大小 Qcache_hits:命中缓存次数 Qcache_inserts: 未命中然后进行正常查询 Qcache_lowmem_prunes:查询因为内存不足而被移除出查询缓存记录 Qcache_not_cached: 没有被缓存的查询数量 Qcache_queries_in_cache:当前缓存中缓存的查询数量 Qcache_total_blocks:当前缓存的block数量 优化建议: Query Cache的使用需要多个参数配合，其中最为关键的是 query_cache_size 和 query_cache_type ，前者设置用于缓存 ResultSet 的内存大小，后者设置在何场景下使用 Query Cache。 MySQL数据库数据变化相对不多，query_cache_size 一般设置为256MB比较合适 ,也可以通过计算Query Cache的命中率来进行调整 ( Qcache_hits / ( Qcache_hits + Qcache_inserts ) * 100) ) 参数: innodb_max_dirty_pages_pct 该参数是InnoDB 存储引擎用来控制buffer pool中脏页的百分比，当脏页数量占比超过这个参数设置的值时，InnoDB会启动刷脏页的操作。 -- innodb_max_dirty_pages_pct 参数可以动态调整，最小值为0， 最大值为99.99，默认值为 75。 mysql&gt; show variables like 'innodb_max_dirty_pages_pct'; +----------------------------+-----------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Value &nbsp; &nbsp; | +----------------------------+-----------+ | innodb_max_dirty_pages_pct | 75.000000 | +----------------------------+-----------+ 优化建议: 该参数比例值越大，从内存到磁盘的写入操作就会相对减少，所以能够一定程度下减少写入操作的磁盘IO。但是，如果这个比例值过大，当数据库 Crash 之后重启的时间可能就会很长，因为会有大量的事务数据需要从日志文件恢复出来写入数据文件中.最大不建议超过90,一般重启恢复的数据在超过1GB的话,启动速度就会变慢. 3) 参数: innodb_old_blocks_pct&amp;innodb_old_blocks_timeinnodb_old_blocks_pct 用来确定LRU链表中old sublist所占比例,默认占用37% mysql&gt; show variables like '%innodb_old_blocks_pct%'; +-----------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; | Value | +-----------------------+-------+ | innodb_old_blocks_pct | 37 &nbsp; &nbsp;| +-----------------------+-------+ innodb_old_blocks_time 用来控制old sublist中page的转移策略，新的page页在进入LRU链表中时，会先插入到old sublist的头部，然后page需要在old sublist中停留innodb_old_blocks_time这么久后，下一次对该page的访问才会使其移动到new sublist的头部，默认值1秒. mysql&gt; show variables like '%innodb_old_blocks_time%'; +------------------------+-------+ | Variable_name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Value | +------------------------+-------+ | innodb_old_blocks_time | 1000 &nbsp;| +------------------------+-------+ 优化建议: 在没有大表扫描的情况下，并且数据多为频繁使用的数据时，我们可以增加innodb_old_blocks_pct的值，并且减小innodb_old_blocks_time的值。让数据页能够更快和更多的进入的热点数据区。 26.什么是写失效？InnoDB的页和操作系统的页大小不一致，InnoDB页大小一般为16K，操作系统页大小为4K，InnoDB的页写入到磁盘时，一个页需要分4次写。 如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的情况，比如只写了4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失。 双写缓冲区 Doublewrite Buffer 为了解决写失效问题，InnoDB实现了double write buffer Files, 它位于系统表空间，是一个存储区域。 在BufferPool的page页刷新到磁盘真正的位置前，会先将数据存在Doublewrite 缓冲区。这样在宕机重启时，如果出现数据页损坏，那么在应用redo log之前，需要通过该页的副本来还原该页，然后再进行redo log重做，double write实现了InnoDB引擎数据页的可靠性. 默认情况下启用双写缓冲区，如果要禁用Doublewrite 缓冲区，可以将 innodb_doublewrite设置为0。 mysql&gt; show variables like '%innodb_doublewrite%'; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | innodb_doublewrite | ON | +--------------------+-------+ 1 row in set (0.01 sec) 数据双写流程 step1：当进行缓冲池中的脏页刷新到磁盘的操作时,并不会直接写磁盘,每次脏页刷新必须要先写double write . step2：通过memcpy函数将脏页复制到内存中的double write buffer . step3: double write buffer再分两次、每次1MB, 顺序写入共享表空间的物理磁盘上, 第一次写. step4: 在完成double write页的写入后，再将double wirite buffer中的页写入各个表的独立表空间文件中(数据文件 .ibd), 第二次写。 为什么写两次 ? 可能有的同学会有疑问，为啥写两次，刷一次数据文件保存数据不就可以了，为什么还要写共享表空间 ?其实是因为共享表空间是在ibdbata文件中划出2M连续的空间，专门给double write刷脏页用的, 由于在这个过程中，double write页的存储是连续的，因此写入磁盘为顺序写，性能很高；完成double write后，再将脏页写入实际的各个表空间文件，这时写入就是离散的了. 27.什么是行溢出？行记录格式 1) 行格式分类 表的行格式决定了它的行是如何物理存储的，这反过来又会影响查询和DML操作的性能。如果在单个page页中容纳更多行，查询和索引查找可以更快地工作，缓冲池中所需的内存更少，写入更新时所需的I/O更少。 InnoDB存储引擎支持四种行格式：Redundant、Compact、Dynamic 和 Compressed . 查询MySQL使用的行格式,默认为: dynamic mysql&gt; show variables like 'innodb_default_row_format'; +---------------------------+---------+ | Variable_name | Value | +---------------------------+---------+ | innodb_default_row_format | dynamic | +---------------------------+---------+ 指定行格式语法 CREATE TABLE &lt;table_name(column_name)&gt; ROW_FORMAT=行格式名称 ALTER TABLE &lt;table_name&gt; ROW_FORMAT=行格式名称 2) COMPACT 行记录格式 Compact 设计目标是高效地存储数据，一个页中存放的行数据越多，其性能就越高。 Compact行记录由两部分组成: 记录放入额外信息 和 记录的真实数据. 记录额外信息部分 服务器为了描述一条记录而添加了一些额外信息(元数据信息)，这些额外信息分为3类，分别是: 变长字段长度列表、NULL值列表和记录头信息. 变长字段长度列表 MySQL支持一些变长的数据类型，比如VARCHAR(M)、VARBINARY(M)、各种TEXT类型，各种BLOB类型，这些变长的数据类型占用的存储空间分为两部分： 真正的数据内容 占用的字节数 变长字段的长度是不固定的，所以在存储数据的时候要把这些数据占用的字节数也存起来，读取数据的时候才能根据这个长度列表去读取对应长度的数据。 在 Compact行格式中，把所有变长类型的列的长度都存放在记录的开头部位形成一个列表，按照列的顺序逆序存放,这个列表就是 变长字段长度列表。 NULL值列表 表中的某些列可能会存储NULL值，如果把这些NULL值都放到记录的真实数据中会比较浪费空间，所以Compact行格式把这些值为NULL的列存储到NULL值列表中。( 如果表中所有列都不允许为 NULL，就不存在NULL值列表 ) 记录头信息 记录头信息是由固定的5个字节组成，5个字节也就是40个二进制位，不同的位代表不同的意思，这些头信息会在后面的一些功能中看到。 名称 大小(单位:bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 标记该记录是否是本层B+树的非叶子节点中的最小记录 n_owned 4 表示当前分组中管理的记录数 heap_no 13 表示当前记录在记录堆中的位置信息 record_type 3 表示当前记录的类型:0 表示普通记录,1 表示B+树非叶子节点记录,2 表示最小记录,3表示最大记录 next_record 16 表示下一条记录的相对位置 delete_mask 这个属性标记着当前记录是否被删除，占用1个二进制位，值为0 的时候代表记录并没有被删除，为1 的时候代表记录被删除掉了 min_rec_mask B+树的每层非叶子节点中的最小记录都会添加该标记。 n_owned 代表每个分组里，所拥有的记录的数量，一般是分组里主键最大值才有的。 heap_no 在数据页的User Records中插入的记录是一条一条紧凑的排列的，这种紧凑排列的结构又被称为堆。为了便于管理这个堆，把记录在堆中的相对位置给定一个编号——heap_no。所以heap_no这个属性表示当前记录在本页中的位置。 record_type 这个属性表示当前记录的类型，一共有4种类型的记录， 0 表示普通用户记录， 1 表示B+树非叶节点记录， 2 表示最小记录， 3 表示最大记录。 next_record 表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量，可以理解为指向下一条记录地址的指针。值为正数说明下一条记录在当前记录后面，为负数说明下一条记录在当前记录的前面。 记录真实数据部分 记录的真实数据除了插入的那些列的数据，MySQL会为每个记录默认的添加一些列（也称为隐藏列），具体的列如下： 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID,唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 生成隐藏主键列的方式有: 1. 服务器会在内存中维护一个全局变量，每当向某个包含隐藏的row_id列的表中插入一条记录时，就会把该变量的值当作新记录的row_id列的值，并且把该变量自增1。 2. 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为7的页面中一个Max Row ID的属性处。 3. 当系统启动时，会将页中的Max Row ID属性加载到内存中，并将该值加上256之后赋值给全局变量，因为在上次关机时该全局变量的值可能大于页中Max Row ID属性值。 4. 3) Compact中的行溢出机制 什么是行溢出 ? MySQL中是以页为基本单位,进行磁盘与内存之间的数据交互的,我们知道一个页的大小是16KB,16KB = 16384字节.而一个varchar(m) 类型列最多可以存储65532个字节,一些大的数据类型比如TEXT可以存储更多. 如果一个表中存在这样的大字段,那么一个页就无法存储一条完整的记录.这时就会发生行溢出,多出的数据就会存储在另外的溢出页中. 总结: 如果某些字段信息过长，无法存储在B树节点中，这时候会被单独分配空间，此时被称为溢出页，该字段被称为页外列。 Compact中的行溢出机制 InnoDB 规定一页至少存储两条记录(B+树特点)，如果页中只能存放下一条记录，InnoDB存储引擎会自动将行数据存放到溢出页中. 当发生行溢出时，数据页只保存了前768字节的前缀数据，接着是20个字节的偏移量，指向行溢出页. 28.如何进行JOIN优化？JOIN 是 MySQL 用来进行联表操作的，用来匹配两个表的数据，筛选并合并出符合我们要求的结果集。 JOIN 操作有多种方式，取决于最终数据的合并效果。常用连接方式的有以下几种: 什么是驱动表 ? 多表关联查询时,第一个被处理的表就是驱动表,使用驱动表去关联其他表. 驱动表的确定非常的关键,会直接影响多表关联的顺序,也决定后续关联查询的性能 驱动表的选择要遵循一个规则: 在对最终的结果集没有影响的前提下,优先选择结果集最小的那张表作为驱动表 3) 三种JOIN算法 1.Simple Nested-Loop Join（ 简单的嵌套循环连接 ) 简单来说嵌套循环连接算法就是一个双层for 循环 ，通过循环外层表的行数据，逐个与内层表的所有行数据进行比较来获取结果. 这种算法是最简单的方案，性能也一般。对内循环没优化。 例如有这样一条SQL: -- 连接用户表与订单表 连接条件是 u.id = o.user_id select * from user t1 left join order t2 on t1.id = t2.user_id; -- user表为驱动表,order表为被驱动表 转换成代码执行时的思路是这样的: for(user表行 uRow : user表){ &nbsp; &nbsp;for(Order表的行 oRow : order表){ &nbsp; &nbsp; &nbsp; &nbsp;if(uRow.id = oRow.user_id){ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return uRow; &nbsp; &nbsp; &nbsp; } &nbsp; } } 匹配过程如下图 SNL 的特点 简单粗暴容易理解，就是通过双层循环比较数据来获得结果 查询效率会非常慢,假设 A 表有 N 行，B 表有 M 行。SNL 的开销如下： A 表扫描 1 次。 B 表扫描 M 次。 *一共有 N 个内循环，每个内循环要 M 次，一共有内循环 N * M 次* 2) Index Nested-Loop Join（ 索引嵌套循环连接 ） Index Nested-Loop Join 其优化的思路: 主要是为了减少内层表数据的匹配次数 , 最大的区别在于，用来进行 join 的字段已经在被驱动表中建立了索引。 从原来的 匹配次数 = 外层表行数 * 内层表行数 , 变成了 匹配次数 = 外层表的行数 * 内层表索引的高度 ，极大的提升了 join的性能。 当 order 表的 user_id 为索引的时候执行过程会如下图： 注意：使用Index Nested-Loop Join 算法的前提是匹配的字段必须建立了索引。 3) Block Nested-Loop Join( 块嵌套循环连接 ) 如果 join 的字段有索引，MySQL 会使用 INL 算法。如果没有的话，MySQL 会如何处理？ 因为不存在索引了，所以被驱动表需要进行扫描。这里 MySQL 并不会简单粗暴的应用 SNL 算法，而是加入了 buffer 缓冲区，降低了内循环的个数，也就是被驱动表的扫描次数。 在外层循环扫描 user表中的所有记录。扫描的时候，会把需要进行 join 用到的列都缓存到 buffer 中。buffer 中的数据有一个特点，里面的记录不需要一条一条地取出来和 order 表进行比较，而是整个 buffer 和 order表进行批量比较。 如果我们把 buffer 的空间开得很大，可以容纳下 user 表的所有记录，那么 order 表也只需要访问一次。 MySQL 默认 buffer 大小 256K，如果有 n 个 join 操作，会生成 n-1 个 join buffer。 mysql&gt; show variables like '%join_buffer%'; +------------------+--------+ | Variable_name &nbsp; &nbsp;| Value &nbsp;| +------------------+--------+ | join_buffer_size | 262144 | +------------------+--------+ mysql&gt; set session join_buffer_size=262144; Query OK, 0 rows affected (0.00 sec) 4) JOIN优化总结 永远用小结果集驱动大结果集(其本质就是减少外层循环的数据数量) 为匹配的条件增加索引(减少内层表的循环匹配次数) 增大join buffer size的大小（一次缓存的数据越多，那么内层包的扫表次数就越少） 减少不必要的字段查询（字段越少，join buffer 所缓存的数据就越多 29.索引哪些情况下会失效？ 查询条件包含 or，会导致索引失效。 隐式类型转换，会导致索引失效，例如 age 字段类型是 int，我们 where age = “1”，这样就会触发隐式类型转换 like 通配符会导致索引失效，注意:”ABC%” 不会失效，会走 range 索引，”% ABC” 索引会失效 联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。 对索引字段进行函数运算。 对索引列运算（如，+、-、*、/），索引失效。 索引字段上使用（!= 或者 &lt; &gt;，not in）时，会导致索引失效。 索引字段上使用 is null， is not null，可能导致索引失效。 相 join 的两个表的字符编码不同，不能命中索引，会导致笛卡尔积的循环计算 mysql 估计使用全表扫描要比使用索引快，则不使用索引。 30.什么是覆盖索引？覆盖索引是一种避免回表查询的优化策略: 只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。 具体的实现方式: 将被查询的字段建立普通索引或者联合索引，这样的话就可以直接返回索引中的的数据，不需要再通过聚集索引去定位行记录，避免了回表的情况发生。 EXPLAIN SELECT user_name,user_age,user_level FROM users WHERE user_name = 'tom' AND user_age = 17; 覆盖索引的定义与注意事项: 如果一个索引包含了 所有需要查询的字段的值 (不需要回表)，这个索引就是覆盖索引。 MySQL只能使用B+Tree索引做覆盖索引 (因为只有B+树能存储索引列值) 在explain的Extra列, 如果出现 **Using index 表示 使用到了覆盖索引 , 所取的数据完全在索引中就能拿到 31.介绍一下MySQL中事务的特性？在关系型数据库管理系统中，一个逻辑工作单元要成为事务，必须满足这 4 个特性，即所谓的 ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。 1）原子性 原子性：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 InnoDB存储引擎提供了两种事务日志：redo log(重做日志)和undo log(回滚日志)。其中redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。 每写一个事务,都会修改Buffer Pool,从而产生相应的Redo/Undo日志: 如果要回滚事务，那么就基于undo log来回滚就可以了，把之前对缓存页做的修改都给回滚了就可以了。 如果事务提交之后，redo log刷入磁盘，结果MySQL宕机了，是可以根据redo log恢复事务修改过的缓存数据的。 实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。 InnoDB 实现回滚，靠的是undo log ：当事务对数据库进行修改时，InnoDB 会生成对应的undo log ；如果事务执行失败或调用了rollback ，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。 2）一致性 一致性：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。 约束一致性：创建表结构时所指定的外键、唯一索引等约束。 数据一致性：是一个综合性的规定，因为它是由原子性、持久性、隔离性共同保证的结果，而不是单单依赖于某一种技术。 3）隔离性 隔离性：指的是一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对其他的并发事务是隔离的。 不考虑隔离性会引发的问题: 脏读 : 一个事务读取到了另一个事务修改但未提交的数据。 不可重复读: 一个事务中多次读取同一行记录的结果不一致，后面读取的跟前面读取的结果不一致。 幻读 : 一个事务中多次按相同条件查询，结果不一致。后续查询的结果和面前查询结果不同，多了或少了几行记录。 数据库事务的隔离级别有4个，由低到高依次为Read uncommitted 、Read committed、Repeatable read 、Serializable ，这四个级别可以逐个解决脏读 、不可重复读 、幻读 这几类问题。 4）持久性 持久性：指的是一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，后续的操作或故障不应该对其有任何影响，不会丢失。 MySQL 事务的持久性保证依赖的日志文件: redo log redo log 也包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。redo log是物理日志，记录的是数据库中物理页的情况 。 当数据发生修改时，InnoDB不仅会修改Buffer Pool中的数据，也会在redo log buffer记录这次操作；当事务提交时，会对redo log buffer进行刷盘，记录到redo log file中。如果MySQL宕机，重启时可以读取redo log file中的数据，对数据库进行恢复。这样就不需要每次提交事务都实时进行刷脏了。 5）ACID总结 事务的持久化是为了应对系统崩溃造成的数据丢失. 只有保证了事务的一致性，才能保证执行结果的正确性 在非并发状态下，事务间天然保证隔离性，因此只需要保证事务的原子性即可保证一致性. 在并发状态下，需要严格保证事务的原子性、隔离性。 32.MySQL 的可重复读怎么实现的？可重复读（repeatable read）定义： 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 MVCC MVCC，多版本并发控制, 用于实现读已提交和可重复读隔离级别。 MVCC的核心就是 Undo log多版本链 + Read view，“MV”就是通过 Undo log来保存数据的历史版本，实现多版本的管理，“CC”是通过 Read-view来实现管理，通过 Read-view原则来决定数据是否显示。同时针对不同的隔离级别， Read view的生成策略不同，也就实现了不同的隔离级别。 Undo log 多版本链 每条数据都有两个隐藏字段: trx_id: 事务id,记录最近一次更新这条数据的事务id. roll_pointer: 回滚指针,指向之前生成的undo log 每一条数据都有多个版本,版本之间通过undo log链条进行连接通过这样的设计方式,可以保证每个事务提交的时候,一旦需要回滚操作,可以保证同一个事务只能读取到比当前版本更早提交的值,不能看到更晚提交的值。 ReadView Read View是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现. Read View简单理解就是对数据在某个时刻的状态拍成照片记录下来。那么之后获取某时刻的数据时就还是原来的照片上的数据，是不会变的. Read View中比较重要的字段有4个: m_ids : 用来表示MySQL中哪些事务正在执行,但是没有提交. min_trx_id: 就是m_ids里最小的值. max_trx_id : 下一个要生成的事务id值,也就是最大事务id creator_trx_id: 就是你这个事务的id 当一个事务第一次执行查询sql时，会生成一致性视图 read-view（快照），查询时从 undo log 中最新的一条记录开始跟 read-view 做对比，如果不符合比较规则，就根据回滚指针回滚到上一条记录继续比较，直到得到符合比较条件的查询结果。 Read View判断记录某个版本是否可见的规则如下 1.如果当前记录的事务id落在绿色部分（trx_id &lt; min_id），表示这个版本是已提交的事务生成的，可读。2.如果当前记录的事务id落在红色部分（trx_id &gt; max_id），表示这个版本是由将来启动的事务生成的，不可读。 如果当前记录的事务id落在黄色部分（min_id &lt;= trx_id &lt;= max_id），则分为两种情况： 若当前记录的事务id在未提交事务的数组中，则此条记录不可读； 若当前记录的事务id不在未提交事务的数组中，则此条记录可读。 RC 和 RR 隔离级别都是由 MVCC 实现，区别在于： RC 隔离级别时，read-view 是每次执行 select 语句时都生成一个； RR 隔离级别时，read-view 是在第一次执行 select 语句时生成一个，同一事务中后面的所有 select 语句都复用这个 read-view 。 33.Repeatable Read 解决了幻读问题吗？可重复读（repeatable read）定义： 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 不过理论上会出现幻读，简单的说幻读指的的当用户读取某一范围的数据行时，另一个事务又在该范围插入了新行，当用户在读取该范围的数据时会发现有新的幻影行。 注意在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此， 幻读在“当前读”下才会出现（查询语句添加for update，表示当前读）； 在 MVCC 并发控制中，读操作可以分为两类: 快照读（Snapshot Read）与当前读 （Current Read）。 快照读快照读是指读取数据时不是读取最新版本的数据，而是基于历史版本读取的一个快照信息（mysql读取undo log历史版本) ，快照读可以使普通的SELECT 读取数据时不用对表数据进行加锁，从而解决了因为对数据库表的加锁而导致的两个如下问题 解决了因加锁导致的修改数据时无法对数据读取问题. 解决了因加锁导致读取数据时无法对数据进行修改的问题. 当前读当前读是读取的数据库最新的数据，当前读和快照读不同，因为要读取最新的数据而且要保证事务的隔离性，所以当前读是需要对数据进行加锁的（插入/更新/删除操作，属于当前读，需要加锁 , select for update 为当前读） 表结构 id key value 0 0 0 1 1 1 假设 select * from where value=1 for update，只在这一行加锁（注意这只是假设），其它行不加锁，那么就会出现如下场景： Session A的三次查询Q1-Q3都是select * from where value=1 for update，查询的value=1的所有row。 T1：Q1只返回一行(1,1,1)； T2：session B更新id=0的value为1，此时表t中value=1的数据有两行 T3：Q2返回两行(0,0,1),(1,1,1) T4：session C插入一行(6,6,1)，此时表t中value=1的数据有三行 T5：Q3返回三行(0,0,1),(1,1,1),(6,6,1) T6：session A事物commit。 其中Q3读到value=1这一样的现象，就称之为幻读，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。 先对“幻读”做出如下解释： 要讨论「可重复读」隔离级别的幻读现象，是要建立在「当前读」的情况下，而不是快照读,因为在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。 Next-key Lock 锁 产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了 next-key 锁，就是记录锁和间隙锁的组合。 RecordLock锁：锁定单个行记录的锁。（记录锁，RC、RR隔离级别都支持） GapLock锁：间隙锁，锁定索引记录间隙(不包括记录本身)，确保索引记录的间隙不变。（范围锁，RR隔离级别支持） Next-key Lock 锁：记录锁和间隙锁组合，同时锁住数据，并且锁住数据前后范围。（记录锁+范围锁，RR隔离级别支持） 总结 RR隔离级别下间隙锁才有效，RC隔离级别下没有间隙锁； RR隔离级别下为了解决“幻读”问题：“快照读”依靠MVCC控制，“当前读”通过间隙锁解决； 间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间； 间隙锁的引入，可能会导致同样语句锁住更大的范围，影响并发度。 34.请说一下数据库锁的种类？MySQL数据库由于其自身架构的特点,存在多种数据存储引擎, MySQL中不同的存储引擎支持不同的锁机制。 MyISAM和MEMORY存储引擎采用的表级锁， InnoDB存储引擎既支持行级锁，也支持表级锁，默认情况下采用行级锁。 BDB采用的是页面锁，也支持表级锁 按照数据操作的类型分 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。 写锁（排他锁）：当前写操作没有完成前，它会阻断其他写锁和读锁。 按照数据操作的粒度分 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 行级锁： 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 按照操作性能可分为乐观锁和悲观锁 乐观锁：一般的实现方式是对记录数据版本进行比对，在数据更新提交的时候才会进行冲突检测，如果发现冲突了，则提示错误信息。 悲观锁：在对一条数据修改的时候，为了避免同时被其他人修改，在修改数据之前先锁定，再修改的控制方式。共享锁和排他锁是悲观锁的不同实现，但都属于悲观锁范畴。 35.请说一下共享锁和排他锁？行级锁分为共享锁和排他锁两种。 行锁的是mysql锁中粒度最小的一种锁，因为锁的粒度很小，所以发生资源争抢的概率也最小，并发性能最大，但是也会造成死锁，每次加锁和释放锁的开销也会变大。 使用MySQL行级锁的两个前提 使用 innoDB 引擎 开启事务 (隔离级别为 Repeatable Read) InnoDB行锁的类型 共享锁（S）：当事务对数据加上共享锁后, 其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 排他锁（X）：如果事务T对数据A加上排他锁后，则其他事务不能再对数据A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 加锁的方式 InnoDB引擎默认更新语句，update,delete,insert 都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果要加可以使用下面的方式: 加共享锁（S）：select * from table_name where … lock in share mode; 加排他锁（x）：select * from table_name where … for update; 锁兼容 共享锁只能兼容共享锁, 不兼容排它锁 排它锁互斥共享锁和其它排它锁 36.InnoDB 的行锁是怎么实现的？InnoDB行锁是通过对索引数据页上的记录加锁实现的，主要实现算法有 3 种：Record Lock、Gap Lock 和 Next-key Lock。 RecordLock锁：锁定单个行记录的锁。（记录锁，RC、RR隔离级别都支持） GapLock锁：间隙锁，锁定索引记录间隙，确保索引记录的间隙不变。（范围锁，RR隔离级别支持） Next-key Lock 锁：记录锁和间隙锁组合，同时锁住数据，并且锁住数据前后范围。（记录锁+范围锁，RR隔离级别支持） 注意： InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁 在RR隔离级别，InnoDB对于记录加锁行为都是先采用Next-Key Lock，但是当SQL操作含有唯一索引时，Innodb会对Next-Key Lock进行优化，降级为RecordLock，仅锁住索引本身而非范围。 各种操作加锁的特点 1）select … from 语句：InnoDB引擎采用MVCC机制实现非阻塞读，所以对于普通的select语句，InnoDB不加锁 2）select … from lock in share mode语句：追加了共享锁，InnoDB会使用Next-Key Lock锁进行处理，如果扫描发现唯一索引，可以降级为RecordLock锁。 3）select … from for update语句：追加了排他锁，InnoDB会使用Next-Key Lock锁进行处理，如果扫描发现唯一索引，可以降级为RecordLock锁。 4）update … where 语句：InnoDB会使用Next-Key Lock锁进行处理，如果扫描发现唯一索引，可以降级为RecordLock锁。 5）delete … where 语句：InnoDB会使用Next-Key Lock锁进行处理，如果扫描发现唯一索引，可以降级为RecordLock锁。 6）insert语句：InnoDB会在将要插入的那一行设置一个排他的RecordLock锁。 下面以“update t1 set name=‘lisi’ where id=10”操作为例，举例子分析下 InnoDB 对不同索引的加锁行为，以RR隔离级别为例。 主键加锁 加锁行为：仅在id=10的主键索引记录上加X锁。 唯一键加锁 加锁行为：现在唯一索引id上加X锁，然后在id=10的主键索引记录上加X锁。 非唯一键加锁 加锁行为：对满足id=10条件的记录和主键分别加X锁，然后在(6,c)-(10,b)、(10,b)-(10,d)、(10,d)-(11,f)范围分别加Gap Lock。 无索引加锁 加锁行为：表里所有行和间隙都会加X锁。（当没有索引时，会导致全表锁定，因为InnoDB引擎锁机制是基于索引实现的记录锁定）。 37.并发事务会产生哪些问题事务并发处理可能会带来一些问题，如下： 更新丢失当两个或多个事务更新同一行记录，会产生更新丢失现象。可以分为回滚覆盖和提交覆盖。 回滚覆盖：一个事务回滚操作，把其他事务已提交的数据给覆盖了。 提交覆盖：一个事务提交操作，把其他事务已提交的数据给覆盖了。 脏读一个事务读取到了另一个事务修改但未提交的数据。 不可重复读一个事务中多次读取同一行记录不一致，后面读取的跟前面读取的不一致。 幻读一个事务中多次按相同条件查询，结果不一致。后续查询的结果和面前查询结果不同，多了或少了几行记录。 “更新丢失”、”脏读”、“不可重复读”和“幻读”等并发事务问题，其实都是数据库一致性问题，为了解决这些问题，MySQL数据库是通过事务隔离级别来解决的，数据库系统提供了以下 4 种事务隔离级别供用户选择。 读未提交Read Uncommitted 读未提交：解决了回滚覆盖类型的更新丢失，但可能发生脏读现象，也就是可能读取到其他会话中未提交事务修改的数据。 已提交读Read Committed 读已提交：只能读取到其他会话中已经提交的数据，解决了脏读。但可能发生不可重复读现象，也就是可能在一个事务中两次查询结果不一致。 可重复度Repeatable Read 可重复读：解决了不可重复读，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上会出现幻读，简单的说幻读指的的当用户读取某一范围的数据行时，另一个事务又在该范围插入了新行，当用户在读取该范围的数据时会发现有新的幻影行。 可串行化所有的增删改查串行执行。它通过强制事务排序，解决相互冲突，从而解决幻度的问题。这个级别可能导致大量的超时现象的和锁竞争，效率低下。 数据库的事务隔离级别越高，并发问题就越小，但是并发处理能力越差（代价）。读未提交隔离级别最低，并发问题多，但是并发处理能力好。以后使用时，可以根据系统特点来选择一个合适的隔离级别，比如对不可重复读和幻读并不敏感，更多关心数据库并发处理能力，此时可以使用Read Commited隔离级别。 事务隔离级别，针对Innodb引擎，支持事务的功能。像MyISAM引擎没有关系。 事务隔离级别和锁的关系 1）事务隔离级别是SQL92定制的标准，相当于事务并发控制的整体解决方案，本质上是对锁和MVCC使用的封装，隐藏了底层细节。 2）锁是数据库实现并发控制的基础，事务隔离性是采用锁来实现，对相应操作加不同的锁，就可以防止其他事务同时对数据进行读写操作。 3）对用户来讲，首先选择使用隔离级别，当选用的隔离级别不能解决并发问题或需求时，才有必要在开发中手动的设置锁。 MySQL默认隔离级别：可重复读 Oracle、SQLServer默认隔离级别：读已提交 一般使用时，建议采用默认隔离级别，然后存在的一些并发问题，可以通过悲观锁、乐观锁等实现处理。 38.说一下MVCC内部细节MVCC概念 MVCC（Multi Version Concurrency Control）被称为多版本并发控制，是指在数据库中为了实现高并发的数据访问，对数据进行多版本处理，并通过事务的可见性来保证事务能看到自己应该看到的数据版本。 MVCC最大的好处是读不加锁，读写不冲突。在读多写少的系统应用中，读写不冲突是非常重要的，极大的提升系统的并发性能，这也是为什么现阶段几乎所有的关系型数据库都支持 MVCC 的原因，不过目前MVCC只在 Read Commited 和 Repeatable Read 两种隔离级别下工作。 回答这个面试题时，主要介绍以下的几个关键内容： 1）行记录的三个隐藏字段 DB_ROW_ID : 如果没有为表显式的定义主键，并且表中也没有定义唯一索引，那么InnoDB会自动为表添加一个row_id的隐藏列作为主键。 DB_TRX_ID : 事务中对某条记录做增删改时,就会将这个事务的事务ID写入到trx_id中. DB_ROLL_PTR: 回滚指针,指向undo log的指针 2）Undo log 多版本链 举例：事务 T-100 和 T-120 对表中 id = 1 的数据行做 update 操作，事务 T-130 进行 select 操作，即使 T-100 已经提交修改，三次 select 语句的结果都是“lisi”。 每一条数据都有多个版本,版本之间通过undo log链条进行连接 3）ReadView Read View是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现. Read View简单理解就是对数据在每个时刻的状态拍成照片记录下来。那么之后获取某时刻的数据时就还是原来的照片上的数据，是不会变的. Read View中比较重要的字段有4个: m_ids : 用来表示MySQL中哪些事务正在执行,但是没有提交. min_trx_id: 就是m_ids里最小的值. max_trx_id : 下一个要生成的事务id值,也就是最大事务id creator_trx_id: 就是你这个事务的id 通过Read View判断记录的某个版本是否可见的方式总结: trx_id = creator_trx_id如果被访问版本的trx_id,与readview中的creator_trx_id值相同,表明当前事务在访问自己修改过的记录,该版本可以被当前事务访问. trx_id &lt; min_trx_id如果被访问版本的trx_id,小于readview中的min_trx_id值,表明生成该版本的事务在当前事务生成readview前已经提交,该版本可以被当前事务访问. trx_id &gt;= max_trx_id如果被访问版本的trx_id,大于或等于readview中的max_trx_id值,表明生成该版本的事务在当前事务生成readview后才开启,该版本不可以被当前事务访问. trx_id &gt; min_trx_id &amp;&amp; trx_id &lt; max_trx_id如果被访问版本的trx_id,值在readview的min_trx_id和max_trx_id之间，就需要判断trx_id属性值是不是在m_ids列表中？ 在：说明创建readview时生成该版本的事务还是活跃的,该版本不可以被访问 不在：说明创建readview时生成该版本的事务已经被提交,该版本可以被访问 何时生成ReadView快照 在 读已提交（Read Committed， 简称RC） 隔离级别下，每一次读取数据前都生成一个ReadVIew。 在 可重复读 （Repeatable Read，简称RR）隔离级别下，在一个事务中，只在 第一次读取数据前生成一个ReadVIew。 4）快照读（Snapshot Read）与当前读 （Current Read） 在 MVCC 并发控制中，读操作可以分为两类: 快照读（Snapshot Read）与当前读 （Current Read）。 快照读快照读是指读取数据时不是读取最新版本的数据，而是基于历史版本读取的一个快照信息（mysql读取undo log历史版本) ，快照读可以使普通的SELECT 读取数据时不用对表数据进行加锁，从而解决了因为对数据库表的加锁而导致的两个如下问题 解决了因加锁导致的修改数据时无法对数据读取问题. 解决了因加锁导致读取数据时无法对数据进行修改的问题. 当前读当前读是读取的数据库最新的数据，当前读和快照读不同，因为要读取最新的数据而且要保证事务的隔离性，所以当前读是需要对数据进行加锁的（Update delete insert select ....lock in share mode , select for update 为当前读） 总结一下 并发环境下，写-写操作有加锁解决方案，但为了提高性能，InnoDB存储引擎提供MVCC，目的是为了解决读-写，写-读操作下不加锁仍能安全进行。 MVCC的过程，本质就是访问版本链，并判断哪个版本可见的过程。该判断算法是通过版本上的trx_id与快照ReadView的若干个信息进行对比。 快照生成的时机因隔离级别不同，读已提交隔离级别下，每一次读取前都会生成一个快照ReadView；而可重复读则仅在一个事务中，第一次读取前生成一个快照。 39.说一下MySQL死锁的原因和处理方法1) 表的死锁 产生原因: 用户A访问表A（锁住了表A），然后又访问表B；另一个用户B访问表B（锁住了表B），然后企图访问表A；这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。 用户A–》A表（表锁）–》B表（表锁） 用户B–》B表（表锁）–》A表（表锁） 解决方案： 这种死锁比较常见，是由于程序的BUG产生的，除了调整的程序的逻辑没有其它的办法。 仔细分析程序的逻辑，对于数据库的多表操作时，尽量按照相同的顺序进行处理，尽量避免同时锁定两个资源，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。 2) 行级锁死锁 产生原因1： 如果在事务中执行了一条没有索引条件的查询，引发全表扫描，把行级锁上升为全表记录锁定（等价于表级锁），多个这样的事务执行后，就很容易产生死锁和阻塞，最终应用系统会越来越慢，发生阻塞或死锁。 解决方案1： SQL语句中不要使用太复杂的关联多表的查询；使用explain“执行计划”对SQL语句进行分析，对于有全表扫描和全表锁定的SQL语句，建立相应的索引进行优化。 产生原因2： 两个事务分别想拿到对方持有的锁，互相等待，于是产生死锁 产生原因3：每个事务只有一个SQL,但是有些情况还是会发生死锁. 事务1,从name索引出发 , 读到的[hdc, 1], [hdc, 6]均满足条件, 不仅会加name索引上的记录X锁, 而且会加聚簇索引上的记录X锁, 加锁顺序为先[1,hdc,100], 后[6,hdc,10] 事务2，从pubtime索引出发，[10,6],[100,1]均满足过滤条件，同样也会加聚簇索引上的记录X锁，加锁顺序为[6,hdc,10]，后[1,hdc,100]。 但是加锁时发现跟事务1的加锁顺序正好相反，两个Session恰好都持有了第一把锁，请求加第二把锁，死锁就发生了。 解决方案: 如上面的原因2和原因3, 对索引加锁顺序的不一致很可能会导致死锁，所以如果可以，尽量以相同的顺序来访问索引记录和表。在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能； 40.介绍一下MySQL的体系架构？ MySQL Server架构自顶向下大致可以分网络连接层、服务层、存储引擎层和系统文件层。 一、网络连接层 客户端连接器（Client Connectors）：提供与MySQL服务器建立的支持。目前几乎支持所有主流的服务端编程技术，例如常见的 Java、C、Python、.NET等，它们通过各自API技术与MySQL建立连接。 二、服务层（MySQL Server） 服务层是MySQL Server的核心，主要包含系统管理和控制工具、连接池、SQL接口、解析器、查询优化器和缓存六个部分。 连接池（Connection Pool）：负责存储和管理客户端与数据库的连接，一个线程负责管理一个连接。 系统管理和控制工具（Management Services &amp; Utilities）：例如备份恢复、安全管理、集群管理等 SQL接口（SQL Interface）：用于接受客户端发送的各种SQL命令，并且返回用户需要查询的结果。比如DML、DDL、存储过程、视图、触发器等。 解析器（Parser）：负责将请求的SQL解析生成一个”解析树”。然后根据一些MySQL规则进一步检查解析树是否合法。 查询优化器（Optimizer）：当“解析树”通过解析器语法检查后，将交由优化器将其转化成执行计划，然后与存储引擎交互。 select uid,name from user where gender=1; 选取–》投影–》联接 策略 1）select先根据where语句进行选取，并不是查询出全部数据再过滤 2）select查询根据uid和name进行属性投影，并不是取出所有字段 3）将前面选取和投影联接起来最终生成查询结果 缓存（Cache&amp;Buffer）： 缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，权限缓存，引擎缓存等。如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。 三、存储引擎层（Pluggable Storage Engines） 存储引擎负责MySQL中数据的存储与提取，与底层系统文件进行交互。MySQL存储引擎是插件式的，服务器中的查询执行引擎通过接口与存储引擎进行通信，接口屏蔽了不同存储引擎之间的差异 。现在有很多种存储引擎，各有各的特点，最常见的是MyISAM和InnoDB。 四、系统文件层（File System） 该层负责将数据库的数据和日志存储在文件系统之上，并完成与存储引擎的交互，是文件的物理存储层。主要包含日志文件，数据文件，配置文件，pid 文件，socket 文件等。 日志文件 错误日志（Error log）默认开启，show variables like ‘%log_error%’ 通用查询日志（General query log）记录一般查询语句，show variables like ‘%general%’; 二进制日志（binary log）记录了对MySQL数据库执行的更改操作，并且记录了语句的发生时间、执行时长；但是它不记录select、show等不修改数据库的SQL。主要用于数据库恢复和主从复制。show variables like ‘%log_bin%’; //是否开启show variables like ‘%binlog%’; //参数查看show binary logs;//查看日志文件 慢查询日志（Slow query log）记录所有执行时间超时的查询SQL，默认是10秒。show variables like ‘%slow_query%’; //是否开启show variables like ‘%long_query_time%’; //时长 配置文件用于存放MySQL所有的配置信息文件，比如my.cnf、my.ini等。 数据文件 db.opt 文件：记录这个库的默认使用的字符集和校验规则。 frm 文件：存储与表相关的元数据（meta）信息，包括表结构的定义信息等，每一张表都会有一个frm 文件。 MYD 文件：MyISAM 存储引擎专用，存放 MyISAM 表的数据（data)，每一张表都会有一个 .MYD 文件。 MYI 文件：MyISAM 存储引擎专用，存放 MyISAM 表的索引相关信息，每一张 MyISAM 表对应一个 .MYI 文件。 ibd文件和 IBDATA 文件：存放 InnoDB 的数据文件（包括索引）。InnoDB 存储引擎有两种表空间方式：独享表空间和共享表空间。独享表空间使用 .ibd 文件来存放数据，且每一张 InnoDB 表对应一个 .ibd 文件。共享表空间使用 .ibdata 文件，所有表共同使用一个（或多个，自行配置）.ibdata 文件。 ibdata1 文件：系统表空间数据文件，存储表元数据、Undo日志等 。 ib_logfile0、ib_logfile1 文件：Redo log 日志文件。 pid 文件pid 文件是 mysqld 应用程序在 Unix/Linux 环境下的一个进程文件，和许多其他 Unix/Linux 服务端程序一样，它存放着自己的进程 id。 socket 文件socket 文件也是在 Unix/Linux 环境下才有的，用户在 Unix/Linux 环境下客户端连接可以不通过 TCP/IP 网络而直接使用 Unix Socket 来连接 MySQL。 41.undo log、redo log、 bin log的作用是什么？undo log 基本概念 undo log是一种用于撤销回退的日志，在数据库事务开始之前，MySQL会先记录更新前的数据到 undo log日志文件里面，当事务回滚时或者数据库崩溃时，可以利用 undo log来进行回退。 Undo Log产生和销毁：Undo Log在事务开始前产生；事务在提交时，并不会立刻删除undo log，innodb会将该事务对应的undo log放入到删除列表中，后面会通过后台线程purge thread进行回收处理。 注意: undo log也会产生redo log，因为undo log也要实现持久性保护。 undo log的作用 提供回滚操作【undo log实现事务的原子性】在数据修改的时候，不仅记录了redo log，还记录了相对应的undo log，如果因为某些原因导致事务执行失败了，可以借助undo log进行回滚。undo log 和 redo log 记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。 提供多版本控制(MVCC)【undo log实现多版本并发控制（MVCC）】MVCC，即多版本控制。在MySQL数据库InnoDB存储引擎中，用undo Log来实现多版本并发控制(MVCC)。当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据版本是怎样的，从而让用户能够读取到当前事务操作之前的数据【快照读】。 redo log 基本概念 InnoDB引擎对数据的更新，是先将更新记录写入redo log日志，然后会在系统空闲的时候或者是按照设定的更新策略再将日志中的内容更新到磁盘之中。这就是所谓的预写式技术（Write Ahead logging）。这种技术可以大大减少IO操作的频率，提升数据刷新的效率。 redo log：被称作重做日志, 包括两部分：一个是内存中的日志缓冲： redo log buffer，另一个是磁盘上的日志文件： redo log file 。 redo log的作用 mysql 每执行一条 DML 语句，先将记录写入 redo log buffer 。后续某个时间点再一次性将多个操作记录写到 redo log file 。当故障发生致使内存数据丢失后，InnoDB会在重启时，经过重放 redo，将Page恢复到崩溃之前的状态 通过Redo log可以实现事务的持久性 。 bin log基本概念 binlog是一个二进制格式的文件，用于记录用户对数据库更新的SQL语句信息，例如更改数据库表和更改内容的SQL语句都会记录到binlog里，但是不会记录SELECT和SHOW这类操作。 binlog在MySQL的Server层实现(引擎共用) binlog为逻辑日志,记录的是一条SQL语句的原始逻辑 binlog不限制大小,追加写入,不会覆盖以前的日志. 默认情况下，binlog日志是二进制格式的，不能使用查看文本工具的命令（比如，cat，vi等）查看，而使用mysqlbinlog解析查看。 bin log的作用 主从复制：在主库中开启Binlog功能，这样主库就可以把Binlog传递给从库，从库拿到Binlog后实现数据恢复达到主从数据一致性。 数据恢复：通过mysqlbinlog工具来恢复数据。 42.redo log与undo log的持久化策略？redo log持久化 缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统缓冲区( OS Buffer )。因此， redo log buffer 写入 redo logfile 实际上是先写入 OS Buffer，然后再通过系统调用 fsync() 将其刷到 redo log file. Redo Buffer 持久化到 redo log 的策略，可通过 Innodb_flush_log_at_trx_commit 设置： 参数值 含义 0 (延迟写) 事务提交时不会将 redo log buffer中日志写入到 os buffer， 而是每秒写入 os buffer并调用 fsync()写入到 redo log file中。 也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。 1 (实时写,实时刷) 事务每次提交都会将 redo log buffer中的日志写入 os buffer并 调用 fsync()刷到 redo log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。 2 (实时写, 延时刷) 每次提交都仅写入到 os buffer，然后是每秒调用 fsync()将 os buffer中的日志写入到 redo log file。 一般建议选择取值2，因为 MySQL 挂了数据没有损失，整个服务器挂了才会损失1秒的事务提交数据 undo log持久化 MySQL中的Undo Log严格的讲不是Log，而是数据，因此他的管理和落盘都跟数据是一样的： Undo的磁盘结构并不是顺序的，而是像数据一样按Page管理 Undo写入时，也像数据一样产生对应的Redo Log (因为undo也是对页面的修改，记录undo这个操作本身也会有对应的redo)。 Undo的Page也像数据一样缓存在Buffer Pool中，跟数据Page一起做LRU换入换出，以及刷脏。Undo Page的刷脏也像数据一样要等到对应的Redo Log 落盘之后 当事务提交的时候，innodb不会立即删除undo log，因为后续还可能会用到undo log，如隔离级别为repeatable read时，事务读取的都是开启事务时的最新提交行版本，只要该事务不结束，该行版本就不能删除，即undo log不能删除。 但是在事务提交的时候，会将该事务对应的undo log放入到删除列表中，未来通过purge来删除。并且提交事务时，还会判断undo log分配的页是否可以重用，如果可以重用，则会分配给后面来的事务，避免为每个独立的事务分配独立的undo log页而浪费存储空间和性能。 43.bin log与undo log的区别？1）redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 2）redo log是物理日志，记录的是“在XXX数据页上做了XXX修改”；binlog是逻辑日志，记录的是原始逻辑，其记录是对应的SQL语句。 物理日志: 记录的是每一个page页中具体存储的值是多少，在这个数据页上做了什么修改. 比如: 某个事物将系统表空间中的第100个页面中偏移量为1000处的那个字节的值1改为2. 逻辑日志: 记录的是每一个page页面中具体数据是怎么变动的，它会记录一个变动的过程或SQL语句的逻辑, 比如: 把一个page页中的一个数据从1改为2，再从2改为3,逻辑日志就会记录1-&gt;2,2-&gt;3这个数据变化的过程. 3）redo log是循环写的，空间一定会用完，需要write pos和check point搭配；binlog是追加写，写到一定大小会切换到下一个，并不会覆盖以前的日志 Redo Log 文件内容是以顺序循环的方式写入文件，写满时则回溯到第一个文件，进行覆盖写。 write pos: 表示日志当前记录的位置，当ib_logfile_4写满后，会从ib_logfile_1从头开始记录； check point: 表示将日志记录的修改写进磁盘，完成数据落盘，数据落盘后checkpoint会将日志上的相关记录擦除掉，即 write pos -&gt; checkpoint 之间的部分是redo log空着的部分，用于记录新的记录，checkpoint -&gt; write pos 之间是redo log 待落盘的数据修改记录 如果 write pos 追上 checkpoint，表示写满，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 3）Redo Log作为服务器异常宕机后事务数据自动恢复使用，Binlog可以作为主从复制和数据恢复使用。Binlog没有自动crash-safe能力 CrashSafe指MySQL服务器宕机重启后，能够保证： 所有已经提交的事务的数据仍然存在。 所有没有提交的事务的数据自动回滚。 44.MySQL的binlog有几种日志格式？分别有什么区别？binlog日志有三种模式 1）ROW（row-based replication, RBR）：日志中会记录每一行数据被修改的情况，然后在slave端对相同的数据进行修改。 优点：能清楚记录每一个行数据的修改细节，能完全实现主从数据同步和数据的恢复。而且不会出现某些特定情况下存储过程或function无法被正确复制的问题。 缺点：批量操作，会产生大量的日志，尤其是alter table会让日志量暴涨。 2）STATMENT（statement-based replication, SBR）：记录每一条修改数据的SQL语句（批量修改时，记录的不是单条SQL语句，而是批量修改的SQL语句事件）, slave在复制的时候SQL进程会解析成和原来master端执行过的相同的SQL再次执行。简称SQL语句复制。 优点：日志量小，减少磁盘IO，提升存储和恢复速度 缺点：在某些情况下会导致主从数据不一致，比如last_insert_id()、now()等函数。 3）MIXED（mixed-based replication, MBR）：以上两种模式的混合使用，一般会使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择写入模式。 企业场景如何选择binlog的模式 如果生产中使用MySQL的特殊功能相对少（存储过程、触发器、函数）。选择默认的语句模式，Statement。 如果生产中使用MySQL的特殊功能较多的，可以选择Mixed模式。 如果生产中使用MySQL的特殊功能较多，又希望数据最大化一致，此时最好Row 模式；但是要注意，该模式的binlog日志量增长非常快. 45.mysql 线上修改大表结构有哪些风险？在线修改大表的可能影响 在线修改大表的表结构执行时间往往不可预估，一般时间较长。 由于修改表结构是表级锁，因此在修改表结构时，影响表写入操作。 如果长时间的修改表结构，中途修改失败，由于修改表结构是一个事务，因此失败后会还原表结构，在这个过程中表都是锁着不可写入。 修改大表结构容易导致数据库CPU、IO等性能消耗，使MySQL服务器性能降低。 在线修改大表结构容易导致主从延时，从而影响业务读取。 修改方式： 对表加锁(表此时只读) 复制原表物理结构 修改表的物理结构 把原表数据导入中间表中 ，数据同步完后，**锁定中间表，并删除原表 rename中间表为原表 刷新数据字典，并释放锁 使用工具： online-schema-change ，是percona推出的一个针对mysql在线ddl的工具。percona是一个mysql分支维护公司，专门提供mysql技术服务的。 46.count(列名)、count(1)和 count(*)有什么区别?进行统计操作时,count中的统计条件可以三种选择: EXPLAIN &nbsp;SELECT COUNT(*) FROM user; EXPLAIN &nbsp;SELECT COUNT(列名) FROM user; EXPLAIN &nbsp;SELECT COUNT(1) FROM user; 执行效果上： count(*) 包括了所有的列,在统计时 不会忽略列值为null的数据。 count(1) 用1表示代码行,在统计时,不会忽略列值为null的数据。 count(列名)在统计时,会忽略列值为空的数据,就是说某个字段的值为null时不统计。 执行效率上： InnoDB引擎：count（字段) &lt; count(1) = count(*) InnoDB通过遍历最小的可用二级索引来处理select count(*) 语句，除非索引或优化器提示指示优化器使用不同的索引。如果二级索引不存在，则通过扫描聚集索引来处理。 InnoDB已同样的方式处理count(1)和count(*) MyISAM引擎：count（字段) &lt; count(1) &lt;= count(*) MyISAM存储了数据的准确行数，使用 count(*)会直接读取该行数， 只有当第一列定义为NOT NULL时，count（1），才会执行该操作，所以优先选择 count(*) count(列名) 会遍历整个表，但不同的是，它会先获取列，然后判断是否为空，然后累加，因此count(列名)性能不如前两者。 注意：count(*)，这是SQL92 定义的标准统计行数的语法，跟数据库无关，与NULL也无关。而count(列名) 是统计列值数量，不计NULL，相同列值算一个。 47.什么是分库分表？什么时候进行分库分表？什么是分库分表 简单来说，就是指通过某种特定的条件，将我们存放在同一个数据库中的数据分散存放到多个数据库（主机）上面，以达到分散单台设备负载的效果。 分库分表解决的问题 分库分表的目的是为了解决由于数据量过大而导致数据库性能降低的问题，将原来单体服务的数据库进行拆分.将数据大表拆分成若干数据表组成，使得单一数据库、单一数据表的数据量变小，从而达到提升数据库性能的目的。 什么情况下需要分库分表 单机存储容量遇到瓶颈. 连接数,处理能力达到上限. 注意: 分库分表之前,要根据项目的实际情况 确定我们的数据量是不是够大,并发量是不是够大,来决定是否分库分表. 数据量不够就不要分表,单表数据量超过1000万或100G的时候, 速度就会变慢(官方测试), 分库分表包括： 垂直分库、垂直分表、水平分库、水平分表 四种方式。 垂直分库 数据库中不同的表对应着不同的业务，垂直切分是指按照业务的不同将表进行分类,分布到不同的数据库上面 将数据库部署在不同服务器上，从而达到多个服务器共同分摊压力的效果 垂直分表 表中字段太多且包含大字段的时候，在查询时对数据库的IO、内存会受到影响，同时更新数据时，产生的binlog文件会很大，MySQL在主从同步时也会有延迟的风险 将一个表按照字段分成多表，每个表存储其中一部分字段。 对职位表进行垂直拆分, 将职位基本信息放在一张表, 将职位描述信息存放在另一张表 垂直拆分带来的一些提升 解决业务层面的耦合，业务清晰 能对不同业务的数据进行分级管理、维护、监控、扩展等 高并发场景下，垂直分库一定程度的提高访问性能 垂直拆分没有彻底解决单表数据量过大的问题 水平分库 将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。 水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈. 简单讲就是根据表中的数据的逻辑关系，将同一个表中的数据按照某种条件拆分到多台数据库（主机）上面, 例如将订单表 按照id是奇数还是偶数, 分别存储在不同的库中。 水平分表 针对数据量巨大的单张表（比如订单表），按照规则把一张表的数据切分到多张表里面去。 但是这些表还是在同一个库中，所以库级别的数据库操作还是有IO瓶颈。 总结 垂直分表: 将一个表按照字段分成多表，每个表存储其中一部分字段。 垂直分库: 根据表的业务不同,分别存放在不同的库中,这些库分别部署在不同的服务器. 水平分库: 把一张表的数据按照一定规则,分配到不同的数据库,每一个库只有这张表的部分数据. 水平分表: 把一张表的数据按照一定规则,分配到同一个数据库的多张表中,每个表只有这个表的部分数据. 48.说说 MySQL 的主从复制？主从复制的用途 实时灾备，用于故障切换 读写分离，提供查询服务 备份，避免影响业务 主从部署必要条件 主库开启binlog日志（设置log-bin参数） 主从server-id不同 从库服务器能连通主库 主从复制的原理 Mysql 中有一种日志叫做 bin 日志（二进制日志）。这个日志会记录下所有修改了数据库的SQL 语句（insert,update,delete,create/alter/drop table, grant 等等）。 主从复制的原理其实就是把主服务器上的 bin 日志复制到从服务器上执行一遍，这样从服务器上的数据就和主服务器上的数据相同了。 主库db的更新事件(update、insert、delete)被写到binlog 主库创建一个binlog dump thread，把binlog的内容发送到从库 从库启动并发起连接，连接到主库 从库启动之后，创建一个I/O线程，读取主库传过来的binlog内容并写入到relay log 从库启动之后，创建一个SQL线程，从relay log里面读取内容，执行读取到的更新事件，将更新内容写入到slave的db 49. 说一下 MySQL 执行一条查询语句的内部执行过程？ ①建立连接（Connectors&amp;Connection Pool），通过客户端/服务器通信协议与MySQL建立连接。MySQL 客户端与服务端的通信方式是 “ 半双工 ”。对于每一个 MySQL 的连接，时刻都有一个线程状态来标识这个连接正在做什么。通讯机制： 全双工：能同时发送和接收数据，例如平时打电话。 半双工：指的某一时刻，要么发送数据，要么接收数据，不能同时。例如早期对讲机 单工：只能发送数据或只能接收数据。例如单行道 线程状态：show processlist; //查看用户正在运行的线程信息，root用户能查看所有线程，其他用户只能看自己的 id：线程ID，可以使用kill xx； user：启动这个线程的用户 Host：发送请求的客户端的IP和端口号 db：当前命令在哪个库执行 Command：该线程正在执行的操作命令 Create DB：正在创建库操作 Drop DB：正在删除库操作 Execute：正在执行一个PreparedStatement Close Stmt：正在关闭一个PreparedStatement Query：正在执行一个语句 Sleep：正在等待客户端发送语句 Quit：正在退出 Shutdown：正在关闭服务器 Time：表示该线程处于当前状态的时间，单位是秒 State：线程状态 Updating：正在搜索匹配记录，进行修改 Sleeping：正在等待客户端发送新请求 Starting：正在执行请求处理 Checking table：正在检查数据表 Closing table : 正在将表中数据刷新到磁盘中 Locked：被其他查询锁住了记录 Sending Data：正在处理Select查询，同时将结果发送给客户端 Info：一般记录线程执行的语句，默认显示前100个字符。想查看完整的使用show full processlist; ②查询缓存（Cache&amp;Buffer），这是MySQL的一个可优化查询的地方，如果开启了查询缓存且在查询缓存过程中查询到完全相同的SQL语句，则将查询结果直接返回给客户端；如果没有开启查询缓存或者没有查询到完全相同的 SQL 语句则会由解析器进行语法语义解析，并生成“解析树”。 缓存Select查询的结果和SQL语句 执行Select查询时，先查询缓存，判断是否存在可用的记录集，要求是否完全相同（包括参数值），这样才会匹配缓存数据命中。 即使开启查询缓存，以下SQL也不能缓存 查询语句使用SQL_NO_CACHE 查询的结果大于query_cache_limit设置 查询中有一些不确定的参数，比如now() show variables like ‘%query_cache%’; //查看查询缓存是否启用，空间大小，限制等 show status like ‘Qcache%’; //查看更详细的缓存参数，可用缓存空间，缓存块，缓存多少等 ③解析器（Parser）将客户端发送的SQL进行语法解析，生成”解析树”。预处理器根据一些MySQL规则进一步检查“解析树”是否合法，例如这里将检查数据表和数据列是否存在，还会解析名字和别名，看看它们是否有歧义，最后生成新的“解析树”。 ④查询优化器（Optimizer）根据“解析树”生成最优的执行计划。MySQL使用很多优化策略生成最优的执行计划，可以分为两类：静态优化（编译时优化）、动态优化（运行时优化）。 等价变换策略 5=5 and a&gt;5 改成 a &gt; 5 a &lt; b and a=5 改成b&gt;5 and a=5 基于联合索引，调整条件位置等 优化count、min、max等函数 InnoDB引擎min函数只需要找索引最左边 InnoDB引擎max函数只需要找索引最右边 MyISAM引擎count(*)，不需要计算，直接返回 提前终止查询 使用了limit查询，获取limit所需的数据，就不在继续遍历后面数据 in的优化 MySQL对in查询，会先进行排序，再采用二分法查找数据。比如where id in (2,1,3)，变成 in (1,2,3) ⑤查询执行引擎负责执行 SQL 语句，此时查询执行引擎会根据 SQL 语句中表的存储引擎类型，以及对应的API接口与底层存储引擎缓存或者物理文件的交互，得到查询结果并返回给客户端。若开启用查询缓存，这时会将SQL 语句和结果完整地保存到查询缓存（Cache&amp;Buffer）中，以后若有相同的 SQL 语句执行则直接返回结果。 如果开启了查询缓存，先将查询结果做缓存操作 返回结果过多，采用增量模式返回 50.Mysql内部支持缓存查询吗？使用缓存的好处：当MySQL接收到客户端的查询SQL之后，仅仅只需要对其进行相应的权限验证之后，就会通过Query Cache来查找结果，甚至都不需要经过Optimizer模块进行执行计划的分析优化，更不需要发生任何存储引擎的交互. mysql5.7支持内部缓存，8.0之后已废弃 mysql缓存的限制 mysql基本没有手段灵活的管理缓存失效和生效，尤其对于频繁更新的表 SQL必须完全一致才会导致cache命中 为了节省内存空间，太大的result set不会被cache (&lt; query_cache_limit)； MySQL缓存在分库分表环境下是不起作用的； 执行SQL里有触发器,自定义函数时，MySQL缓存也是不起作用的； 在表的结构或数据发生改变时，基于该表相关cache立即全部失效。 替代方案 应用层组织缓存，最简单的是使用redis，ehcached等","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}]},{"title":"redis基础","slug":"31-redis基础","date":"2023-09-11T06:58:37.000Z","updated":"2023-09-11T10:11:09.201Z","comments":true,"path":"posts/31.html","link":"","permalink":"http://example.com/posts/31.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) Redis为什么快？ 纯内存KV操作 Redis的操作都是基于内存的，CPU不是 Redis性能瓶颈,，Redis的瓶颈是机器内存和网络带宽。 在计算机的世界中，CPU的速度是远大于内存的速度的，同时内存的速度也是远大于硬盘的速度。redis的操作都是基于内存的，绝大部分请求是纯粹的内存操作，非常迅速。 单线程操作 使用单线程可以省去多线程时CPU上下文会切换的时间，也不用去考虑各种锁的问题，不存在加锁释放锁操作，没有死锁问题导致的性能消耗。对于内存系统来说，多次读写都是在一个CPU上，没有上下文切换效率就是最高的！既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章的采用单线程的方案了 Redis 单线程指的是网络请求模块使用了一个线程，即一个线程处理所有网络请求，其他模块该使用多线程，仍会使用了多个线程。 I/O 多路复用 为什么 Redis 中要使用 I/O 多路复用这种技术呢？ 首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的 Reactor 设计模式 Redis基于Reactor模式开发了自己的网络事件处理器，称之为文件事件处理器(File Event Hanlder)。 Redis合适的应用场景？1、会话缓存（Session Cache）最常用的一种使用 Redis 的情景是会话缓存（sessioncache），用 Redis 缓存会话比其他存储（如Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 2、全页缓存（FPC）除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地FPC。再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 3、队列Reids 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop操作。如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用Redis 作为 broker，你可以从这里去查看。 4、排行榜/计数器Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（SortedSet）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据结构。微信搜索公众号：Java专栏，获取最新面试手册所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可：当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：ZRANGE user_scores 0 10 WITHSCORESAgora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。 5、发布/订阅最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！ Redis6.0之前为什么一直不使用多线程？官方曾做过类似问题的回复：使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。 使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。Redis通过AE事件模型以及IO多路复用等技术，处理性能非常高，因此没有必要使用多线程。单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等,“线程不安全” 的命令都可以无锁进行。 Redis6.0为什么要引入多线程？Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理80,000到100,000 QPS，这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。 但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的QPS。常见的解决方案是在分布式架构中对数据进行分区并采用多个服务器，但该方案有非常大的缺点，例如要管理的Redis服务器太多，维护代价大；某些适用于单个Redis服务器的命令不适用于数据分区；数据分区无法解决热点读/写问题；数据偏斜，重新分配和放大/缩小变得更加复杂等等。 Redis有哪些高级功能？消息队列、自动过期删除、事务、数据持久化、分布式锁、附近的人、慢查询分析、Sentinel和集群等多项功能。 为什么要用Redis？使用缓存的目的就是提升读写性能。而实际业务场景下，更多的是为了提升读性能，带来更好的性能，带来更高的并发量。Redis的读写性能比Mysql好的多，我们就可以把Mysql中的热点数据缓存到Redis中，提升读取性能，同时也减轻了Mysql的读取压力 Redis与memcached相对有哪些优势？(1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 (2) redis的速度比memcached快很多 (3) redis可以持久化其数据 (4)Redis支持数据的备份，即master-slave模式的数据备份。 (5) 使用底层模型不同，它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 (6）value大小：redis最大可以达到1GB，而memcache只有1MB 怎么理解Redis中事务？事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行，事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。不过Redis的是弱事物。 事务是Redis实现在服务器端的行为，用户执行MULTI命令时，服务器会将对应这个用户的客户端对象设置为一个特殊的状态，在这个状态下后续用户执行的查询命令不会被真的执行，而是被服务器缓存起来，直到用户执行EXEC命令为止，服务器会将这个用户对应的客户端对象中缓存的命令按照提交的顺序依次执行。 Redis提供了简单的事务，之所以说它简单，主要是因为它不支持事务中的回滚特性,同时无法实现命令之间的逻辑关系计算，当然也体现了Redis 的“keep it simple”的特性 为什么要使用pipelinePipeline（流水线)机制能改善上面这类问题,它能将一组 Redis命令进行组装,通过一次RTT传输给Redis,再将这组Redis命令的执行结果按顺序返回给客户端,没有使用Pipeline执行了n条命令,整个过程需要n次RTT。 使用Pipeline 执行了n次命令，整个过程需要1次RTT。 Redis的过期策略以及内存淘汰机制？​ redis采用的是定期删除+惰性删除策略。 为什么不用定时删除策略? 定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略. 定期删除+惰性删除是如何工作的呢? 定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。 于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除 ​ 1.noeviction:返回错误当内存限制达到，并且客户端尝试执行会让更多内存被使用的命令。 ​ 2.allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。 ​ 3.volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。 ​ 4.allkeys-random: 回收随机的键使得新添加的数据有空间存放。 ​ 5.volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。 ​ 6.volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 什么是缓存穿透？如何避免？缓存穿透：指查询一个一定不存在的数据，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到 DB 去查询，可能导致 DB 挂掉。 解决方案：1.查询返回的数据为空，仍把这个空结果进行缓存，但过期时间会比较短；2.布隆过滤器：将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对 DB 的查询。 什么是缓存雪崩？如何避免？缓存雪崩：设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 DB，DB 瞬时压力过重雪崩。与缓存击穿的区别：雪崩是很多 key，击穿是某一个key 缓存。 解决方案：将缓存失效时间分散开，比如可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 使用Redis如何设计分布式锁？基于 Redis 实现的分布式锁，一个严谨的的流程如下： 1、加锁 SET lock_key $unique_id EX $expire_time NX 2、操作共享资源 3、释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再DEL 释放锁 怎么使用Redis实现消息队列？基于List的 LPUSH+BRPOP 的实现 足够简单，消费消息延迟几乎为零，但是需要处理空闲连接的问题。 如果线程一直阻塞在那里，Redis客户端的连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用，这个时候blpop和brpop或抛出异常，所以在编写客户端消费者的时候要小心，如果捕获到异常，还有重试。 其他缺点包括： 做消费者确认ACK麻烦，不能保证消费者消费消息后是否成功处理的问题（宕机或处理异常等），通常需要维护一个Pending列表，保证消息处理确认；不能做广播模式，如pub/sub，消息发布/订阅模型；不能重复消费，一旦消费就会被删除；不支持分组消费。 基于Sorted-Set的实现 多用来实现延迟队列，当然也可以实现有序的普通的消息队列，但是消费者无法阻塞的获取消息，只能轮询，不允许重复消息。 PUB/SUB，订阅/发布模式 优点： 典型的广播模式，一个消息可以发布到多个消费者；多信道订阅，消费者可以同时订阅多个信道，从而接收多类消息；消息即时发送，消息不用等待消费者读取，消费者会自动接收到信道发布的消息。 缺点： 消息一旦发布，不能接收。换句话就是发布时若客户端不在线，则消息丢失，不能寻回；不能保证每个消费者接收的时间是一致的；若消费者客户端出现消息积压，到一定程度，会被强制断开，导致消息意外丢失。通常发生在消息的生产远大于消费速度时；可见，Pub/Sub 模式不适合做消息存储，消息积压类的业务，而是擅长处理广播，即时通讯，即时反馈的业务。 基于Stream类型的实现 基本上已经有了一个消息中间件的雏形，可以考虑在生产过程中使用。 什么是bigkey？会有什么影响？bigkey是指key对应的value所占的内存空间比较大，例如一个字符串类型的value可以最大存到512MB，一个列表类型的value最多可以存储23-1个元素。 如果按照数据结构来细分的话，一般分为字符串类型bigkey和非字符串类型bigkey。 字符串类型：体现在单个value值很大，一般认为超过10KB就是bigkey，但这个值和具体的OPS相关。 非字符串类型：哈希、列表、集合、有序集合,体现在元素个数过多。 bigkey无论是空间复杂度和时间复杂度都不太友好，下面我们将介绍它的危害。 bigkey的危害 bigkey的危害体现在三个方面: 内存空间不均匀.(平衡):例如在Redis Cluster中，bigkey 会造成节点的内存空间使用不均匀。 超时阻塞:由于Redis单线程的特性，操作bigkey比较耗时，也就意味着阻塞Redis可能性增大。 网络拥塞:每次获取bigkey产生的网络流量较大 假设一个bigkey为1MB，每秒访问量为1000，那么每秒产生1000MB 的流量,对于普通的千兆网卡(按照字节算是128MB/s)的服务器来说简直是灭顶之灾，而且一般服务器会采用单机多实例的方式来部署,也就是说一个bigkey可能会对其他实例造成影响,其后果不堪设想。 Redis如何解决key冲突？遇到hash冲突采用链表进行处理 怎么提高缓存命中率？需要在业务需求，缓存粒度，缓存策略，技术选型等各个方面去通盘考虑并做权衡。尽可能的聚焦在高频访问且时效性要求不高的热点业务上，通过缓存预加载（预热）、增加存储容量、调整缓存粒度、更新缓存等手段来提高命中率。 Redis持久化方式有哪些？有什么区别？RDB、AOF、混合持久化。 RDB的优缺点： 优点：RDB持久化文件，速度比较快，而且存储的是一个二进制文件，传输起来很方便。 缺点：RDB无法保证数据的绝对安全，有时候就是1s也会有很大的数据丢失。 AOF的优缺点： 优点：AOF相对RDB更加安全，一般不会有数据的丢失或者很少，官方推荐同时开启AOF和RDB。 缺点：AOF持久化的速度，相对于RDB较慢，存储的是一个文本文件，到了后期文件会比较大，传输困难。 混合持久化的优缺点: 优点 结合了RDB和AOF两种方式的优点，能在较短的时间内基本恢复全量的数据。 缺点 同样存在可读性差、重写阻塞的问题。 为什么Redis需要把所有数据放到内存中？Redis为了达到最快的读写速度,将数据都读到内存中,并通过异步的方式将数据写入磁盘,所以Redis具有快速和数据持久化的特征。如果不将数据放在内存中,磁盘I/O速度为严重影响Redis的性能。 如何保证缓存与数据库双写时的数据一致性第一种方案：采用延时双删策略 具体的步骤就是： 先删除缓存； 再写数据库； 休眠500毫秒； 再次删除缓存。 第二种方案：异步更新缓存(基于订阅binlog的同步机制) 技术整体思路： MySQL binlog增量订阅消费+消息队列+增量数据更新到redis Redis集群方案应该怎么做？ Redis Sentinel体量较小时，选择 Redis Sentinel，单主 Redis 足以支撑业务。 Redis ClusterRedis 官方提供的集群化方案，体量较大时，选择 Redis Cluster，通过分片，使用更多内存。 TwemproxTwemprox是Twtter开源的一个 Redis和 Memcached 代理服务器，主要用于管理 Redis和Memcached 集群，减少与Cache 服务器直接连接的数量。 CodisCodis是一个代理中间件，当客户端向Codis发送指令时，Codis负责将指令转发到后面的Redis来执行，并将结果返回给客户端。一个Codis实例可以连接多个Redis实例，也可以启动多个Codis实例来支撑，每个Codis节点都是对等的，这样可以增加整体的QPS需求，还能起到容灾功能。 客户端分片在Redis Cluster还没出现之前使用较多，现在基本很少热你使用了，在业务代码层实现，起几个毫无关联的Redis实例，在代码层，对 Key 进行 hash 计算，然后去对应的 Redis实例操作数据。这种方式对 hash 层代码要求比较高，考虑部分包括，节点失效后的替代算法方案，数据震荡后的自动脚本恢复，实例的监控，等等。 Redis集群方案什么情况下会导致整个集群不可用？1.当访问一个 Master 和 Slave 节点都挂了的槽的时候，会报槽无法获取。 2.当集群 Master 节点个数小于 3 个的时候，或者集群可用节点个数为偶数的时候，基于 fail 的这种选举机制的自动主从切换过程可能会不能正常工作，一个是标记 fail 的过程，一个是选举新的 master 的过程，都有可能异常。 Redis哈希槽的概念slot：称为哈希槽 Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； Redis集群会有写操作丢失吗？为什么？以下情况可能导致写操作丢失： 过期 key 被清理 最大内存不足，导致 Redis 自动清理部分 key 以节省空间 主库故障后自动重启，从库自动同步 单独的主备方案，网络不稳定触发哨兵的自动切换主从节点，切换期间会有数据丢失 Redis常见性能问题和解决方案有哪些？一、缓存穿透：就是查询一个压根就不存在的数据，即缓存中没有，数据库中也没有 解决方案：使用布隆过滤器，把数据先加载到布隆过滤器中，访问前先判断是否存在于布隆过滤器中，不存在代表这笔数据压根就不存在。 缺点：布隆过滤器是不可变的，可能一开始过滤器和数据库数据时一致的，后面数据库数据变了，或变多或变少，而对应的布隆过滤器的数据也要改变，这时会比较麻烦。 二、缓存击穿：数据库中有，缓存中没有。缓存击穿实际就是一个并发问题，一般来说查询数据，先查询缓存，有直接返回，没有再查询数据库并放到缓存中之后返回，但这种场景在并发情况下就会有问题，假设同时又100个请求执行上面 逻辑的代码，则可能会出现多个请求都查询数据库，因为大家同时执行，都查到了缓存中没有数据。 解决方案：加锁。如果是单机部署，则可以使用JVM级别的锁，如lock、synchronized。如果是集群部署，则需要使用分布式锁，如基于redis、zookeeper、mysql等实现的分布式锁。 三、缓存雪崩：大部分数据同时失效、过期，新的缓存又没来，导致大量的请求都去访问数据库而导致的服务器压力过大、宕机、系统崩溃。 解决方案：搭建高可用的redis集群，避免压力集中于一个节点；缓存失效时间错开，避免缓存同时失效而都去请求数据库。 热点数据和冷数据是什么热数据：半年以内被频繁访问的在线类数据，对访问频率和响应速度要求较高，可以部署在 CPU 驱动器附近，以便就近计算； 冷数据：非频繁访问的离线类数据，对访问频率和响应速度要求较低，可以集中化部署，保存在硬盘或远离数据中心的驱动器上 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不 大。频繁修改的数据，看情况考虑使用缓存 对于上面两个例子，寿星列表、导航信息都存在一个特点，就是信息修改频率不高，读取通常非常高的 场景。 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。 再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。 什么情况下可能会导致Redis阻塞数据集中过期 不合理地使用API或数据结构 CPU饱和 持久化阻塞 什么时候选择Redis，什么时候选择Memcached？实际业务分析 如果业务中更加侧重性能的⾼效性，对持久化要求不⾼，那么应该优先选择 Memcached。 如果业务中对持久化有需求或者对数据涉及到存储、排序等一系列复杂的操作，比如业务中有排⾏榜类应⽤、社交关系存储、数据排重、实时配置等功能，那么应该优先选择 Redis Redis过期策略都有哪些？LRU算法知道吗？ noeviction:返回错误当内存限制达到，并且客户端尝试执行会让更多内存被使用的命令。 allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。 volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。 allkeys-random: 回收随机的键使得新添加的数据有空间存放。 volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。 volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。","categories":[{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}]},{"title":"MQ消息队列","slug":"29-MQ消息队列","date":"2023-09-11T06:58:13.000Z","updated":"2023-09-11T10:11:46.787Z","comments":true,"path":"posts/29.html","link":"","permalink":"http://example.com/posts/29.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 1、为什么使用消息队列？其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用MQ可能会很麻烦，但是你现在用了MQ之后带给了你很多的好处。消息队列的常见使用场景，其实场景有很多，但是比较核心的有3个：解耦、异步、削峰。 解耦： A系统发送个数据到BCD三个系统，接口调用发送，那如果E系统也要这个数据呢？那如果C系统现在不需要了呢？现在A系统又要发送第二种数据了呢？而且A系统要时时刻刻考虑BCDE四个系统如果挂了咋办？要不要重发？我要不要把消息存起来？ 你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用MQ给他异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个MQ去进行系统的解耦。 异步： A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要30ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是30 + 300 + 450 + 200 = 980ms，接近1s，异步后，BCD三个系统分别写库的时间，A系统就不再考虑了。 削峰： 每天0点到16点，A系统风平浪静，每秒并发请求数量就100个。结果每次一到16点~23点，每秒并发请求数量突然会暴增到1万条。但是系统最大的处理能力就只能是每秒钟处理1000个请求啊。怎么办？需要我们进行流量的削峰，让系统可以平缓的处理突增的请求。 2、消息队列有什么优点和缺点?优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点: 系统可用性降低 系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统的接口就好了，ABCD四个系统好好的，没啥问题，你偏加个MQ进来，万一MQ挂了怎么办？MQ挂了，整套系统崩溃了，业务也就停顿了。 系统复杂性提高 硬生生加个MQ进来，怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？ 一致性问题 A系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是BCD三个系统那里，BD两个系统写库成功了，结果C系统写库失败了，你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉。 3、常见消息队列的比较 4、Kafka的特性 消息持久化 高吞吐量 扩展性 多客户端支持 Kafka Streams 5、RabbitMQ中的vhost起什么作用？虚拟消息服务器，vhost，本质上就是一个mini版的mq服务器，有自己的队列、交换器和绑定，最重要的，自己的权限机制。Vhost提供了逻辑上的分离，可以将众多客户端进行区分，又可以避免队列和交换器的命名冲突。Vhost必须在连接时指定，rabbitmq包含缺省vhost：“/”，通过缺省用户和口令guest进行访问。 rabbitmq里创建用户，必须要被指派给至少一个vhost，并且只能访问被指派内的队列、交换器和绑定。Vhost必须通过rabbitmq的管理控制工具创建。 6、RabbitMQ上的一个queue中存放的message是否有数量限制？限制是多少默认情况下一般是无限制，因为限制取决于机器的内存，但是消息过多会导致处理效率的下降。 可以通过参数来限制， x-max-length ：对队列中消息的条数进行限制 ， x-max-length-bytes ：对队列中消息的总量进行限制 7、说一说Kafka你熟悉的参数？必选属性 创建生产者对象时有三个属性必须指定。 bootstrap.servers 该属性指定broker的地址清单，地址的格式为host:port。清单里不需要包含所有的broker地址，生产者会从给定的broker里查询其他broker的信息。不过最少提供2个broker的信息(用逗号分隔，比如: 127.0.0.1:9092,192.168.0.13:9092)，一旦其中一个宕机，生产者仍能连接到集群上。 key.serializer 生产者接口允许使用参数化类型，可以把Java对象作为键和值传broker，但是broker希望收到的消息的键和值都是字节数组，所以，必须提供将对象序列化成字节数组的序列化器。key.serializer必须设置为实现org.apache.kafka.common.serialization.Serializer的接口类，Kafka的客户端默认提供了ByteArraySerializer,IntegerSerializer, StringSerializer，也可以实现自定义的序列化器。 value.serializer 同 key.serializer。 acks：​ Kafk内部的复制机制是比较复杂的，这里不谈论内部机制（后续章节进行细讲），我们只讨论生产者发送消息时与副本的关系。 ​ 指定了必须要有多少个分区副本收到消息，生产者才会认为写入消息是成功的，这个参数对消息丢失的可能性有重大影响。 ​ acks=0：生产者在写入消息之前不会等待任 何来自服务器的响应，容易丢消息，但是吞吐量高。 ​ acks=1：只要集群的首领节点收到消息，生产者会收到来自服务器的成功响应。如果消息无法到达首领节点（比如首领节点崩溃，新首领没有选举出来），生产者会收到一个错误响应，为了避免数据丢失，生产者会重发消息。不过，如果一个没有收到消息的节点成为新首领，消息还是会丢失。默认使用这个配置。 ​ acks=all：只有当所有参与复制的节点都收到消息，生产者才会收到一个来自服务器的成功响应。延迟高。 ​ 金融业务，主备外加异地灾备。所以很多高可用场景一般不是设置2个副本，有可能达到5个副本，不同机架上部署不同的副本，异地上也部署一套副本。 ​ buffer.memory ​ 设置生产者内存缓冲区的大小（结合生产者发送消息的基本流程），生产者用它缓冲要发送到服务器的消息。如果数据产生速度大于向broker发送的速度，导致生产者空间不足，producer会阻塞或者抛出异常。缺省33554432 (32M) ​ max.block.ms ​ 指定了在调用send()方法或者使用partitionsFor()方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到max.block.ms时，生产者会抛出超时异常。缺省60000ms ​ retries ​ 发送失败时，指定生产者可以重发消息的次数（缺省Integer.MAX_VALUE）。默认情况下，生产者在每次重试之间等待100ms，可以通过参数retry.backoff.ms参数来改变这个时间间隔。 ​ receive.buffer.bytes和send.buffer.bytes ​ 指定TCP socket接受和发送数据包的缓存区大小。如果它们被设置为-1，则使用操作系统的默认值。如果生产者或消费者处在不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。缺省102400 ​ batch.size ​ 当多个消息被发送同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。当批次内存被填满后，批次里的所有消息会被发送出去。但是生产者不一定都会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。缺省16384(16k) ，如果一条消息超过了批次的大小，会写不进去。 ​ linger.ms ​ 指定了生产者在发送批次前等待更多消息加入批次的时间。它和batch.size以先到者为先。也就是说，一旦我们获得消息的数量够batch.size的数量了，他将会立即发送而不顾这项设置，然而如果我们获得消息字节数比batch.size设置要小的多，我们需要“linger”特定的时间以获取更多的消息。这个设置默认为0，即没有延迟。设定linger.ms=5，例如，将会减少请求数目，但是同时会增加5ms的延迟，但也会提升消息的吞吐量。 ​ compression.type ​ producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，压缩性能越好。snappy占用cpu少，提供较好的性能和可观的压缩比，如果比较关注性能和网络带宽，用这个。如果带宽紧张，用gzip，会占用较多的cpu，但提供更高的压缩比。 ​ client.id ​ 当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪。 ​ max.in.flight.requests.per.connection ​ 指定了生产者在接收到服务器响应之前可以发送多个消息，值越高，占用的内存越大，当然也可以提升吞吐量。发生错误时，可能会造成数据的发送顺序改变,默认是5 (修改）。 ​ 如果需要保证消息在一个分区上的严格顺序，这个值应该设为1。不过这样会严重影响生产者的吞吐量。 ​ request.timeout.ms ​ 客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常，默认30秒。 ​ metadata.fetch.timeout.ms ​ 是指我们所获取的一些元数据的第一个时间数据。元数据包含：topic，host，partitions。此项配置是指当等待元数据fetch成功完成所需要的时间，否则会跑出异常给客户端 ​ max.request.size ​ 控制生产者发送请求最大大小。默认这个值为1M，如果一个请求里只有一个消息，那这个消息不能大于1M，如果一次请求是一个批次，该批次包含了1000条消息，那么每个消息不能大于1KB。注意：broker具有自己对消息记录尺寸的覆盖，如果这个尺寸小于生产者的这个设置，会导致消息被拒绝。这个参数和Kafka主机的message.max.bytes 参数有关系。如果生产者发送的消息超过message.max.bytes设置的大小，就会被Kafka服务器拒绝。 ​ 以上参数不用去，一般来说，就记住acks、batch.size、linger.ms、max.request.size就行了，因为这4个参数重要些，其他参数一般没有太大必要调整。 8、kafka中，可以不用zookeeper么?新版本的kafka可以不用，3.0以上可以使用Kafka with Kraft，就可以完全抛弃zookeeper 9、 说一说RabbitMQ中的AMQPRabbitMQ就是 AMQP 协议的 Erlang 的实现(当然 RabbitMQ 还支持 STOMP2、 MQTT3 等协议 ) AMQP 的模型架构 和 RabbitMQ 的模型架构是一样的，生产者将消息发送给交换器，交换器和队列绑定 。RabbitMQ 中的交换器、交换器类型、队列、绑定、路由键等都是遵循的 AMQP 协议中相 应的概念。 10、RabbitMQ开启持久化机制，有什么要注意的点？1、效率变低 2、开启持久化需要交换器、队列、消息三者都需要持久化 11、kafka适合哪些场景？Kafka是由LinkedIn开发的一个分布式基于发布/订阅的消息系统，使用Scala编写，它以可水平扩展和高吞吐率而被广泛使用 1. 消息系统Kafka被当作传统消息中间件的替代品。与大多数消息系统相比，Kafka具有更好的吞吐量，内置的分区，多副本和容错性，这使其成为大规模消息处理应用程序的良好解决方案。 在我们的经验中，消息的使用通常是相对较低的吞吐量，但可能需要较低的端到端延迟，并且通常需要强大的持久性保证，这些Kafka都能提供。 在这些要点中，Kafka可与传统消息系统（如ActiveMQ或RabbitMQ）媲美。 消息队列在实际应用中常用的使用场景：异步处理，应用解耦，流量削锋和消息通讯四个场景。 2. 网站行为跟踪Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。 3. 指标用Kafka采集应用程序和服务器健康相关的指标，如CPU占用率、IO、内存、连接数、TPS、QPS等，然后将指标信息进行处理，从而构建一个具有监控仪表盘、曲线图等可视化监控系统。例如，很多公司采用Kafka与ELK（ElasticSearch、Logstash和Kibana）整合构建应用服务监控系统。 4. 日志聚合其实开源产品有很多，包括Scribe、Apache Flume,很多人使用Kafka代替日志聚合（log aggregation）。 日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和副本机制确保了更强的耐用性保，并且端到端延迟更低。 5. 流处理保存收集流数据，以提供之后对接的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行 阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。 例如一个文章推荐的处理流程，可能是先从RSS数据源中抓取文章的内 容，然后将其丢入一个叫做“文章”的topic中，后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返还给用户。 从0.10.0.0版本开始，Apache Kafka提供了一个名为Kafka Streams的轻量级，但功能强大的流处理库，可执行如上所述的数据处理。除了Kafka Streams之外，替代开源流处理工具还包括Apache Storm和Apache Samza。 6. 事件源事件源是一种应用程序设计的方式，该方式的状态转移被记录为按时间顺序排序的记录序列。Kafka可以存储大量的日志数据，这使得它成为一个对这种方式的应用来说绝佳的后台。比如动态汇总（News feed）。 不适合使用Kafka的场景— 12、RabbitMQ中交换器4种类型？主要有以下4种。 ​ fanout: 把所有发送到该交换器的消息路由到所有与该交换器绑定的队列中。 ​ direct:把消息路由到BindingKey和RoutingKey完全匹配的队列中。 ​ topic: 匹配规则： RoutingKey 为一个 点号’.’: 分隔的字符串。比如: java.xiaoka.show BindingKey和RoutingKey一样也是点号“.“分隔的字符串。 BindingKey可使用 * 和 # 用于做模糊匹配，*匹配一个单词，#匹配多个或者0个 ​ headers:不依赖路由键匹配规则路由消息。是根据发送消息内容中的headers属性进行匹配。性能差，基本用不到。 13、为什么Kafka不支持读写分离？1、数据一致性问题: 数据从主节点转到从节点，必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。 2、延时问题：Kafka追求高性能，如果走主从复制，延时严重 14、Kafka中是怎么做到消息顺序性的？一个 topic，一个 partition，一个 consumer，内部单线程消费。 生产者在发送消息的时候指定要发送到特定Partition(分区) 将 producer 发送的数据封装成一个 ProducerRecord 对象。 （1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值； （2）没有指明 partition 值但有 key 的情况下，在Producer往Kafka插入数据时，控制同一Key分发到同一Partition，并且设置参数max.in.flight.requests.per.connection=1，也即同一个链接只能发送一条消息，如此便可严格保证Kafka消息的顺序 15、Kafka为什么那么快？ 利用 Partition 实现并行处理 顺序写磁盘 充分利用 Page Cache 零拷贝技术 批处理 数据压缩 16、如何解决重复消费？消息被重复消费，就是消费方多次接受到了同一条消息。根本原因就是，第一次消费完之后，消费方给 MQ 确认已消费的反馈，MQ 没有成功接受。比如网络原因、MQ 重启等。所以 MQ 是无法保证消息不被重复消费的，只能业务系统层面考虑。不被重复消费的问题，就被转化为消息消费的幂等性的问题。幂等性就是指一次和多次请求的结果一致，多次请求不会产生副作用。保证消息消费的幂等性可以考虑下面的方式：给消息生成全局 id，消费成功过的消息可以直接丢弃消息中保存业务数据的主键字段，结合业务系统需求场景进行处理，避免多次插入、是否可以根据主键多次更新而并不影响结果等 17、Rocketmq如何保证高可用性？1、架构层面 避免用单节点或者简单的一主一从架构，可以采取多主从的架构，并且主从之间采用同步复制的方式进行数据双写。 2、刷盘策略 RocketMQ默认的异步刷盘，可以改成同步刷盘SYNC_FLUSH。 3、生产消息的高可用 当消息发送失败了，在消息重试的时候，会尽量规避上一次发送的 Broker，选择还没推送过该消息的Broker，以增大消息发送的成功率。 4、消费消息的高可用 消费者获取到消息之后，可以等到整个业务处理完成，再进行CONSUME_SUCCESS状态确认，如果业务处理过程中发生了异常那么就会触发broker的重试机制。 18、RocketMq的存储机制消息生产者发送消息到broker，都是会按照顺序存储在CommitLog文件中，每个commitLog文件的大小为1G CommitLog-存储所有的消息元数据，包括Topic、QueueId以及message CosumerQueue-消费逻辑队列：存储消息在CommitLog的offset IndexFile-索引文件：存储消息的key和时间戳等信息，使得RocketMq可以采用key和时间区间来查询消息 也就是说，rocketMq将消息均存储在CommitLog中，并分别提供了CosumerQueue和IndexFile两个索引，来快速检索消息 19、RocketMq性能比较高的原因？1.顺序写 顺序写比随机写的性能会高很多，不会有大量寻址的过程 2.异步刷盘 相比较于同步刷盘，异步刷盘的性能会高很多 3.零拷贝 使用mmap的方式进行零拷贝，提高了数据传输的效率 20、让你来设计一个消息队列，你会怎么设计？数据存储角度： ​ 理论上，从速度来看，分布式文件系统&gt;分布式KV（持久化）&gt;数据库，而可靠性却截然相反，如果追求性能可以基于文件系统的顺序写。 高可用角度： ​ 分区+复制+选举的思想 网络框架角度： ​ 选用高效的Netty框架，producer 同步异步发送消息，consumer 同步异步接收消息。同步能够保证结果，异步能够保证性能。 21、有几百万消息持续积压几小时，说说怎么解决？发生了线上故障，几千万条数据在MQ里积压很久。是修复consumer的问题，让他恢复消费速度，然后等待几个小时消费完毕？这是个解决方案。不过有时候我们还会进行临时紧急扩容。 一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条。1000多万条，所以如果积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来。 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停掉。 新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量。然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue。 接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据。 这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据。 等快速消费完积压数据之后，再恢复原先部署架构，重新用原先的consumer机器来消费消息。 22、Rocketmq中Broker的部署方式1.单台 Master 部署; 2.多台 Master部署 3.多台主从部署 23、Rocketmq中Broker的刷盘策略有哪些？同步刷盘 SYNC_FLUSH（同步刷盘）：生产者发送的每一条消息都在保存到磁盘成功后才返回告诉生产者成功。这种方式不会存在消息丢失的问题，但是有很大的磁盘IO开销，性能有一定影响。 异步刷盘 ASYNC_FLUSH（异步刷盘）：生产者发送的每一条消息并不是立即保存到磁盘，而是暂时缓存起来，然后就返回生产者成功。随后再异步的将缓存数据保存到磁盘，有两种情况：1是定期将缓存中更新的数据进行刷盘，2是当缓存中更新的数据条数达到某一设定值后进行刷盘。这种异步的方式会存在消息丢失（在还未来得及同步到磁盘的时候宕机），但是性能很好。默认是这种模式。 24、什么是路由注册？RocketMQ如何进行路由注册？RocketMQ的路由注册是通过broker向NameServer发送心跳包实现的，首先borker每隔30s向nameserver发送心跳语句，nameserver处理 25、什么是路由发现？RocketMQ如何进行路由发现？RocketMQ的路由发现不是实时的，NameServer不会主动向客户端推送，而是客户端定时拉取主题最新的路由，然后更新。 step1：调用RouterInfoManager的方法，从路由表topicQueueTable、brokerAddrTable、filterServerTable分别填充信息； step2：如果主题对应的消息为顺序消息，则从NameServerKVconfig中获取关于顺序消息相关的配置填充路由信息； 26、什么是路由剔除？RocketMQ如何进行路由剔除？路由删除有两个触发节点： 1）NameServer定时扫描brokerLiveTable检测上次心跳包与当前系统时间的时间差，如果大于120S，就需要删除； 2）Broker在正常关闭使，会执行unregisterBroker命令。 两种方法删除的逻辑都是一致的。 step1：申请写锁 step2：从brokerLiveTable、filterServerTable移除，从brokerAddrTable、clusterAddrTable、topicQueueTable移除 step3：释放锁 27、使用RocketMQ过程中遇到过什么问题？1、消息挤压问题 2、消息丢失问题 3、消息重复消费问题 4、RocketMQ内存不够OOM问题 28、RocketMQ的总体架构，以及每个组件的功能？RocketMQ 一共由四个部分组成：NameServer、Broker、Producer、Consumer，它们分别对应着发现、存、发、收四个功能。这四部分的功能很像邮政系统，Producer 相当于负责发送信件的发件人，Consumer 相当于负责接收信件的收件人，Broker 相当于负责暂存信件传输的邮局，NameServer 相当于负责协调各个地方邮局的管理机构。一般情况下，为了保证高可用，每一部分都是以集群形式部署的。 29、讲一讲RocketMQ中的分布式事务及实现 详细看这里 30、讲一讲RocketMQ中事务回查机制的实现 详细看这里","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"Spring基础","slug":"28-Spring基础","date":"2023-09-11T06:57:18.000Z","updated":"2023-09-11T10:01:04.931Z","comments":true,"path":"posts/28.html","link":"","permalink":"http://example.com/posts/28.html","excerpt":"","text":"讲师：邓澎波 (上传仅供个人学习交流使用 如有侵权立刻删除) Spring面试专题1.Spring应该很熟悉吧？来介绍下你的Spring的理解1.1 Spring的发展历程 先介绍Spring是怎么来的，发展中有哪些核心的节点，当前的最新版本是什么等 通过上图可以比较清晰的看到Spring的各个时间版本对应的时间节点了。也就是Spring从之前单纯的xml的配置方式，到现在的完全基于注解的编程方式发展。 1.2 Spring的组成 Spring是一个轻量级的IoC和AOP容器框架。是为Java应用程序提供基础性服务的一套框架，目的是用于简化企业应用程序的开发，它使得开发者只需要关心业务需求。常见的配置方式有三种：基于XML的配置、基于注解的配置、基于Java的配置. 主要由以下几个模块组成： Spring Core：核心类库，提供IOC服务； Spring Context：提供框架式的Bean访问方式，以及企业级功能（JNDI、定时任务等）； Spring AOP：AOP服务； Spring DAO：对JDBC的抽象，简化了数据访问异常的处理； Spring ORM：对现有的ORM框架的支持； Spring Web：提供了基本的面向Web的综合特性，例如多方文件上传； Spring MVC：提供面向Web应用的Model-View-Controller实现。 1.3 Spring的好处 序号 好处 说明 1 轻量 Spring 是轻量的，基本的版本大约2MB。 2 控制反转 Spring通过控制反转实现了松散耦合，对象们给出它们的依赖，&lt;br&gt;而不是创建或查找依赖的对象们。 3 面向切面编程(AOP) Spring支持面向切面的编程，并且把应用业务逻辑和系统服务分开。 4 容器 Spring 包含并管理应用中对象的生命周期和配置。 5 MVC框架 Spring的WEB框架是个精心设计的框架，是Web框架的一个很好的替代品。 6 事务管理 Spring 提供一个持续的事务管理接口，&lt;br&gt;可以扩展到上至本地事务下至全局事务（JTA）。 7 异常处理 Spring 提供方便的API把具体技术相关的异常 &lt;br&gt;(比如由JDBC，Hibernate or JDO抛出的)转化为一致的unchecked 异常。 8 最重要的 用的人多！！！ 2.Spring框架中用到了哪些设计模式2.1 单例模式 单例模式应该是大家印象最深的一种设计模式了。在Spring中最明显的使用场景是在配置文件中配置注册bean对象的时候设置scope的值为singleton 。 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.dpb.pojo.User\" id=\"user\" scope=\"singleton\"&gt; &lt;property name=\"name\" value=\"波波烤鸭\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 2.2 原型模式 原型模式也叫克隆模式，Spring中该模式使用的很明显，和单例一样在bean标签中设置scope的属性prototype即表示该bean以克隆的方式生成 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.dpb.pojo.User\" id=\"user\" scope=\"prototype\"&gt; &lt;property name=\"name\" value=\"波波烤鸭\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 2.3 模板模式 模板模式的核心是父类定义好流程，然后将流程中需要子类实现的方法就抽象话留给子类实现，Spring中的JdbcTemplate就是这样的实现。我们知道jdbc的步骤是固定 加载驱动, 获取连接通道, 构建sql语句. 执行sql语句, 关闭资源 在这些步骤中第3步和第四步是不确定的,所以就留给客户实现，而我们实际使用JdbcTemplate的时候也确实是只需要构建SQL就可以了.这就是典型的模板模式。我们以query方法为例来看下JdbcTemplate中的代码. 2.4 观察者模式 观察者模式定义的是对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。使用比较场景是在监听器中而spring中Observer模式常用的地方也是listener的实现。如ApplicationListener. 2.5 工厂模式简单工厂模式： 简单工厂模式就是通过工厂根据传递进来的参数决定产生哪个对象。Spring中我们通过getBean方法获取对象的时候根据id或者name获取就是简单工厂模式了。 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd\"&gt; &lt;context:annotation-config/&gt; &lt;bean class=\"com.dpb.pojo.User\" id=\"user\" &gt; &lt;property name=\"name\" value=\"波波烤鸭\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 工厂方法模式： 在Spring中我们一般是将Bean的实例化直接交给容器去管理的，实现了使用和创建的分离，这时容器直接管理对象，还有种情况是，bean的创建过程我们交给一个工厂去实现，而Spring容器管理这个工厂。这个就是我们讲的工厂模式，在Spring中有两种实现一种是静态工厂方法模式，一种是动态工厂方法模式。以静态工厂来演示 /** * User 工厂类 * @author dpb[波波烤鸭] * */ public class UserFactory { /** * 必须是static方法 * @return */ public static UserBean getInstance(){ return new UserBean(); } } application.xml文件中注册 &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 静态工厂方式配置 配置静态工厂及方法 --&gt; &lt;bean class=\"com.dpb.factory.UserFactory\" factory-method=\"getInstance\" id=\"user2\"/&gt; &lt;/beans&gt; 2.6 适配器模式 将一个类的接口转换成客户希望的另外一个接口。使得原本由于接口不兼容而不能一起工作的那些类可以在一起工作。这就是适配器模式。在Spring中在AOP实现中的Advice和interceptor之间的转换就是通过适配器模式实现的。 class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable { @Override public boolean supportsAdvice(Advice advice) { return (advice instanceof MethodBeforeAdvice); } @Override public MethodInterceptor getInterceptor(Advisor advisor) { MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); // 通知类型匹配对应的拦截器 return new MethodBeforeAdviceInterceptor(advice); } } 2.7 装饰者模式 装饰者模式又称为包装模式(Wrapper),作用是用来动态的为一个对象增加新的功能。装饰模式是一种用于代替继承的技术，无须通过继承增加子类就能扩展对象的新功能。使用对象的关联关系代替继承关系，更加灵活，同时避免类型体系的快速膨胀。 spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。 具体的使用在Spring session框架中的SessionRepositoryRequestWrapper使用包装模式对原生的request的功能进行增强，可以将session中的数据和分布式数据库进行同步，这样即使当前tomcat崩溃，session中的数据也不会丢失。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt; &lt;version&gt;1.3.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; 2.8 代理模式 代理模式应该是大家非常熟悉的设计模式了，在Spring中AOP的实现中代理模式使用的很彻底. 2.9 策略模式 策略模式对应于解决某一个问题的一个算法族，允许用户从该算法族中任选一个算法解决某一问题，同时可以方便的更换算法或者增加新的算法。并且由客户端决定调用哪个算法，spring中在实例化对象的时候用到Strategy模式。XmlBeanDefinitionReader,PropertiesBeanDefinitionReader 2.10 责任链默认AOP中的拦截器链 2.11 委托者模式DelegatingFilterProxy，整合Shiro，SpringSecurity的时候都有用到。 ….. 3.Autowired和Resource关键字的区别？ 这是一个相对比较简单的问题，@Resource和@Autowired都是做bean的注入时使用，其实@Resource并不是Spring的注解，它的包是javax.annotation.Resource，需要导入，但是Spring支持该注解的注入。 3.1 共同点 两者都可以写在字段和setter方法上。两者如果都写在字段上，那么就不需要再写setter方法. 3.2 不同点@Autowired @Autowired为Spring提供的注解，需要导入org.springframework.beans.factory.annotation.Autowired;只按照byType注入。 public class TestServiceImpl { // 下面两种@Autowired只要使用一种即可 @Autowired private UserDao userDao; // 用于字段上 @Autowired public void setUserDao(UserDao userDao) { // 用于属性的方法上 this.userDao = userDao; } } @Autowired注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualififier注解一起使用。如下： public class TestServiceImpl { @Autowired @Qualifier(\"userDao\") private UserDao userDao; } @Resource @Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略. public class TestServiceImpl { // 下面两种@Resource只要使用一种即可 @Resource(name=\"userDao\") private UserDao userDao; // 用于字段上 @Resource(name=\"userDao\") public void setUserDao(UserDao userDao) { // 用于属性的setter方法上 this.userDao = userDao; } } @Resource装配顺序： 如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常。 如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常。 如果指定了type，则从上下文中找到类似匹配的唯一bean进行装配，找不到或是找到多个，都会抛出异常。 如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配。 @Resource的作用相当于@Autowired，只不过@Autowired按照byType自动注入。 4.Spring中常用的注解有哪些，重点介绍几个@Controller @Service @RestController @RequestBody,@Indexd @Import等 @Indexd提升 @ComponentScan的效率 @Import注解是import标签的替换，在SpringBoot的自动装配中非常重要，也是EnableXXX的前置基础。 5.循环依赖面试的重点，大厂必问之一： 5.1 什么是循环依赖看下图 上图是循环依赖的三种情况，虽然方式有点不一样，但是循环依赖的本质是一样的，就你的完整创建要依赖与我，我的完整创建也依赖于你。相互依赖从而没法完整创建造成失败。 5.2 代码演示 我们再通过代码的方式来演示下循环依赖的效果 public class CircularTest { public static void main(String[] args) { new CircularTest1(); } } class CircularTest1{ private CircularTest2 circularTest2 = new CircularTest2(); } class CircularTest2{ private CircularTest1 circularTest1 = new CircularTest1(); } 执行后出现了 StackOverflowError 错误 上面的就是最基本的循环依赖的场景，你需要我，我需要你，然后就报错了。而且上面的这种设计情况我们是没有办法解决的。那么针对这种场景我们应该要怎么设计呢？这个是关键！ 5.3 分析问题 首先我们要明确一点就是如果这个对象A还没创建成功，在创建的过程中要依赖另一个对象B，而另一个对象B也是在创建中要依赖对象A，这种肯定是无解的，这时我们就要转换思路，我们先把A创建出来，但是还没有完成初始化操作，也就是这是一个半成品的对象，然后在赋值的时候先把A暴露出来，然后创建B，让B创建完成后找到暴露的A完成整体的实例化，这时再把B交给A完成A的后续操作，从而揭开了循环依赖的密码。也就是如下图： 5.4 自己解决 明白了上面的本质后，我们可以自己来尝试解决下： 先来把上面的案例改为set/get来依赖关联 public class CircularTest { public static void main(String[] args) throws Exception{ System.out.println(getBean(CircularTest1.class).getCircularTest2()); System.out.println(getBean(CircularTest2.class).getCircularTest1()); } private static &lt;T&gt; T getBean(Class&lt;T&gt; beanClass) throws Exception{ // 1.获取 实例对象 Object obj = beanClass.newInstance(); // 2.完成属性填充 Field[] declaredFields = obj.getClass().getDeclaredFields(); // 遍历处理 for (Field field : declaredFields) { field.setAccessible(true); // 针对private修饰 // 获取成员变量 对应的类对象 Class&lt;?&gt; fieldClass = field.getType(); // 获取对应的 beanName String fieldBeanName = fieldClass.getSimpleName().toLowerCase(); // 给成员变量赋值 如果 singletonObjects 中有半成品就获取，否则创建对象 field.set(obj,getBean(fieldClass)); } return (T) obj; } } class CircularTest1{ private CircularTest2 circularTest2; public CircularTest2 getCircularTest2() { return circularTest2; } public void setCircularTest2(CircularTest2 circularTest2) { this.circularTest2 = circularTest2; } } class CircularTest2{ private CircularTest1 circularTest1; public CircularTest1 getCircularTest1() { return circularTest1; } public void setCircularTest1(CircularTest1 circularTest1) { this.circularTest1 = circularTest1; } } 然后我们再通过把对象实例化和成员变量赋值拆解开来处理。从而解决循环依赖的问题 public class CircularTest { // 保存提前暴露的对象，也就是半成品的对象 private final static Map&lt;String,Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(); public static void main(String[] args) throws Exception{ System.out.println(getBean(CircularTest1.class).getCircularTest2()); System.out.println(getBean(CircularTest2.class).getCircularTest1()); } private static &lt;T&gt; T getBean(Class&lt;T&gt; beanClass) throws Exception{ //1.获取类对象对应的名称 String beanName = beanClass.getSimpleName().toLowerCase(); // 2.根据名称去 singletonObjects 中查看是否有半成品的对象 if(singletonObjects.containsKey(beanName)){ return (T) singletonObjects.get(beanName); } // 3. singletonObjects 没有半成品的对象，那么就反射实例化对象 Object obj = beanClass.newInstance(); // 还没有完整的创建完这个对象就把这个对象存储在了 singletonObjects中 singletonObjects.put(beanName,obj); // 属性填充来补全对象 Field[] declaredFields = obj.getClass().getDeclaredFields(); // 遍历处理 for (Field field : declaredFields) { field.setAccessible(true); // 针对private修饰 // 获取成员变量 对应的类对象 Class&lt;?&gt; fieldClass = field.getType(); // 获取对应的 beanName String fieldBeanName = fieldClass.getSimpleName().toLowerCase(); // 给成员变量赋值 如果 singletonObjects 中有半成品就获取，否则创建对象 field.set(obj,singletonObjects.containsKey(fieldBeanName)? singletonObjects.get(fieldBeanName):getBean(fieldClass)); } return (T) obj; } } class CircularTest1{ private CircularTest2 circularTest2; public CircularTest2 getCircularTest2() { return circularTest2; } public void setCircularTest2(CircularTest2 circularTest2) { this.circularTest2 = circularTest2; } } class CircularTest2{ private CircularTest1 circularTest1; public CircularTest1 getCircularTest1() { return circularTest1; } public void setCircularTest1(CircularTest1 circularTest1) { this.circularTest1 = circularTest1; } } 运行程序你会发现问题完美的解决了 在上面的方法中的核心是getBean方法，Test1 创建后填充属性时依赖Test2，那么就去创建 Test2，在创建 Test2 开始填充时发现依赖于 Test1，但此时 Test1 这个半成品对象已经存放在缓存到 singletonObjects 中了，所以Test2可以正常创建，在通过递归把 Test1 也创建完整了。 最后总结下该案例解决的本质： 5.5 Spring循环依赖针对Spring中Bean对象的各种场景。支持的方案不一样： 然后我们再来看看Spring中是如何解决循环依赖问题的呢？刚刚上面的案例中的对象的生命周期的核心就两个 而Spring创建Bean的生命周期中涉及到的方法就很多了。下面是简单列举了对应的方法 基于前面案例的了解，我们知道肯定需要在调用构造方法方法创建完成后再暴露对象，在Spring中提供了三级缓存来处理这个事情，对应的处理节点如下图： 对应到源码中具体处理循环依赖的流程如下： 上面就是在Spring的生命周期方法中和循环依赖出现相关的流程了。那么源码中的具体处理是怎么样的呢？我们继续往下面看。 首先在调用构造方法的后会放入到三级缓存中 下面就是放入三级缓存的逻辑 protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) { Assert.notNull(singletonFactory, \"Singleton factory must not be null\"); // 使用singletonObjects进行加锁，保证线程安全 synchronized (this.singletonObjects) { // 如果单例对象的高速缓存【beam名称-bean实例】没有beanName的对象 if (!this.singletonObjects.containsKey(beanName)) { // 将beanName,singletonFactory放到单例工厂的缓存【bean名称 - ObjectFactory】 this.singletonFactories.put(beanName, singletonFactory); // 从早期单例对象的高速缓存【bean名称-bean实例】 移除beanName的相关缓存对象 this.earlySingletonObjects.remove(beanName); // 将beanName添加已注册的单例集中 this.registeredSingletons.add(beanName); } } } 然后在填充属性的时候会存入二级缓存中 earlySingletonObjects.put(beanName,bean); registeredSingletons.add(beanName); 最后把创建的对象保存在了一级缓存中 protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) { // 将映射关系添加到单例对象的高速缓存中 this.singletonObjects.put(beanName, singletonObject); // 移除beanName在单例工厂缓存中的数据 this.singletonFactories.remove(beanName); // 移除beanName在早期单例对象的高速缓存的数据 this.earlySingletonObjects.remove(beanName); // 将beanName添加到已注册的单例集中 this.registeredSingletons.add(beanName); } } 5.6 疑问点这些疑问点也是面试官喜欢问的问题点 为什么需要三级缓存三级缓存主要处理的是AOP的代理对象，存储的是一个ObjectFactory 三级缓存考虑的是带你对象，而二级缓存考虑的是性能-从三级缓存的工厂里创建出对象，再扔到二级缓存（这样就不用每次都要从工厂里拿） 没有三级环境能解决吗？没有三级缓存是可以解决循环依赖问题的 三级缓存分别什么作用一级缓存：正式对象 二级缓存：半成品对象 三级缓存：工厂 6.Spring的生命周期 结合图，把Bean对象在Spring中的关键节点介绍一遍 7.Spring中支持几种作用域Spring容器中的bean可以分为5个范围： prototype：为每一个bean请求提供一个实例。 singleton：默认，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护。 request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收。 session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效。 global-session：全局作用域，global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 8.说说事务的隔离级别 事务隔离级别指的是一个事务对数据的修改与另一个并行的事务的隔离程度，当多个事务同时访问相同数据时，如果没有采取必要的隔离机制，就可能发生以下问题： 问题 描述 脏读 一个事务读到另一个事务未提交的更新数据，所谓脏读，就是指事务A读到了事务B还没有提交的数据，比如银行取钱，事务A开启事务，此时切换到事务B，事务B开启事务–&gt;取走100元，此时切换回事务A，事务A读取的肯定是数据库里面的原始数据，因为事务B取走了100块钱，并没有提交，数据库里面的账务余额肯定还是原始余额，这就是脏读 幻读 是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。 同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象 发生了幻觉一样。 不可重复读 在一个事务里面的操作中发现了未被操作的数据 比方说在同一个事务中先后执行两条一模一样的select语句，期间在此次事务中没有执行过任何DDL语句，但先后得到的结果不一致，这就是不可重复读 Spring支持的隔离级别 隔离级别 描述 DEFAULT 使用数据库本身使用的隔离级别 &lt;br&gt; ORACLE（读已提交） MySQL（可重复读） READ_UNCOMITTED 读未提交（脏读）最低的隔离级别，一切皆有可能。 READ_COMMITED 读已提交，ORACLE默认隔离级别，有幻读以及不可重复读风险。 REPEATABLE_READ 可重复读，解决不可重复读的隔离级别，但还是有幻读风险。 SERLALIZABLE 串行化，最高的事务隔离级别，不管多少事务，挨个运行完一个事务的所有子事务之后才可以执行另外一个事务里面的所有子事务，这样就解决了脏读、不可重复读和幻读的问题了 再必须强调一遍，不是事务隔离级别设置得越高越好，事务隔离级别设置得越高，意味着势必要花手段去加锁用以保证事务的正确性，那么效率就要降低，因此实际开发中往往要在效率和并发正确性之间做一个取舍，一般情况下会设置为READ_COMMITED，此时避免了脏读，并发性也还不错，之后再通过一些别的手段去解决不可重复读和幻读的问题就好了。 9.事务的传播行为保证事务：ACID 事务的传播行为针对的是嵌套的关系 Spring中的7个事务传播行为: 事务行为 说明 PROPAGATION_REQUIRED 支持当前事务，假设当前没有事务。就新建一个事务 PROPAGATION_SUPPORTS 支持当前事务，假设当前没有事务，就以非事务方式运行 PROPAGATION_MANDATORY 支持当前事务，假设当前没有事务，就抛出异常 PROPAGATION_REQUIRES_NEW 新建事务，假设当前存在事务。把当前事务挂起 PROPAGATION_NOT_SUPPORTED 以非事务方式运行操作。假设当前存在事务，就把当前事务挂起 PROPAGATION_NEVER 以非事务方式运行，假设当前存在事务，则抛出异常 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 举例说明 案例代码 ServiceA ServiceA { void methodA() { ServiceB.methodB(); } } ServiceB ServiceB { void methodB() { } } 1.PROPAGATION_REQUIRED 假如当前正要运行的事务不在另外一个事务里，那么就起一个新的事务 比方说，ServiceB.methodB的事务级别定义PROPAGATION_REQUIRED, 那么因为执行ServiceA.methodA的时候，ServiceA.methodA已经起了事务。这时调用ServiceB.methodB，ServiceB.methodB看到自己已经执行在ServiceA.methodA的事务内部。就不再起新的事务。而假如ServiceA.methodA执行的时候发现自己没有在事务中，他就会为自己分配一个事务。这样，在ServiceA.methodA或者在ServiceB.methodB内的不论什么地方出现异常。事务都会被回滚。即使ServiceB.methodB的事务已经被提交，可是ServiceA.methodA在接下来fail要回滚，ServiceB.methodB也要回滚 2.PROPAGATION_SUPPORTS 假设当前在事务中。即以事务的形式执行。假设当前不在一个事务中，那么就以非事务的形式执行 3PROPAGATION_MANDATORY 必须在一个事务中执行。也就是说，他仅仅能被一个父事务调用。否则，他就要抛出异常 4.PROPAGATION_REQUIRES_NEW 这个就比较绕口了。 比方我们设计ServiceA.methodA的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别为PROPAGATION_REQUIRES_NEW。那么当运行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起。ServiceB.methodB会起一个新的事务。等待ServiceB.methodB的事务完毕以后，他才继续运行。他与PROPAGATION_REQUIRED 的事务差别在于事务的回滚程度了。由于ServiceB.methodB是新起一个事务，那么就是存在两个不同的事务。假设ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚。ServiceB.methodB是不会回滚的。假设ServiceB.methodB失败回滚，假设他抛出的异常被ServiceA.methodA捕获，ServiceA.methodA事务仍然可能提交。 5.PROPAGATION_NOT_SUPPORTED 当前不支持事务。比方ServiceA.methodA的事务级别是PROPAGATION_REQUIRED 。而ServiceB.methodB的事务级别是PROPAGATION_NOT_SUPPORTED ，那么当执行到ServiceB.methodB时。ServiceA.methodA的事务挂起。而他以非事务的状态执行完，再继续ServiceA.methodA的事务。 6.PROPAGATION_NEVER 不能在事务中执行。如果ServiceA.methodA的事务级别是PROPAGATION_REQUIRED。 而ServiceB.methodB的事务级别是PROPAGATION_NEVER ，那么ServiceB.methodB就要抛出异常了。 7.PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 10.Spring事务实现的方式编程式事务管理：这意味着你可以通过编程的方式管理事务，这种方式带来了很大的灵活性，但很难维护。 声明式事务管理：这种方式意味着你可以将事务管理和业务代码分离。你只需要通过注解或者XML配置管理事务。 11.事务注解的本质是什么 @Transactional 这个注解仅仅是一些（和事务相关的）元数据，在运行时被事务基础设施读取消费，并使用这些元数据来配置bean的事务行为。 大致来说具有两方面功能，一是表明该方法要参与事务，二是配置相关属性来定制事务的参与方式和运行行为 声明式事务主要是得益于Spring AOP。使用一个事务拦截器，在方法调用的前后/周围进行事务性增强（advice），来驱动事务完成。 @Transactional注解既可以标注在类上，也可以标注在方法上。当在类上时，默认应用到类里的所有方法。如果此时方法上也标注了，则方法上的优先级高。 另外注意方法一定要是public的。 https://cloud.fynote.com/share/d/IVeyV0Jp 12.谈谈你对BeanFactory和ApplicationContext的理解目的：考察对IoC的理解 BeanFactory:Bean工厂 ==》IoC容器 BeanDefinition ===》Bean定义 BeanDefinitionRegistry ==》 BeanDefinition 和 BeanFactory的关联 …. ApplicationContext:应用上下文 ApplicationContext ac = new ClasspathXmlApplicationContext(xxx.xml); 13.谈谈你对BeanFactoryPostProcessor的理解BeanFactoryPostProcessor:是在BeanFactory创建完成后的后置处理 BeanFactory：对外提供Bean对象 需要知道怎么提供Bean对象–&gt;BeanDefinition XML/注解 –》 BeanDefinition –》注册 –》完成BeanFactory的处理 1.需要交代BeanFactoryPostProcessor的作用 2.举个例子 @Configuration 注解 –》 Java被 @Configuration注解标识–&gt; 这是一个Java配置类 @Configuration–》@Component –&gt;BeanDefinition –&gt; 存储在BeanFactory中 是当做一个普通的Bean管理的 @Bean @Primary 。。。。 ConfigurationClassPostProcessor 14.谈谈你对BeanPostProcessor的理解针对Bean对象初始化前后。 针对Bean对象创建之后的处理操作。 SpringIoC 核心流程 BeanDefinition –&gt; 注册BeanDefinition –&gt;BeanFactory –》BeanFactory的后置处理 –&gt; 单例bean –&gt; AOP –》 代理对象 —&gt; advice pointcut join point 。。。 BeanPostProcessor 提供了一种扩展机制 自定义接口的实现 –》 注册到BeanFactory的 Map中 BeanDefinition –&gt; 注册BeanDefinition –&gt;BeanFactory –》BeanFactory的后置处理 –&gt; 单例bean –&gt; 遍历上面的Map 执行相关的行为 –&gt; 。。。。。 ===》 AOP IoC 和AOP的关系 有了IoC 才有 AOP DI 15.谈谈你对SpringMVC的理解控制框架：前端控制器–》Servlet –》Web容器【Tomcat】 SpringMVC和Spring的关系 IoC容器关系–》父子关系 https://www.processon.com/view/link/63dc99aba7d181715d1f4569 Spring和SpringMVC的关系理解 Spring和SpringMVC整合的项目中 Controller Service Dao 具体的有两个容器Spring中的IoC容器。然后SpringMVC中也有一个IoC容器 Controller中定义的实例都是SpringMVC组件维护的 Service和Dao中的实例都是由Spring的IoC容器维护的 这两个容器有一个父子容器的关系 Spring容器是SpringMVC容器的父容器 16.谈谈你对DelegatingFilterProxy的理解web.xml Shiro SpringSecurity Spring整合的第三方的组件会非常多 JWT 单独登录 OAuth2.0 组件：组合起来的零件–》组件 组合起来的技术栈–》组件框架 17.谈谈你对SpringBoot的理解约定由于配置 自动装配 SpringBoot和Spring的关系 SpringBoot的初始化 –&gt; IoC Spring的初始化 SpringBoot的启动 –&gt; IoC @SpringApplication注解 –&gt; @Configuration –&gt;ConfigurationClassPostProcessor –》 @Import注解 –》 延迟加载 –》 自动装配 –&gt; SPI 去重 排除 过滤 spring.factories SSM框架的整合 1。导入依赖 2。添加配置文件 3。设置配置文件 web.xml 18.介绍下Import注解的理解@Import注解是在Spring3.0的时候提供。目的是为了替换在XML配置文件中的import标签。 @Import注解除了可以导入第三方的Java配置类还扩展了其他的功能 可以把某个类型的对象注入到容器中 导入的类型如果实现了ImportSelector接口。那么会调用接口中声明的方法。然后把方法返回的类型全类路径的类型对象注入到容器中 如果导入的类型实现了ImportBeanDefinitionRegistrar这个接口。那么就会调用声明的方法在该方法中显示的提供注册器来完成注入 19.SpringBoot自动装配中为什么用DeferredImportSelector在SpringBoot自动装配中核心是会加载所有依赖中的META-INF/spring.factories文件中的配置信息。 我们可以有多个需要加载的spring.factories文件。那么我们就需要多次操作。我们可以考虑把所有的信息都加载后再统一把这些需要注入到容器中的内容注入进去 DeferredImportSelector：延迟注入Bean实例的作用 20.SpringBoot中有了属性文件为什么还要加一个bootstrap.yml文件? 在单体的SpringBoot项目中其实我们是用不到bootstrap.yml文件的，bootsrap.yml文件的使用需要SpringCloud的支持，因为在微服务环境下我们都是有配置中心的，来统一的管理系统的相关配置属性，那么怎么去加载配置中心的内容呢？一个SpringBoot项目启动的时候默认只会加载对应的application.yml中的相关信息，这时bootstrap.yml的作用就体现出来了，会在SpringBoot正常启动前创建一个父容器来通过bootstrap.yml中的配置来加载配置中心的内容。 21.如果要对属性文件中的账号密码加密如何实现？ 其实这是一个比较篇实战的一个问题，我们在application.yml中保存的MySQL数据库的账号密码或者其他服务的账号密码，都可以保存加密后的内容，那么我们在处理的时候要怎么解密呢？这个其实比较简单只需要对SpringBoot的执行流程清楚就可以了，第一个我们可以通过自定义监听器可以在加载解析了配置文件之后对加密的文件中做解密处理同时覆盖之前加密的内容，或者通过对应的后置处理器来处理，具体的实现如下： 然后我们通过案例代码来演示下，加深大家的理解首先我们在属性文件中配置加密后的信息 spring.datasource.driverClassName=com.mysql.cj.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/mb?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true spring.datasource.username=root # 对通过3DES对密码加密 spring.datasource.password=t5Jd2CzFWEw= spring.datasource.type=com.alibaba.druid.pool.DruidDataSource mybatis.mapper-locations=classpath:mapper/*.xml 在SpringBoot项目启动的时候在在刷新Spring容器之前执行的，所以我们要做的就是在加载完环境配置信息后，获取到配置的 spring.datasource.password=t5Jd2CzFWEw= 这个信息，然后解密并修改覆盖就可以了。 然后在属性文件的逻辑其实是通过发布事件触发对应的监听器来实现的 所以第一个解决方案就是你自定义一个监听器，这个监听器在加载属性文件(ConfigFileApplicationListener)的监听器之后处理,这种方式稍微麻烦点， 还有一种方式就是通过加载属性文件的一个后置处理器来处理，这就以个为例来实现 3DES的工具类 /** * 3DES加密算法，主要用于加密用户id，身份证号等敏感信息,防止破解 */ public class DESedeUtil { //秘钥 public static final String KEY = \"~@#$y1a2n.&amp;@+n@$%*(1)\"; //秘钥长度 private static final int secretKeyLength = 24; //加密算法 private static final String ALGORITHM = \"DESede\"; //编码 private static final String CHARSET = \"UTF-8\"; /** * 转换成十六进制字符串 * @param key * @return */ public static byte[] getHex(String key){ byte[] secretKeyByte = new byte[24]; try { byte[] hexByte; hexByte = new String(DigestUtils.md5Hex(key)).getBytes(CHARSET); //秘钥长度固定为24位 System.arraycopy(hexByte,0,secretKeyByte,0,secretKeyLength); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } return secretKeyByte; } /** * 生成密钥，返回加密串 * @param key 密钥 * @param encodeStr 将加密的字符串 * @return */ public static String encode3DES(String key,String encodeStr){ try { Cipher cipher = Cipher.getInstance(ALGORITHM); cipher.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(getHex(key), ALGORITHM)); return Base64.encodeBase64String(cipher.doFinal(encodeStr.getBytes(CHARSET))); }catch(Exception e){ e.printStackTrace(); } return null; } /** * 生成密钥,解密，并返回字符串 * @param key 密钥 * @param decodeStr 需要解密的字符串 * @return */ public static String decode3DES(String key, String decodeStr){ try { Cipher cipher = Cipher.getInstance(ALGORITHM); cipher.init(Cipher.DECRYPT_MODE, new SecretKeySpec(getHex(key),ALGORITHM)); return new String(cipher.doFinal(new Base64().decode(decodeStr)),CHARSET); } catch(Exception e){ e.printStackTrace(); } return null; } public static void main(String[] args) { String userId = \"123456\"; String encode = DESedeUtil.encode3DES(KEY, userId); String decode = DESedeUtil.decode3DES(KEY, encode); System.out.println(\"用户id&gt;&gt;&gt;\"+userId); System.out.println(\"用户id加密&gt;&gt;&gt;\"+encode); System.out.println(\"用户id解密&gt;&gt;&gt;\"+decode); } } 声明后置处理器 public class SafetyEncryptProcessor implements EnvironmentPostProcessor { @Override public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) { for (PropertySource&lt;?&gt; propertySource : environment.getPropertySources()) { System.out.println(\"propertySource = \" + propertySource); if(propertySource instanceof OriginTrackedMapPropertySource){ OriginTrackedMapPropertySource source = (OriginTrackedMapPropertySource) propertySource; for (String propertyName : source.getPropertyNames()) { //System.out.println(propertyName + \"=\" + source.getProperty(propertyName)); if(\"spring.datasource.password\".equals(propertyName)){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 做解密处理 String property = (String) source.getProperty(propertyName); String s = DESedeUtil.decode3DES(DESedeUtil.KEY, property); System.out.println(\"密文：\" + property); System.out.println(\"解密后的：\" + s); map.put(propertyName,s); // 注意要添加到前面，覆盖 environment.getPropertySources().addFirst(new MapPropertySource(propertyName,map)); } } } } } } 然后在META-INF/spring.factories文件中注册 org.springframework.boot.env.EnvironmentPostProcessor=com.bobo.util.SafetyEncryptProcessor 然后启动项目就可以了 搞定 22.谈谈Indexed注解的作用@Indexed注解是Spring5.0提供 Indexed注解解决的问题：是随着项目越来越复杂那么@ComponentScan需要扫描加载的Class会越来越多。在系统启动的时候会造成性能损耗。所以Indexed注解的作用其实就是提升系统启动的性能。 在系统编译的时候那么会收集所有被@Indexed注解标识的Java类。然后记录在META-INF/spring.components文件中。那么系统启动的时候就只需要读取一个该文件中的内容就不用在遍历所有的目录了。提升的效率 23.@Component, @Controller, @Repository,@Service 有何区别？@Component ：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。 @Controller ：这将一个类标记为 Spring Web MVC 控制器。标有它的Bean 会自动导入到 IoC 容器中。 @Service ：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用@Service 而不是 @Component，因为它以更好的方式指定了意图。 @Repository ：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。 24.有哪些通知类型(Advice)前置通知：Before - 这些类型的 Advice 在 joinpoint 方法之前执行，并使用@Before 注解标记进行配置。后置通知：After Returning - 这些类型的 Advice 在连接点方法正常执行后执行，并使用@AfterReturning 注解标记进行配置。异常通知：After Throwing - 这些类型的 Advice 仅在 joinpoint 方法通过抛出异常退出并使用 @AfterThrowing 注解标记配置时执行。最终通知：After (finally) - 这些类型的 Advice 在连接点方法之后执行，无论方法退出是正常还是异常返回，并使用 @After 注解标记进行配置。环绕通知：Around - 这些类型的 Advice 在连接点之前和之后执行，并使用@Around 注解标记进行配置。 25.什么是 Spring 的依赖注入？依赖注入，是 IOC 的一个方面，是个通常的概念，它有多种解释。这概念是说你不用创建对象，而只需要描述它如何被创建。你不在代码里直接组装你的组件和服务，但是要在配置文件里描述哪些组件需要哪些服务，之后一个容器（IOC 容器）负责把他们组装起来。 BeanFactory BeanDefinition BeanDefinitionRegistry ApplicationContext –&gt; DI –&gt; AOP –&gt;事务。日志 26.Spring 框架中的单例 bean 是线程安全的吗?不，Spring 框架中的单例 bean 不是线程安全的。 Spring容器中的Bean是否线程安全，容器本身并没有提供Bean的线程安全策略，因此可以说Spring容器中的Bean本身不具备线程安全的特性，但是具体还是要结合具体scope的Bean去研究。 Spring 的 bean 作用域（scope）类型 1、singleton:单例，默认作用域。 2、prototype:原型，每次创建一个新对象。 3、request:请求，每次Http请求创建一个新对象，适用于WebApplicationContext环境下。 4、session:会话，同一个会话共享一个实例，不同会话使用不用的实例。 5、global-session:全局会话，所有会话共享一个实例。 线程安全这个问题，要从单例与原型Bean分别进行说明。 原型Bean 对于原型Bean,每次创建一个新对象，也就是线程之间并不存在Bean共享，自然是不会有线程安全的问题。 单例Bean 对于单例Bean,所有线程都共享一个单例实例Bean，因此是存在资源的竞争。 如果单例Bean是一个无状态Bean，也就是线程中的操作不会对Bean的成员执行查询以外的操作，那么这个单例Bean是线程安全的。比如Spring mvc 的 Controller、Service、Dao等，这些Bean大多是无状态的，只关注于方法本身。 spring单例，为什么controller、service和dao确能保证线程安全？ Spring中的Bean默认是单例模式的，框架并没有对bean进行多线程的封装处理。 实际上大部分时间Bean是无状态的（比如Dao） 所以说在某种程度上来说Bean其实是安全的。 但是如果Bean是有状态的 那就需要开发人员自己来进行线程安全的保证，最简单的办法就是改变bean的作用域 把 “singleton”改为“protopyte” 这样每次请求Bean就相当于是 new Bean() 这样就可以保证线程的安全了。 有状态就是有数据存储功能 无状态就是不会保存数据 controller、service和dao层本身并不是线程安全的，只是如果只是调用里面的方法，而且多线程调用一个实例的方法，会在内存中复制变量，这是自己的线程的工作内存，是安全的。","categories":[{"name":"spring框架","slug":"spring框架","permalink":"http://example.com/categories/spring%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"SpringCloud基础","slug":"27-SpringCloud基础","date":"2023-09-11T06:56:58.000Z","updated":"2023-09-11T10:13:32.368Z","comments":true,"path":"posts/27.html","link":"","permalink":"http://example.com/posts/27.html","excerpt":"","text":"未完待续。。。","categories":[{"name":"spring","slug":"spring","permalink":"http://example.com/categories/spring/"}],"tags":[{"name":"springCloud","slug":"springCloud","permalink":"http://example.com/tags/springCloud/"}]},{"title":"SpringBoot基础","slug":"26-SpringBoot基础","date":"2023-09-11T06:56:45.000Z","updated":"2023-09-11T10:12:09.900Z","comments":true,"path":"posts/26.html","link":"","permalink":"http://example.com/posts/26.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 什么是SpringBoot？Spring Boot 是由 Pivotal 团队提供的基于 Spring 的全新框架，旨在简化 Spring 应用的初始搭建和开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 约定大于（优于）配置 SpringBoot的run方法做了什么事？run方法其实就是做了一个IOC初始化的操作 @ComponentScan注解是干什么的？被@ComponentScan修饰的java类，如果没有指定具体的扫描路径。实际上他默认的扫描路径是当前JAVA类所在的路径及其子路径，都是被扫描的范围。 @EnableAutoConfiguration注解是干什么的？@EnableAutoConfiguration 开启自动配置 开启自动装配 Import注解的三种使用用法1.静态注入的方式 2.实现了我们的ImportSelector接口，并且实现了selectImports方法，那么我们的返回值就是selectImports的方法的返回类型。 3.实现了ImportBeanDefinitionRegistrar接口，并且实现了registerBeanDefinitions方法。那么这个时候我们可以自行封装BeanDefinition SpringBoot自动装配的核心配置文件有哪些？META-INF/spring.factories 候选 META-INF/spring-autoconfigure-metadata.properties 过滤 SpringBoot自动装配的流程是怎样的？首先，咱们的SpringBoot肯定是执行了Main方法中的run方法，而我们的run方法中会做IOC的初始化，那么我们的SpringBoot显然不会进行配置文件的初始化，而是注解初始化。那么显然我们会将java配置类的类对象传递进去，我们会走到@SpringBootApplication注解，接下来，很显然，起作用的是@EnableAutoConfiguration注解，接下来，这个注解会去加载我们的spring.factories跟spring-autoconfigure-metadata.properties这两个配置文件，进行候选以及筛选的工作，加载进去内存之后，实际上我们会在AutoConfigationImportSelect中加载spring.factories跟spring-autoconfigure-metadata.properties，我们会在返回的时候加载进入容器，这就是自动装配的流程。 bootstrap.yml的意义SpringBoot中默认支持的属性文件有下面4种 application.properties application.xml application.yml aplication.yaml 那么为什么还有一类bootstrap.yml bootstrap.properties文件 bootstrap.yml在SpringBoot中默认是不支持的，需要在SpringCloud环境下才支持，作用是在SpringBoot项目启动之前启动的一个父容器，该父容器可以在SpringBoot容器启动之前完成一些加载初始化的操作。比如加载配置中心中的信息。 运行SpringBoot项目的方式1、 打包用命令或者者放到容器中运行 2、 用 Maven/ Gradle 插件运行 3、 直接执行 main 方法运行 SpringBoot如何解决跨域问题跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 SpringBoot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。 @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"/**\") .allowedOrigins(\"*\") .allowCredentials(true) .allowedMethods(\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\") .maxAge(3600); } SpringBoot中如何配置log4j在引用log4j之前，需要先排除项目创建时候带的日志，因为那个是Logback，然后再引入log4j的依赖，引入依赖之后，去src/main/resources目录下的log4j-spring.properties配置文件，就可以开始对应用的日志进行配置使用。 介绍几个常用的starter1、 spring-boot-starter-web ：提供web开发需要servlet与jsp支持 + 内嵌的 Tomcat 。 2、 spring-boot-starter-data-jpa ：提供 Spring JPA + Hibernate 。 3、 spring-boot-starter-data-Redis ：提供 Redis 。 4、 mybatis-spring-boot-starter ：第三方的mybatis集成starter。 5、spring-boot-starter-data-solr solr支持 SpringBoot的优点Spring Boot 优点非常多，如： 一、独立运行 Spring Boot而且内嵌了各种servlet容器，Tomcat、Jetty等，现在不再需要打成war包部署到容器 中，Spring Boot只要打成一个可执行的jar包就能独立运行，所有的依赖包都在一个jar包内。 二、简化配置 spring-boot-starter-web启动器自动依赖其他组件，简少了maven的配置。 三、自动配置 Spring Boot能根据当前类路径下的类、jar包来自动配置bean，如添加一个spring-boot-starter web启动器就能拥有web的功能，无需其他配置。 四、无代码生成和XML配置 Spring Boot配置过程中无代码生成，也无需XML配置文件就能完成所有配置工作，这一切都是借助 于条件注解完成的，这也是Spring4.x的核心功能之一。 五、应用监控 Spring Boot提供一系列端点可以监控服务及应用，做健康检测 Spring Boot、 Spring MVC 和 Spring 有什么区别？1、Spring Spring最重要的特征是依赖注入。所有 SpringModules 不是依赖注入就是 IOC 控制反转。 当我们恰当的使用 DI 或者是 IOC 的时候，我们可以开发松耦合应用。松耦合应用的单元测试可以很容易的进行。 2、Spring MVC Spring MVC 提供了一种分离式的方法来开发 Web 应用。通过运用像 DispatcherServelet，MoudlAndView 和 ViewResolver 等一些简单的概念，开发 Web 应用将会变的非常简单。 3、SpringBoot Spring 和 SpringMVC 的问题在于需要配置大量的参数。 Spring Boot 通过一个自动配置和启动的项来目解决这个问题。为了更快的构建产品就绪应用程序，Spring Boot 提供了一些非功能性特征。 什么是 Spring Boot Starter ？启动器是一套方便的依赖描述符，它可以放在自己的程序中。你可以一站式的获取你所需要的 Spring 和相关技术，而不需要依赖描述符的通过示例代码搜索和复制黏贴的负载。 例如，如果你想使用 Sping 和 JPA 访问数据库，只需要你的项目包含 spring-boot-starter-data-jpa 依赖项，你就可以完美进行。 如何重新加载Spring Boot上的更改，而无需重新启动服务器？这可以使用DEV工具来实现。通过这种依赖关系，您可以节省任何更改，嵌入式tomcat将重新启动。 Spring Boot有一个开发工具（DevTools）模块，它有助于提高开发人员的生产力。Java开发人员面临的一个主要挑战是将文件更改自动部署到服务器并自动重启服务器。 开发人员可以重新加载Spring Boot上的更改，而无需重新启动服务器。这将消除每次手动部署更改的需要。Spring Boot在发布它的第一个版本时没有这个功能。 这是开发人员最需要的功能。DevTools模块完全满足开发人员的需求。该模块将在生产环境中被禁用。它还提供H2数据库控制台以更好地测试应用程序。 页面自动装载： 同样的，如果你想自动装载页面，有可以看看 FiveReload **RequestMapping 和 GetMapping 的不同之处在哪里？******RequestMapping 具有类属性的，可以进行 GET,POST,PUT 或者其它的注释中具有的请求方法。GetMapping 是 GET 请求方法中的一个特例。它只是 ResquestMapping 的一个延伸，目的是为了提高清晰度。 **我们如何连接一个像 MySQL 或者Orcale 一样的外部数据库？******让我们以 MySQL 为例来思考这个问题： 第一步 - 把 mysql 连接器的依赖项添加至 pom.xml 第二步 - 从 pom.xml 中移除 H2 的依赖项 或者至少把它作为测试的范围。 第三步 - 安装你的 MySQL 数据库 第四步 - 配置你的 MySQL 数据库连接 配置 application.properties rootspring.jpa.hibernate.ddl-auto=none&nbsp; spring.datasource.url=jdbc:mysql://localhost:3306/test spring.datasource.username=root spring.datasource.password=root 第五步 - 重新启动，你就准备好了！ **Spring Boot 需要独立的容器运行吗？******可以不需要，内置了 Tomcat/ Jetty 等容器。 **你如何理解 Spring Boot 中的 Starters？******Starters可以理解为启动器，它包含了一系列可以集成到应用里面的依赖包，你可以一站式集成 Spring 及其他技术，而不需要到处找示例代码和依赖包。如你想使用 Spring JPA 访问数据库，只要加入 spring-boot-starter-data-jpa 启动器依赖就能使用了。 **Spring Boot 支持哪些日志框架？推荐和默认的日志框架是哪个？******Spring Boot 支持 Java Util Logging, Log4j2, Lockback 作为日志框架，如果你使用 Starters 启动器，Spring Boot 将使用 Logback 作为默认日志框架. **SpringBoot 实现热部署有哪几种方式？******主要有两种方式： 1、Spring Loaded 这可以使用DEV工具来实现。通过这种依赖关系，您可以节省任何更改，嵌入式tomcat将重新启动。 Spring Boot有一个开发工具（DevTools）模块，它有助于提高开发人员的生产力。Java开发人员面临的一个主要挑战是将文件更改自动部署到服务器并自动重启服务器。 开发人员可以重新加载Spring Boot上的更改，而无需重新启动服务器。这将消除每次手动部署更改的需要。Spring Boot在发布它的第一个版本时没有这个功能。 这是开发人员最需要的功能。DevTools模块完全满足开发人员的需求。该模块将在生产环境中被禁用。它还提供H2数据库控制台以更好地测试应用程序。 2、Spring-boot-devtools： 同样的，如果你想自动装载页面，有可以看看 FiveReload Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？启动类上面的注解是@SpringBootApplication，它也是 Spring Boot 的核心注解，主要组合包含了以下 3 个注解： ● @SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。 ● @EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项， 如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。 ● @ComponentScan：Spring组件扫描 Spring Boot 有哪几种读取配置的方式Spring Boot默认的配置文件有两种格式: application.properties 和 application.yml。 查找顺序是首先从application.properties 查找， @PropertySource @PropertySource注解用于指定资源文件读取的位置，它不仅能读取properties文件，也能读取xml文件，并且通过YAML解析器，配合自定义PropertySourceFactory实现解析YAML文件。 @Value 使用 @Value 读取配置文件 这种方法适用于对象的参数比较少的情况 我们可以直接在对象的属性上使用 @Value 注解，同时以 ${} 的形式传入配置文件中对应的属性。同时需要在该类的上方使用 @Configuration 注解，将该类作为配置 @Environment Environment 是 SpringCore 中的一个用于读取配置文件的类，将此类使用 @Autowired 注入到类中就可以使用它的getProperty方法来获取某个配置项的值。 @ConfigurationProperties 使用 @ConfigurationProperties 读取配置文件如果对象的参数比较多情况下,推荐使用 @ConfigurationProperties 会更简单一些，不需要在每一个字段的上面的使用@Value注解。 @ConfigurationProperties注解声明当前类为配置读取类 prefix=”rabbitmq” 表示读取前缀为rabbitmq的属性 Spring Boot 如何定义多套不同环境配置？基于properties配置文件第一步创建各环境对应的properties配置文件applcation.properties application-dev.properties application-test.properties application-prod.properties第二步然后在applcation.properties文件中指定当前的环境spring.profiles.active=test,这时候读取的就是application-test.properties文件。基于yml配置文件只需要一个applcation.yml文件就能搞定，推荐此方式。 Spring Boot 可以兼容老 Spring 项目吗，如何做？可以兼容，使用 @ImportResource 注解导入老 Spring 项目配置文件。 如何在 Spring Boot 启动的时候运行一些特定的代码？可以实现接口 ApplicationRunner 或者 CommandLineRunner，这两个接口实现方式一样，它们都只提供了一个 run 方法，实现上述接口的类加入IOC容器即可生效。 你如何理解 Spring Boot 配置加载顺序？1、开发者工具 Devtools 全局配置参数；2、单元测试上的 @TestPropertySource 注解指定的参数；3、单元测试上的 @SpringBootTest 注解指定的参数；4、命令行指定的参数，如 java -jar springboot.jar --name=\"Java技术栈\"；5、命令行中的 SPRING_APPLICATION_JSON 指定参数, 如 java -Dspring.application.json='{\"name\":\"Java技术栈\"}' -jar springboot.jar6、ServletConfig 初始化参数；7、ServletContext 初始化参数；8、JNDI参数（如 java:comp/env/spring.application.json）；9、Java系统参数（来源：System.getProperties()）；10、操作系统环境变量参数；11、RandomValuePropertySource 随机数，仅匹配：ramdom.*；12、JAR包外面的配置文件参数（application-{profile}.properties（YAML））13、JAR包里面的配置文件参数（application-{profile}.properties（YAML））14、JAR包外面的配置文件参数（application.properties（YAML））15、JAR包里面的配置文件参数（application.properties（YAML））16、@Configuration配置文件上 @PropertySource 注解加载的参数；17、默认参数（通过 SpringApplication.setDefaultProperties 指定）； 数字越小优先级越高，即数字小的会覆盖数字大的参数值。 如何实现SpringBoot 应用程序的安全性?为了实现Spring Boot的安全性，我们使用 spring-boot-starter-security依赖项，并且必须添加安全配置。它只需要很少的代码。配置类将必须扩展WebSecurityConfigurerAdapter并覆盖其方法。 SpringBoot中如何实现定时任务?定时任务也是一个常见的需求，Spring Boot 中对于定时任务的支持主要还是来自 Spring 框架。 在 Spring Boot 中使用定时任务主要有两种不同的方式， 一个就是使用 Spring 中的 @Scheduled 注解， 另一个则是使用第三方框架 Quartz。 使用 Spring 中的 @Scheduled 的方式主要通过 @Scheduled 注解来实现。 使用 Quartz ，则按照 Quartz 的方式，定义 Job 和 Trigger 即可。 SpringBoot 中的监视器是什么呢? Spring boot actuator是spring启动框架中的重要功能之一。 Spring boot监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。 有几个指标必须在生产环境中进行检查和监控。 即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。 监视器模块公开了一组可直接作为HTTP URL访问的REST端点来检查状态。 SpringBoot打成的jar和普通jar有什么区别?Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。 Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。 普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \\BOOT-INF\\classes 目录下才是我们的代码，因此无法被直接引用。 如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。","categories":[{"name":"spring","slug":"spring","permalink":"http://example.com/categories/spring/"}],"tags":[{"name":"spring springboot","slug":"spring-springboot","permalink":"http://example.com/tags/spring-springboot/"}]},{"title":"Mybatis基础","slug":"30-Mybatis基础","date":"2023-09-11T06:56:25.000Z","updated":"2023-09-11T10:22:11.253Z","comments":true,"path":"posts/30.html","link":"","permalink":"http://example.com/posts/30.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 一、介绍下MyBatis中的工作原理1。介绍MyBatis的基本情况：ORM 2。原理： MyBatis框架的初始化操作 处理SQL请求的流程 1.系统启动的时候会加载解析全局配置文件和对应映射文件。加载解析的相关信息存储在 Configuration 对象 @Test public void test1() throws Exception{ // 1.获取配置文件 InputStream in = Resources.getResourceAsStream(\"mybatis-config.xml\"); // 2.加载解析配置文件并获取SqlSessionFactory对象 // SqlSessionFactory 的实例我们没有通过 DefaultSqlSessionFactory直接来获取 // 而是通过一个Builder对象来建造的 // SqlSessionFactory 生产 SqlSession 对象的 SqlSessionFactory 应该是单例 // 全局配置文件和映射文件 也只需要在 系统启动的时候完成加载操作 // 通过建造者模式来 构建复杂的对象 1.完成配置文件的加载解析 2.完成SqlSessionFactory的创建 SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); // 3.根据SqlSessionFactory对象获取SqlSession对象 SqlSession sqlSession = factory.openSession(); // 4.通过SqlSession中提供的 API方法来操作数据库 List&lt;User&gt; list = sqlSession.selectList(\"com.boge.mapper.UserMapper.selectUserList\"); // 获取接口的代码对象 得到的其实是 通过JDBC代理模式获取的一个代理对象 // UserMapper mapper = sqlSession.getMapper(UserMapper.class); //List&lt;User&gt; list = mapper.selectUserList(); System.out.println(\"list.size() = \" + list.size()); // 5.关闭会话 sqlSession.close(); // 关闭session 清空一级缓存 } SqlSessionFactory: new DefaultSqlSessionFactory 全局配置文件的加载解析【Configuration】，映射文件的加载解析【Configuration，MappedStatement】 SqlSession：new DefaultSqlSession,创建相关的事务工厂，完成Executor的创建，已经二级缓存 CachingExecutor的装饰，同时完成了插件逻辑的植入。 selectOne(); 二级缓存 -&gt; 一级缓存 –&gt; 数据库插入 SqlSession.getMapper(); 源码结构 二、介绍下MyBatis中的缓存设计1。缓存的作用 缓存的作用：减低数据源的访问频率。从而提高数据源的处理能力。或者提高服务器的响应速度 2。MyBatis中的缓存设计 MyBatis中的缓存的架构设计：装饰器模式 MyBatis中的一级缓存和二级缓存 一级缓存：session级别 二级缓存：SqlSessionFactory级别 缓存的设计 通过装饰模式实现缓存功能扩展 缓存的应用 一级缓存和二级缓存 一级缓存和二级缓存的顺序问题：先二级缓存再一级缓存 为什么会先走二级缓存再走一级缓存？ 二级缓存的作用域是SqlSessionFactory级别-90%找到 一级缓存是SqlSession级别的-5%找到 1 2 2 1 一级缓存开关 二级缓存开关 三、聊下MyBatis中如何实现缓存的扩展1。考察你的MyBatis中缓存架构的理解 2。考察你对MyBatis缓存的扩展。实际动手能力 创建Cache接口的实现。重新getObject和putObject方法 怎么让我们自定义的实现：在cache标签中通过type属性关联我们自定义的Cache接口的实现 四、MyBatis中涉及到的设计模式1。从MyBatis的整体架构设计来分析 基础模块： 缓存模块：装饰器模式 日志模块：适配器模式【策略模式】代理模式 反射模块：工厂模式，装饰器模式 Mapping:代理模式 SqlSessionFactory ：SqlSessionFactoryBuilder 建造者模式 模板方法模式： 五、谈谈你对SqlSessionFactory的理解SqlSessionFactory是MyBatis中非常核心的一个API。是一个SqlSessionFactory工厂。目的是创建SqlSession对象。SqlSessionFactory应该是单例。SqlSessionFactory对象的创建是通过SqlSessionFactoryBuilder来实现。在SqlSessionFactoryBuilder即完成了SqlSessionFactory对象的创建。也完成了全局配置文件和相关的映射文件的加载和解析操作。相关的加载解析的信息会被保存在Configuration对象中。 而且涉及到了两种涉及模式：工厂模式，建造者模式 六、谈谈你对SqlSession的理解SqlSession是MyBatis中非常核心的一个API：作用是通过相关API来实现对应的数据库数据的操作。 SqlSession对象的获取需要通过SqlSessionFactory来实现。是一个会话级别的。当一个新的会话到来的时候。我们需要新建一个SqlSession对象来处理。当一个会话结束后我们需要关闭相关的会话资源。处理请求的方式： 通过相关的增删改查的API直接处理 可以通过getMapper(xxx.class) 来获取相关的mapper接口的代理对象来处理 七、谈谈你对MyBatis的理解MyBatis应该是我们在工作中使用频率最高的一个ORM框架。持久层框架 提供非常方便的API来实现增删改查操作 支持灵活的缓存处理方案，一级缓存、二级缓存，三级缓存 还支持相关的延迟数据加载的处理 还提供了非常多的灵活标签来实现复杂的业务处理，if forech where trim set bind … 相比于Hibernate会更加的灵活 八、谈谈MyBatis中的分页原理1。谈谈分页的理解：数据太多。用户并不需要这么多。我们的内存也放不下这么多的数据 SQL： MySQL：limit Oracle：rowid 2。谈谈MyBatis中的分页实现 在MyBatis中实现分页有两种实现 逻辑分页：RowBounds 物理分页：拦截器实现 九、Spring中是如何解决DefaultSqlSession的数据安全问题的DefaultSqlSession是线程非安全的。也就意味着我们不能够把DefaultSqlSession声明在成员变量中。 在Spring中提供了一个SqlSessionTemplate来实现SqlSession的相关的定义。然后在SqlSessionTemplate中的每个方法都通过SqlSessionProxy来操作。这个是一个动态代理对象。然后在动态代理对象中通过方法级别的DefaultSqlSession来实现相关的数据库的操作 十、谈谈你对MyBatis中的延迟加载的理解延迟加载：等一会加载。在多表关联查询操作的时候可以使用到的一种方案。如果是单表操作就完全没有延迟加载的概念。比如。查询用户和部门信息。如果我们仅仅只是需要用户的信息。而不需要用户对应的部门信息。这时就可以使用延迟加载机制来处理。 1。需要开启延迟加载 2。需要配置多表关联 association 一对一的关联配置 collection 一对多的关联配置 延迟加载的原理：代理对象 十一、谈谈对MyBatis中插件的原理理解MyBatis中的插件设计的目的是什么：方便我们开发人员实现对MyBatis功能的增强 设计中允许我们对： Executor ParameterHandler ResultSetHandler StatementHandler 这四个对象的相关方法实现增强 要实现自定义的拦截器： 创建自定义的Java类。通过@Interceptors注解来定义相关的方法签名 我们需要在对应的配置文件中通过plugins来注册自定义的拦截器 我们可以通过拦截器做哪些操作？ 检查执行的SQL。比如 sql 中有select * . delete from 。。。 对执行的SQL的参数做处理 对查询的结果做装饰处理 对查询SQL的分表处理 十二、使用MyBatis的mapper接口调用时有哪些要求？MyBatis中的Mapper接口实现的本质是代理模式 Mapper映射文件的namespace的值必须是Mapper接口对应的全类路径的名称 Mapper接口中的方法名必须在mapper的映射文件中有对应的sql的id Mapper接口中的入参类型必须和mapper映射文件中的每个sql 的parameterType类型相同 Mapper接口中的出参类型必须和mapper映射文件中的么个sql的resultType类型相同 接口名称和Mapper映射文件同名 十三、如何获取MyBatis中自增的主键需要获取自增的主键：在同一个事务中操作多表。我们需要关联的id信息。 &lt;insert id=\"xxx\" useGeneratedKeys=\"true\" keyProperty=\"id\"&gt; User user = new User(); userMapper.insert(user); System.out.println(\"自增的主键:id\" + user.getId()); 十四、不同Mapper中的id是否可以相同？可以相同：每一个映射文件的namespace都会设置为对应的mapper接口的全类路径名称。也就是保证了每一个Mapper映射文件的namespace是惟一的。那么我们只需要满足在同一个映射文件中的id是不同的就可以了 UserMapper.xml: com.boge.mapper.UserMapper #selectList RoleMapper.xml com.boge.mapper.RoleMapper #selectList 十五、谈谈你对MyBatis的架构设计的理解 接口层：面向开发者。提供相关的API 核心层：MyBatis的核心功能的实现：增删改查操作 基础模块：由很多相互之间没用关联的模块组成。作用是支持核心层来完成核心的功能 十六、传统JDBC的不足和MyBatis的解决方案 我们需要频繁的创建和释放数据库库的连接对象。会造成系统资源的浪费。从而影响系统的性能，针对这种情况我们的解决方案是数据库连接池。然后在MyBatis中的全局配置文件中我们可以设置相关的数据库连接池。当然和Spring整合后我们也可以配置相关的数据库连接。 SQL语句我们是直接添加到了代码中了，造成维护的成本增加。所以对应SQL的动态性要求比较高。这时我们可以考虑把SQL和我们的代码分离，在MyBatis中专门提供了映射文件。我们在映射文件中通过标签来写相关的SQL 向SQL中传递参数也很麻烦，因为SQL语句的where条件不一定。可能有很多值也可能很少。占位符和参数需要一一对应。在MyBatis中自动完成java对象和sql中参数的映射 对于结果集的映射也很麻烦，主要是SQL本身的变化会导致解析的难度。我们的解决方案。在MyBatis中通过ResultSetHandler来自动把结果集映射到对应的Java对象中。 传统的JDBC操作不支持事务。缓存。延迟加载等功能。在MyBatis中都提供了相关的实现 十七、MyBatis编程步骤是怎么样的？ 创建SqlSessionFactory–》SqlSessionFactoryBuilder –》建造成模式 –》Configuration 通过创建的SqlSessionFactory对象来获取SqlSession对象 –》 Executor 通过SqlSession对象执行数据库操作 –》API和Mapper接口代理对象 –》缓存 –》装饰者模式 调用SqlSession中的commit方法来显示的提交事务 –》 数据源和事务模块 –》 JDBC和Managed 调式SqlSession中的close方法来关闭会话 十八、当实体中的属性和表中的字段不一致的情况下怎么办？ 我们可以在对应的SQL语句中通过别名的方式来解决这个问题 我们通过自定义resultMap标签来设置属性和字段的映射关系 十九、谈谈你对MyBatis中的Executor的理解Executor的类型有三类： SIMPLE:默认 SimpleExecutor：每次操作都是一个新的Statement对象 REUSE： ReuseExecutor，会根据SQL缓存Statement对象。实现Statement对象的复用 BATCH： BatchExecutor 批处理 二十、如何设置MyBatis的Executor类型Executor的类型有三类： SIMPLE:默认 SimpleExecutor：每次操作都是一个新的Statement对象 REUSE： ReuseExecutor，会根据SQL缓存Statement对象。实现Statement对象的复用 BATCH： BatchExecutor 批处理 如何指定我们需要使用的类型呢？ 可以通过SqlSessionFactory的openSession方法中来指导对应的处理器类型 可以通过全局配置文件中的settings来配置默认的执行器 二十一、MyBatis中如何实现多个传参1.循序传值public void selectUser(String name,int deptId); &lt;select id=\"selectUser\" resultMap=\"baseResultMap\"&gt; select * from t_user where user_name = #{0} and dept_id= #{1} &lt;/select&gt; #{}里面的数字代表的是入参的顺序 但是这种方法不建议使用，SQL层次表达不直观，而且一旦循序错了很难找到。 2. @Param注解传值public void selectUser(@Param(\"name\")String name,@Param(\"deptId\")int deptId); &lt;select id=\"selectUser\" resultMap=\"baseResultMap\"&gt; select * from t_user where user_name = #{name} and dept_id= #{deptId} &lt;/select&gt; #{}里面的名称对应的就是@Param注解中修饰的名称。 这种方案我们是非常推荐使用的。因为很直观。 3. 通过Map传值public void selectUser(Map&lt;String,Object&gt; map); &lt;select id=\"selectUser\" parameterType=\"java.util.Map\" resultMap=\"baseResultMap\"&gt; select * from t_user where user_name = #{name} and dept_id= #{deptId} &lt;/select&gt; #{}里面的名称就是Map中对应的Key 这种方案适合传递多个参数，且参数灵活应变值得推荐 4.通过自定义对象传递public void selectUser(User user); &lt;select id=\"selectUser\" parameterType=\"com.boge.bean.User\" resultMap=\"baseResultMap\"&gt; select * from t_user where user_name = #{name} and dept_id= #{deptId} &lt;/select&gt; #{} 中的名称就是自定义对象的属性名称 这种方案很直观。但是需要创建一个实体类。扩展不容易。需要添加属性。但是代码的可读性很高。业务逻辑处理也非常方便。值得推荐 二十二、谈谈你对日志模块的理解1。MyBatis中的日志模块使用了适配器模式 2。如果我们需要适配MyBatis没有提供的日志框架。那么对应的需要添加相关的适配类 3。在全局配置文件中设置日志的实现 4。在MyBatis的日志框架中提供了一个 jdbc 这个包。里面实现了JDBC相关操作的日志记录 二十三、谈谈MyBatis中能够记录SQL执行的原理在MyBatis中对执行JDBC操作的日志记录的本质是创建了相关核心对象的代理对象 Connection – ConnectionLogger PreparedStatement – PreparedStatementLogger ResultSet –ResultSetLogger 本质就是通过代理对象来实现的。代理对象中完成相关的日志操作。然后再调用对应的目标对象完成相关的数据库的操作处理。 二十四、MyBatis中数据源模块的设计在MyBatis中单独设计了DataSource这个数据源模块 在使用MyBatis的时候我们都需要单独的设置DataSource 完成相关的DataSource节点的解析 UnpooledDataSource：非数据库连接池的实现 PooledDataSource：数据库连接池的实现 从连接池中获取连接对象：如果有空闲连接直接返回。活跃连接数是否超过了最大连接数。是否有连接超时的连接 数据库连接池关闭连接。如果空闲连接没有超过最大连接数那么就放回空闲队列中。否则关闭真实的连接 二十五、MyBatis中事务模块的设计1。谈谈你对事务的理解【ACID】 2。MyBatis中的事务的管理 事务接口的定义：定义了事务的基本行为 3。在MyBatis的事务管理中有两个选择 jdbc：在MyBatis中自己处理事务的管理 Managed：在MyBatis中没有处理任何的事务操作。这种情况下事务的处理会交给Spring容器来管理 4。如何设置事务管理的方式 5。在MyBatis中执行DML操作事务的处理逻辑 SqlSession.commit(); 二十六、谈谈你对Mapper接口的设计理解1。谈下MyBatis中Mapper接口对应的规则 2。谈下MyBatis中的Mapper接口的设计原理–代理模式的使用 3。代理对象执行的逻辑的本质还是会执行SqlSession中相关的DML操作的方法 4。为什么会多一个代理对象的处理 二十七、谈谈你对Reflector模块的理解Reflector是MyBatis中提供的一个针对反射封装简化的模块：简化反射的相关操作。MyBatis是一个ORM框架。表结构的数据和Java对象中数据的映射。那么不可避免的会存在非常多的反射操作。 Reflector是一个独立的模块。我们是可以把这个模块单独抽取出来直接使用的。 反射模块的具体的设计 二十八、谈谈你对MyBatis中的类型转换模块的理解MyBatis中是如何解决Java中的类型和数据库中字段类型的映射。 类型转换处理器的设计 TypeHandler –》 BaseTypeHandler—》具体的TypeHandler 预处理占位符赋值 二十九、谈谈MyBatis和Spring的整合的理解1。回答的比较简单些。梳理下MyBatis和Spring整合的步骤 单纯的Spring和MyBatis的整合 在SpringBoot项目中的整合 2。重点分析下整合的jar包的原理 MybatisSqlSessionFactoryBean–》 这个就是我们需要关注的重点了。 三十、谈谈你对MyBatis的理解MyBatis是一个非常主流的半自动的ORM框架。非常简便的帮助我们完成相关的数据库操作。 提供动态SQL，缓存和延迟加载等高级功能。 然后整体的架构非常简单 外层接口 核心处理层 基础模块","categories":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://example.com/categories/Mybatis/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://example.com/tags/Mybatis/"}]},{"title":"设计模式","slug":"25-设计模式","date":"2023-09-11T06:55:47.000Z","updated":"2023-09-11T08:27:55.682Z","comments":true,"path":"posts/25.html","link":"","permalink":"http://example.com/posts/25.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 1.说一下开发中需要遵守的设计原则？设计模式中主要有六大设计原则，简称为SOLID ，是由于各个原则的首字母简称合并的来(两个L算一个,solid 稳定的)，六大设计原则分别如下： 1、单一职责原则 单一职责原则的定义描述非常简单，也不难理解。一个类只负责完成一个职责或者功能。也就是说在类的设计中 我们不要设计大而全的类,而是要设计粒度小、功能单一的类。 比如 我们设计一个类里面既包含了用户的一些操作,又包含了支付的一些操作,那这个类的职责就不够单一,应该将该类进行拆分,拆分成多个功能更加单一的,粒度更细的类. 2、开放封闭原则 定义：对扩展开放，对修改关闭 对扩展开放和对修改关闭表示当一个类或一个方法有新需求或者需求发生改变时应该采用扩展的方式而不应该采用修改原有逻辑的方式来实现。因为扩展了新的逻辑如果有问题只会影响新的业务，不会影响老业务；而如果采用修改的方式，很有可能就会影响到老业务受影响。 优点： 新老逻辑解耦，需求发生改变不会影响老业务的逻辑 改动成本最小，只需要追加新逻辑，不需要改的老逻辑 提供代码的稳定性和可扩展性 3、里氏替换原则 要理解里氏替换原则，其实就是要理解两个问题： 什么是替换？ 什么是与期望行为一致的替换（Robert Martin所说的“必须能够替换”）？ 1 ) 什么是替换 ? 替换的前提是面向对象语言所支持的多态特性，同一个行为具有多个不同表现形式或形态的能力。 以JDK的集合框架为例，List接口的定义为有序集合，List接口有多个派生类，比如大家耳熟能详的 ArrayList, LinkedList。那当某个方法参数或变量是 List接口类型时，既可以是 ArrayList的实现, 也可以是 LinkedList的实现，这就是替换。 2 ) 什么是与期望行为一致的替换？ 在不了解派生类的情况下，仅通过接口或基类的方法，即可清楚的知道方法的行为，而不管哪种派生类的实现，都与接口或基类方法的期望行为一致。 不需要关心是哪个类对接口进行了实现,因为不管底层如何实现,最终的结果都会符合接口中关于方法的描述(也就是与接口中方法的期望行为一致). 或者说接口或基类的方法是一种契约，使用方按照这个契约来使用，派生类也按照这个契约来实现。这就是与期望行为一致的替换。 4、接口隔离原则 定义：要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 接口隔离原则与单一职责原则的区别 接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的： 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。 5、依赖倒置原则 定义：依赖倒置原则（Dependence Inversion Principle，DIP）是指在设计代码架构时，高层模块不应该依赖于底层模块，二者都应该依赖于抽象。抽象不应该依赖于细节，细节应该依赖于抽象。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 依赖倒置原则的好处: 减少类间的耦合性，提高系统的稳定性 . (根据类与类之间的耦合度从弱到强排列：依赖关系、关联关系、聚合关系、组合关系、泛化关系和实现关系 ) 降低并行开发引起的风险 (两个类之间有依赖关系，只要制定出两者之间的接口（或抽象类）就可以独立开发了) 提高代码的可读性和可维护性 6、迪米特法则 简单来说迪米特法则想要表达的思想就是: 不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。 如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 软件开发中我们要基于这六个原则,设计建立稳定、灵活、健壮的程序. 2.什么是设计模式？使用过设计模式吗？设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结 在GOF编写的设计模式(可复用面向对象软件的基础)一书中说道: 本书涉及的设计模式并不描述新的或未经证实的设计，我们只收录那些在不同系统中多次使用过的成功设计。 大部分设计模式要解决的都是代码的可重用性、可扩展性问题 如果说数据结构和算法是教你如何写出高效代码，那设计模式讲的是如何写出可扩展、可读、可维护的高质量代码，所以，它们跟平时的编码会有直接的关系，也会直接影响到你的开发能力。 设计模式的好处 不再编写 bullshit-code 提高复杂代码的设计和开发能力 有助于我们读懂源码,学习框架更加事半功倍 GoF设计模式只有23个，但是它们各具特色 ，每个模式都为某一个可重复的设计问题提供了一套解决方案。 根据它们的用途，设计模式可分为 创建型(Creational) ，结构型(Structural) 和行为型(Behavioral) 创建型模式(5种)：提供创建对象的机制，提升已有代码的灵活性和可复用性 常用的有：单例模式、工厂模式（工厂方法和抽象工厂）、建造者模式。 不常用的有：原型模式。 结构型模式(7种)：介绍如何将对象和类组装成较大的结构，并同时保持结构的灵活和高效 常用的有：代理模式、桥接模式、装饰者模式、适配器模式。 不常用的有：门面模式、组合模式、享元模式。 行为模式(11种)：负责对象间的高效沟通和职责传递委派 常用的有：观察者模式、模板模式、策略模式、职责链模式、迭代器模式、状态模式。 不常用的有：访问者模式、备忘录模式、命令模式、解释器模式、中介模式。 3.说一下单例模式，及其应用场景？定义 单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一，此模式保证某个类在运行期间，只有一个实例对外提供服务，而这个类被称为单例类。 单例模式也比较好理解，比如一个人一生当中只能有一个真实的身份证号，一个国家只有一个政府，类似的场景都是属于单例模式。 使用单例模式要做的两件事 保证一个类只有一个实例 为该实例提供一个全局访问节点 单例模式结构 单例的实现 饿汉式 懒汉式 双重检测 静态内部类 枚举方式 应用场景 资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如上述中的日志文件，应用配置。 控制资源的情况下，方便资源之间的互相通信。如线程池等。 4.介绍一下代理模式的种类和它们之间区别？1）静态代理 这种代理方式需要代理对象和目标对象实现一样的接口。 优点：可以在不修改目标对象的前提下扩展目标对象的功能。 缺点： 冗余。由于代理对象要实现与目标对象一致的接口，会产生过多的代理类。 不易维护。一旦接口增加方法，目标对象与代理对象都要进行修改。 2）JDK动态代理 动态代理利用了JDK API,动态地在内存中构建代理对象,从而实现对目标对象的代理功能.动态代理又被称为JDK代理或接口代理. 静态代理与动态代理的区别: 静态代理在编译时就已经实现了,编译完成后代理类是一个实际的class文件 动态代理是在运行时动态生成的,即编译完成后没有实际的class文件,而是在运行时动态生成类字节码,并加载到JVM中. 3）CGLIB 动态代理 cglib (Code Generation Library ) 是一个第三方代码生成类库，运行时在内存中动态生成一个子类对象从而实现对目标对象功能的扩展。cglib 为没有实现接口的类提供代理，为JDK的动态代理提供了很好的补充。 最底层是字节码 ASM是操作字节码的工具 cglib基于ASM字节码工具操作字节码（即动态生成代理，对方法进行增强） SpringAOP基于cglib进行封装，实现cglib方式的动态代理 4）三种代理模式实现方式的对比 jdk代理和CGLIB代理 使用CGLib实现动态代理，CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类，在JDK1.6之前比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的类或者方法进行代理，因为CGLib原理是动态生成被代理类的子类。 在JDK1.6、JDK1.7、JDK1.8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLib代理效率，只有当进行大量调用的时候，JDK1.6和JDK1.7比CGLib代理效率低一点，但是到JDK1.8的时候，JDK代理效率高于CGLib代理。所以如果有接口使用JDK动态代理，如果没有接口使用CGLIB代理。 动态代理和静态代理 动态代理与静态代理相比较，最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理（InvocationHandler.invoke）。这样，在接口方法数量比较多的时候，我们可以进行灵活处理，而不需要像静态代理那样每一个方法进行中转。 如果接口增加一个方法，静态代理模式除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。而动态代理不会出现该问题 5.工厂模式有哪几种，之间有什么区别？在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 《设计模式》一书中，工厂模式被分为了三种：简单工厂、工厂方法和抽象工厂。（不过，在书中作者将简单工厂模式看作是工厂方法模式的一种特例。 1）简单工厂模式 简单工厂不是一种设计模式，反而比较像是一种编程习惯。简单工厂模式又叫做静态工厂方法模式（static Factory Method pattern）,它是通过使用静态方法接收不同的参数来返回不同的实例对象. 实现方式: 定义一个工厂类，根据传入的参数不同返回不同的实例，被创建的实例具有共同的父类或接口。 适用场景： （1）需要创建的对象较少。 （2）客户端不关心对象的创建过程。 优点： 封装了创建对象的过程，可以通过参数直接获取对象。把对象的创建和业务逻辑层分开，这样以后就避免了修改客户代码，如果要实现新产品直接修改工厂类，而不需要在原代码中修改，这样就降低了客户代码修改的可能性，更加容易扩展。 缺点： 增加新产品时还是需要修改工厂类的代码，违背了“开闭原则”。 2）工厂方法模式 工厂方法模式 Factory Method pattern,属于创建型模式. 概念: 定义一个用于创建对象的接口，让子类决定实例化哪个产品类对象。工厂方法使一个产品类的实例化延迟到其工厂的子类。 工厂方法模优缺点 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程； 在系统增加新的产品时只需要添加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原则； 缺点： 每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度。 3）抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）属于创建型模式，它实际上是对工厂方法模式的扩展，相当于一个超级工厂，用于创建其他工厂的模式。 在抽象工厂模式中，接口是负责创建一个相关对象的工厂，而且每个工厂都能按照工厂模式提供对象。其实抽象工厂也是为了减少工厂方法中的子类和工厂类数量，基于此提出的设计模式。 在抽象工厂模式中,每一个具体工厂都提供了多个工厂方法,用于产生多种不同类型的产品 抽象工厂模式优点 对于不同产品系列有比较多共性特征时，可以使用抽象工厂模式，有助于提升组件的复用性. 当需要提升代码的扩展性并降低维护成本时，把对象的创建和使用过程分开，能有效地将代码统一到一个级别上 解决跨平台带来的兼容性问题 抽象工厂模式缺点 增加新的产品等级结构麻烦,需要对原有结构进行较大的修改,甚至需要修改抽象层代码,这显然会带来较大不变,违背了开闭原则. 6.介绍一下观察者设计模式？观察者模式(observer pattern)的原始定义是：定义对象之间的一对多依赖关系，这样当一个对象改变状态时，它的所有依赖项都会自动得到通知和更新。 解释一下上面的定义: 观察者模式它是用于建立一种对象与对象之间的依赖关系,一个对象发生改变时将自动通知其他对象,其他对象将相应的作出反应. 在观察者模式中发生改变的对象称为观察目标,而被通知的对象称为观察者,一个观察目标可以应对多个观察者,而且这些观察者之间可以没有任何相互联系,可以根据需要增加和删除观察者,使得系统更易于扩展. 观察者模式的别名有发布-订阅(Publish/Subscribe)模式,模型-视图(Model-View)模式、源-监听(Source-Listener) 模式等 观察者模式结构中通常包括: 观察目标和观察者两个继承层次结构. 在观察者模式中有如下角色： Subject：抽象主题（抽象被观察者），抽象主题角色把所有观察者对象保存在一个集合里，每个主题都可以有任意数量的观察者，抽象主题提供一个接口，可以增加和删除观察者对象。 ConcreteSubject：具体主题（具体被观察者），该角色将有关状态存入具体观察者对象，在具体主题的内部状态发生改变时，给所有注册过的观察者发送通知。 Observer：抽象观察者，是观察者的抽象类，它定义了一个更新接口，使得在得到主题更改通知时更新自己。 ConcrereObserver：具体观察者，实现抽象观察者定义的更新接口，以便在得到主题更改通知时更新自身的状态。在具体观察者中维护一个指向具体目标对象的引用,它存储具体观察者的有关状态,这些状态需要与具体目标保持一致. 观察者模式的优点 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。 被观察者发送通知，所有注册的观察者都会收到信息【可以实现广播机制】 观察者模式的缺点 如果观察者非常多的话，那么所有的观察者收到被观察者发送的通知会耗时比较多 如果被观察者有循环依赖的话，那么被观察者发送通知会使观察者循环调用，会导致系统崩溃 观察者模式常见的使用场景 当一个对象状态的改变需要改变其他对象时。比如，商品库存数量发生变化时，需要通知商品详情页、购物车等系统改变数量。 一个对象发生改变时只想要发送通知，而不需要知道接收者是谁。比如，订阅微信公众号的文章，发送者通过公众号发送，订阅者并不知道哪些用户订阅了公众号。 需要创建一种链式触发机制时。比如，在系统中创建一个触发链，A 对象的行为将影响 B 对象，B 对象的行为将影响 C 对象……这样通过观察者模式能够很好地实现。 微博或微信朋友圈发送的场景。这是观察者模式的典型应用场景，一个人发微博或朋友圈，只要是关联的朋友都会收到通知；一旦取消关注，此人以后将不会收到相关通知。 7.装饰器模式与代理模式的区别？1）代理模式(Proxy Design Pattern ) 原始定义是：让你能够提供对象的替代品或其占位符。代理控制着对于原对象的访问，并允许将请求提交给对象前后进行一些处理。 代理模式的适用场景 功能增强当需要对一个对象的访问提供一些额外操作时,可以使用代理模式 远程（Remote）代理实际上，RPC 框架也可以看作一种代理模式，GoF 的《设计模式》一书中把它称作远程代理。通过远程代理，将网络通信、数据编解码等细节隐藏起来。客户端在使用 RPC 服务的时候，就像使用本地函数一样，无需了解跟服务器交互的细节。除此之外，RPC 服务的开发者也只需要开发业务逻辑，就像开发本地使用的函数一样，不需要关注跟客户端的交互细节。 防火墙（Firewall）代理当你将浏览器配置成使用代理功能时，防火墙就将你的浏览器的请求转给互联网；当互联网返回响应时，代理服务器再把它转给你的浏览器。 保护（Protect or Access）代理控制对一个对象的访问，如果需要，可以给不同的用户提供不同级别的使用权限。 2）装饰器模式(decorator pattern) 的原始定义是：动态的给一个对象添加一些额外的职责. 就扩展功能而言,装饰器模式提供了一种比使用子类更加灵活的替代方案. 装饰器模式的适用场景 快速动态扩展和撤销一个类的功能场景。 比如，有的场景下对 API 接口的安全性要求较高，那么就可以使用装饰模式对传输的字符串数据进行压缩或加密。如果安全性要求不高，则可以不使用。 不支持继承扩展类的场景。 比如，使用 final 关键字的类，或者系统中存在大量通过继承产生的子类。 装饰器模式与代理模式的区别 对装饰器模式来说，装饰者（decorator）和被装饰者（decoratee）都实现同一个 接口。 对代理模式来说，代理类（proxy class）和真实处理的类（real class）都实现同一个接口。 他们之间的边界确实比较模糊，两者都是对类的方法进行扩展，具体区别如下： 装饰器模式强调的是增强自身，在被装饰之后你能够在被增强的类上使用增强后的功能。增强后你还是你，只不过能力更强了而已；代理模式强调要让别人帮你去做一些本身与你业务没有太多关系的职责（记录日志、设置缓存）。代理模式是为了实现对象的控制，因为被代理的对象往往难以直接获得或者是其内部不想暴露出来。 装饰模式是以对客户端透明的方式扩展对象的功能，是继承方案的一个替代方案；代理模式则是给一个对象提供一个代理对象，并由代理对象来控制对原有对象的引用； 装饰模式是为装饰的对象增强功能；而代理模式对代理的对象施加控制，但不对对象本身的功能进行增强； 8.JDK 类库常用的设计模式有哪些？1）抽象工厂 javax.xml.parsers.DocumentBuilderFactory抽象类 public static DocumentBuilderFactory newInstance()方法 类功能：使得应用程序可以通过XML文件，获得一个能生成DOM对象的解析器。 方法功能：获取一个DocumentBuilderFactory的新实例。这一静态方法会创建一个新的工厂实例。 2）建造者模式 java.lang.StringBuilder，这是一个final类。 public StringBuilder append(String str)方法，这一方法是对父类的覆写。 类功能：用于一个不可更改的字符序列。 方法功能：根据现有字符序列和追加字符，通过系统拷贝方法System.arraycopy生成一个新的字符序列。 3）工厂模式 java.text.NumberFormat抽象类。 public final static NumberFormat getInstance()方法。 类功能：用于数字格式的抽象基类。 方法功能：返回一个“对当前默认场景下的一个通用数字格式”的NumberFormat。显然属于工厂模式的使用。 4）原型模式 java.lang.Object protected native Object clone() 方法 类功能：所有类的父类 方法功能：根据现有实例，返回一个浅拷贝对象。 5）单例模式 java.lang.RunTime类 public static Runtime getRuntime() 类功能：每一个运行的java应用都会有一个唯一的RunTime类的实例，这个实例使得应用程序在运行期间能够受到运行环境的影响。 方法功能：返回一个和当前java应用关联的RunTime对象。 6）适配器模式 java.util.Arrays。 public static List asList(T… a)方法。 类功能：此类包含了大量对数组操作的方法。 方法功能：将一个引用类型的数组转为一个List。从而可以使用List类的操作来操作数组对象，但是有一点要注意：就是不能使用add(),remove()操作，因为返回的list底层是基于数组的，数组结构是不能更改的。 list类就是这里的适配器，通过这个适配器，对数组的直接操作变为间接操作。 9.Mybatis框架中使用的设计模式有哪些？Builder模式 在Mybatis环境的初始化过程中，SqlSessionFactoryBuilder会调用 XMLConfigBuilder读取所有的 MybatisMapConfig.xml和所有的 *Mapper.xml文件，构建Mybatis运行的核心对象 Configuration对象，然后将该 Configuration对象作为参数构建一个 SqlSessionFactory对象。 工厂模式 在Mybatis中比如 SqlSessionFactory使用的是工厂模式，该工厂没有那么复杂的逻辑，是一个简单工厂模式。 SqlSession可以认为是一个Mybatis工作的核心的接口，通过这个接口可以执行执行SQL语句、获取Mappers、管理事务。类似于连接MySQL的 Connection对象。 单例模式 在Mybatis中有两个地方用到单例模式，ErrorContext和 LogFactory，其中 ErrorContext是用在每个线程范围内的单例，用于记录该线程的执行环境错误信息，而 LogFactory则是提供给整个Mybatis使用的日志工厂，用于获得针对项目配置好的日志对象。 public class ErrorContext { private static final ThreadLocal&lt;ErrorContext&gt; LOCAL = new ThreadLocal&lt;&gt;(); private ErrorContext() { } public static ErrorContext instance() { ErrorContext context = LOCAL.get(); if (context == null) { context = new ErrorContext(); LOCAL.set(context); } return context; } } 构造函数是private修饰，具有一个static的局部instance变量和一个获取instance变量的方法，在获取实例的方法中，先判断是否为空如果是的话就先创建，然后返回构造好的对象。 只是这里有个有趣的地方是，LOCAL的静态实例变量使用了 ThreadLocal修饰，也就是说它属于每个线程各自的数据，而在 instance()方法中，先获取本线程的该实例，如果没有就创建该线程独有的 ErrorContext。 代理模式 代理模式可以认为是Mybatis的核心使用的模式，正是由于这个模式，我们只需要编写 Mapper.java接口，不需要实现，由Mybatis后台帮我们完成具体SQL的执行。 适配器模式 在Mybatsi的logging包中，有一个Log接口： 该接口定义了Mybatis直接使用的日志方法，而Log接口具体由谁来实现呢？Mybatis提供了多种日志框架的实现，这些实现都匹配这个Log接口所定义的接口方法，最终实现了所有外部日志框架到Mybatis日志包的适配。 10.Spring框架中使用的设计模式有哪些？1）简单工厂 BeanFactory。Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 2）工厂方法 FactoryBean接口 实现了FactoryBean接口的bean是一类叫做factory的bean。其特点是，spring会在使用getBean()调用获得该bean时，会自动调用该bean的getObject()方法，所以返回的不是factory这个bean，而是这个bean.getOjbect()方法的返回值。 3）单例模式 Spring依赖注入Bean实例默认是单例的。 Spring的依赖注入（包括lazy-init方式）都是发生在AbstractBeanFactory的getBean里。getBean的doGetBean方法调用getSingleton进行bean的创建。 4）适配器模式 SpringMVC中的适配器HandlerAdatper。 HandlerAdatper使得Handler的扩展变得容易，只需要增加一个新的Handler和一个对应的HandlerAdapter即可。 因此Spring定义了一个适配接口，使得每一种Controller有一种对应的适配器实现类，让适配器代替controller执行相应的方法。这样在扩展Controller时，只需要增加一个适配器类就完成了SpringMVC的扩展了。 5）装饰器模式 Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。 动态地给一个对象添加一些额外的职责。 就增加功能来说，Decorator模式相比生成子类更为灵活。 6）代理模式 AOP底层，就是动态代理模式的实现。 7）观察者模式 Spring 基于观察者模式，实现了自身的事件机制也就是事件驱动模型，事件驱动模型通常也被理解成观察者或者发布/订阅模型。 8）策略模式 Spring框架的资源访问Resource接口。该接口提供了更强的资源访问能力，Spring 框架本身大量使用了 Resource 接口来访问底层资源。 Rsource 接口是具体资源访问策略的抽象，也是所有资源访问类所实现的接口。 Resource 接口本身没有提供访问任何底层资源的实现逻辑，针对不同的底层资源，Spring 将会提供不同的 Resource 实现类，不同的实现类负责不同的资源访问逻辑。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"JVM严镇涛版","slug":"24-JVM严镇涛版","date":"2023-09-11T06:55:18.000Z","updated":"2023-09-11T08:26:18.474Z","comments":true,"path":"posts/24.html","link":"","permalink":"http://example.com/posts/24.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 枫叶云链接：http://cloud.fynote.com/s/4976 JVM面试题大全 Lecturer ：严镇涛1.为什么需要JVM，不要JVM可以吗？1.JVM可以帮助我们屏蔽底层的操作系统 一次编译，到处运行 2.JVM可以运行Class文件 2.JDK，JRE以及JVM的关系 3.我们的编译器到底干了什么事？仅仅是将我们的 .java 文件转换成了 .class 文件，实际上就是文件格式的转换，对等信息转换。 4.类加载机制 类加载机制其实就是虚拟机把Class文件加载到内存，并对数据进行校验，转换解析和初始化，形成可以虚拟机直接使用的Java类型，即java.lang.Class。 1.装载 Class文件 – &gt;二进制字节流 –&gt;类加载器 1）通过一个类的全限定名获取这个类的二进制字节流 2）将这个字节流所代表的静态存储结构转换为方法区的运行时数据结构 3）在java堆中生成一个代表这个类的java.lang.Class对象，做为我们方法区的数据访问入口 2.链接： 1）验证：保证我们加载的类的正确性 文件格式验证 元数据验证 字节码验证 符号引用验证 2）准备 为类的静态变量分配内存，并将其初始化为当前类型的默认值。 private static int a = 1 ； 那么他在准备这个阶段 a = 0； 3）解析 解析是从运行时常量池中的符号引用动态确定具体值的过程。 把类中的符号引用转换成直接引用 3.初始化 执行到Clinit方法，为静态变量赋值，初始化静态代码块，初始化当前类的父类 5.类加载器的层次 6.双亲委派机制父类委托机制 源码 String 自己写 String protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } 7.如何打破双亲委派1.复写 2.SPI Service Provider Interface 服务提供接口 日志 Xml解析 JBDC 可拔插设计 可以随时替换实现 3.OSGI 热部署 热更新 8.运行时数据区 1.方法区 线程共享 方法区是逻辑上堆的一部分 所以他有个名字：非堆 运行时常量池、字段和方法数据，以及方法和构造函数的代码，包括类和实例初始化和接口初始化中使用 的特殊方法 如果方法区域中的内存无法满足分配请求，Java 虚拟机将抛出一个 OutOfMemoryError 2.堆 线程共享 堆是为所有类实例和数组分配内存的运行时数据区域 内存不足 OutOfMemoryError 3.java虚拟机栈 执行java方法的 线程私有的 StackOverflowError 4.本地方法栈 执行本地方法 线程私有 StackOverflowError 5.程序计数器 记录程序执行到的位置 线程私有 9.栈帧结构是什么样子的？ 附加信息：栈帧的高度，虚拟机版本信息 栈帧信息：附加信息+动态链接+方法的返回地址 局部变量表：方法中定义的局部变量以及方法的参数都会存放在这张表中 ，单纯的存储单元 操作数栈 以压栈以及出栈的方式存储操作数 举例： int a = 1； int b = 1； int c = a + b ； 方法的返回地址：当你一个方法执行的时候，只有两种方式可以推出 1.遇到方法的返回的字节码指令 2.出现了异常，有异常处理器，则交给异常处理器 ，没有呢？抛异常 10.动态链接动态链接是为了支持方法的动态调用过程 。 动态链接将这些符号方法引用转换为具体的方法引用 符号引用转变为直接引用 为了支持java的多态 void a(){ b(); } void b(){ c(); } void c(){ } 11.java堆为什么要进行分代设计 新老年代划分 Eden区与S区 12.老年代的担保机制13.为什么Eden：S0：S1 是8：1：198%的对象朝生夕死 14.对象的创建过程 15.方法区与元数据区以及持久代到底是什么关系方法区 JVM规范 落地：JDK1.7之前 持久代 Perm Space JVM虚拟机自己的内存 JDK1.8之后 元数据区 / 元空间 MetaSpace 直接内存 16.什么时候才会进行垃圾回收 GC是由JVM自动完成的，根据JVM系统环境而定，所以时机是不确定的。当然，我们可以手动进行垃圾回收，比如调用System.gc()方法通知JVM进行一次垃圾回收，但是具体什么时刻运行也无法控制。也就是说System.gc()只是通知要回收，什么时候回收由JVM决定。但是不建议手动调用该方法，因为GC消耗的资源比较大。 （1）当Eden区或者S区不够用了 （2）老年代空间不够用了 （3）方法区空间不够用了 （4）System.gc() //通知 时机也不确定 执行的Full GC 17. 如何确定一个对象是垃圾？ 要想进行垃圾回收，得先知道什么样的对象是垃圾。 引用计数法 对于某个对象而言，只要应用程序中持有该对象的引用，就说明该对象不是垃圾，如果一个对象没有任何指针对其引用，它就是垃圾。 弊端:如果AB相互持有引用，导致永远不能被回收。 循环引用 内存泄露 –&gt;内存溢出 可达性分析/根搜索算法 通过GC Root的对象，开始向下寻找，看某个对象是否可达 能作为GC Root:类加载器、Thread、虚拟机栈的本地变量表、static成员、常量引用、本地方法栈的变量等。 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。 18.对象被判定为不可达对象之后就“死”了吗 垃圾收集算法 已经能够确定一个对象为垃圾之后，接下来要考虑的就是回收，怎么回收呢？得要有对应的算法，下面介绍常见的垃圾回收算法。高效 健壮 标记-清除(Mark-Sweep) 标记 找出内存中需要回收的对象，并且把它们标记出来 此时堆中所有的对象都会被扫描一遍，从而才能确定需要回收的对象，比较耗时 清除 清除掉被标记需要回收的对象，释放出对应的内存空间 缺点 标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程 序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 (1)标记和清除两个过程都比较耗时，效率不高 (2)会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-复制(Mark-Copying)将内存划分为两块相等的区域，每次只使用其中一块，如下图所示： 当其中一块内存使用完了，就将还存活的对象复制到另外一块上面，然后把已经使用过的内存空间一次清除掉。 缺点:空间利用率降低。 标记-整理(Mark-Compact) 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都有100%存活的极端情况，所以老年代一般不能直接选用这种算法。 标记过程仍然与”标记-清除”算法一样，但是后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 其实上述过程相对”复制算法”来讲，少了一个”保留区” 让所有存活的对象都向一端移动，清理掉边界意外的内存。 分代收集算法 既然上面介绍了3中垃圾收集算法，那么在堆内存中到底用哪一个呢？ Young区：复制算法(对象在被分配之后，可能生命周期比较短，Young区复制效率比较高) Old区：标记清除或标记整理(Old区对象存活时间比较长，复制来复制去没必要，不如做个标记再清理) 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 Serial Serial收集器是最基本、发展历史最悠久的收集器，曾经（在JDK1.3.1之前）是虚拟机新生代收集的唯一选择。 它是一种单线程收集器，不仅仅意味着它只会使用一个CPU或者一条收集线程去完成垃圾收集工作，更重要的是其在进行垃圾收集的时候需要暂停其他线程。 优点：简单高效，拥有很高的单线程收集效率 缺点：收集过程需要暂停所有线程 算法：复制算法 适用范围：新生代 应用：Client模式下的默认新生代收集器 Serial Old Serial Old收集器是Serial收集器的老年代版本，也是一个单线程收集器，不同的是采用”标记-整理算法“，运行过程和Serial收集器一样。 ParNew 可以把这个收集器理解为Serial收集器的多线程版本。 优点：在多CPU时，比Serial效率高。 缺点：收集过程暂停所有应用程序线程，单CPU时比Serial效率差。 算法：复制算法 适用范围：新生代 应用：运行在Server模式下的虚拟机中首选的新生代收集器 Parallel Scavenge Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去和ParNew一样，但是Parallel Scanvenge更关注系统的吞吐量。 吞吐量=运行用户代码的时间/(运行用户代码的时间+垃圾收集时间) 比如虚拟机总共运行了100分钟，垃圾收集时间用了1分钟，吞吐量=(100-1)/100=99%。 若吞吐量越大，意味着垃圾收集的时间越短，则用户代码可以充分利用CPU资源，尽快完成程序的运算任务。 -XX:MaxGCPauseMillis控制最大的垃圾收集停顿时间， -XX:GCRatio直接设置吞吐量的大小。 Parallel Old Parallel Old收集器是Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法进行垃圾回收，也是更加关注系统的吞吐量。 CMS 官网： https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/cms.html#concurrent_mark_sweep_cms_collector CMS(Concurrent Mark Sweep)收集器是一种以获取 最短回收停顿时间为目标的收集器。 采用的是”标记-清除算法”,整个过程分为4步 (1)初始标记 CMS initial mark &nbsp; &nbsp; 标记GC Roots直接关联对象，不用Tracing，速度很快 (2)并发标记 CMS concurrent mark 进行GC Roots Tracing (3)重新标记 CMS remark &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 修改并发标记因用户程序变动的内容 (4)并发清除 CMS concurrent sweep 清除不可达对象回收空间，同时有新垃圾产生，留着下次清理称为浮动垃圾 由于整个过程中，并发标记和并发清除，收集器线程可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行的。 优点：并发收集、低停顿 缺点：产生大量空间碎片、并发阶段会降低吞吐量 G1(Garbage-First) 官网： https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc.html#garbage_first_garbage_collection **使用G1收集器时，Java堆的内存布局与就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。 ** 每个Region大小都是一样的，可以是1M到32M之间的数值，但是必须保证是2的n次幂 如果对象太大，一个Region放不下[超过Region大小的50%]，那么就会直接放到H中 设置Region大小：-XX:G1HeapRegionSize=M 所谓Garbage-Frist，其实就是优先回收垃圾最多的Region区域 （1）分代收集（仍然保留了分代的概念） （2）空间整合（整体上属于“标记-整理”算法，不会导致空间碎片） （3）可预测的停顿（比CMS更先进的地方在于能让使用者明确指定一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒） 工作过程可以分为如下几步 初始标记（Initial Marking） &nbsp; &nbsp; 标记以下GC Roots能够关联的对象，并且修改TAMS的值，需要暂停用户线程 并发标记（Concurrent Marking） &nbsp; 从GC Roots进行可达性分析，找出存活的对象，与用户线程并发执行 最终标记（Final Marking） &nbsp; &nbsp; &nbsp; 修正在并发标记阶段因为用户程序的并发执行导致变动的数据，需暂停用户线程 筛选回收（Live Data Counting and Evacuation） 对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间制定回收计划 ZGC 官网： https://docs.oracle.com/en/java/javase/11/gctuning/z-garbage-collector1.html#GUID-A5A42691-095E-47BA-B6DC-FB4E5FAA43D0 JDK11新引入的ZGC收集器，不管是物理上还是逻辑上，ZGC中已经不存在新老年代的概念了 会分为一个个page，当进行GC操作时会对page进行压缩，因此没有碎片问题 只能在64位的linux上使用，目前用得还比较少 （1）可以达到10ms以内的停顿时间要求 （2）支持TB级别的内存 （3）堆内存变大后停顿时间还是在10ms以内 垃圾收集器分类 串行收集器-&gt;Serial和Serial Old 只能有一个垃圾回收线程执行，用户线程暂停。 适用于内存比较小的嵌入式设备。 并行收集器[吞吐量优先]-&gt;Parallel Scanvenge、Parallel Old、ParNeww 多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 适用于科学计算、后台处理等若交互场景。 并发收集器[停顿时间优先]-&gt;CMS、G1、ZGC 用户线程和垃圾收集线程同时执行(但并不一定是并行的，可能是交替执行的)，垃圾收集线程在执行的时候不会停顿用户线程的运行。 适用于相对时间有要求的场景，比如Web。 吞吐量和停顿时间 **停顿时间-&gt;垃圾收集器 **进行 垃圾回收终端应用执行响应的时间 吞吐量-&gt;运行用户代码时间/(运行用户代码时间+垃圾收集时间) 停顿时间越短就越适合需要和用户交互的程序，良好的响应速度能提升用户体验； 高吞吐量则可以高效地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 小结:这两个指标也是评价垃圾回收器好处的标准。 生产环境中，如何选择合适的垃圾收集器 https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/collectors.html#sthref28 优先调整堆的大小让服务器自己来选择 如果内存小于100M，使用串行收集器 如果是单核，并且没有停顿时间要求，使用串行或JVM自己选 如果允许停顿时间超过1秒，选择并行或JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器 如何判断是否使用G1垃圾收集器 https://docs.oracle.com/javase/8/docs/technotes/guides/vm/G1.html#use_cases JDK 7开始使用，JDK 8非常成熟，JDK 9默认的垃圾收集器，适用于新老生代。 是否使用G1收集器？ （1）50%以上的堆被存活对象占用 （2）对象分配和晋升的速度变化非常大 （3）垃圾回收时间比较长 JVM常用命令 jps 查看java进程 The jps command lists the instrumented Java HotSpot VMs on the target system. The command is limited to reporting information on JVMs for which it has the access permissions. jinfo （1）实时查看和调整JVM配置参数 The jinfo command prints Java configuration information for a specified Java process or core file or a remote debug server. The configuration information includes Java system properties and Java Virtual Machine (JVM) command-line flags. （2）查看用法 jinfo -flag name PID 查看某个java进程的name属性的值 jinfo -flag MaxHeapSize PID jinfo -flag UseG1GC PID 3）修改 参数只有被标记为manageable的flags可以被实时修改 jinfo -flag [+|-] PID jinfo -flag &lt;name&gt;=&lt;value&gt; PID （4）查看曾经赋过值的一些参数 jinfo -flags PID jstat （1）查看虚拟机性能统计信息 The jstat command displays performance statistics for an instrumented Java HotSpot VM. The target JVM is identified by its virtual machine identifier, or vmid option. （2）查看类装载信息 jstat -class PID 1000 10 &nbsp; 查看某个java进程的类装载信息，每1000毫秒输出一次，共输出10次 （3）查看垃圾收集信息 jstat -gc PID 1000 10 jstack （1）查看线程堆栈信息 The jstack command prints Java stack traces of Java threads for a specified Java process, core file, or remote debug server. （2）用法 jstack PID JVM死锁情况分析//运行主类 public class DeadLockDemo { public static void main(String[] args) { DeadLock d1=new DeadLock(true); DeadLock d2=new DeadLock(false); Thread t1=new Thread(d1); Thread t2=new Thread(d2); t1.start(); t2.start(); } } //定义锁对象 class MyLock{ public static Object obj1=new Object(); public static Object obj2=new Object(); } //死锁代码 class DeadLock implements Runnable{ private boolean flag; DeadLock(boolean flag){ this.flag=flag; } public void run() { if(flag) { while(true) { synchronized(MyLock.obj1) { System.out.println(Thread.currentThread().getName()+\"----if获得obj1锁\"); synchronized(MyLock.obj2) { System.out.println(Thread.currentThread().getName()+\"----if获得obj2锁\"); } } } } else { while(true){ synchronized(MyLock.obj2) { System.out.println(Thread.currentThread().getName()+\"----否则获得obj2锁\"); synchronized(MyLock.obj1) { System.out.println(Thread.currentThread().getName()+\"----否则获得obj1锁\"); } } } } } } jmap （1）生成堆转储快照 The jmap command prints shared object memory maps or heap memory details of a specified process, core file, or remote debug server. （2）打印出堆内存相关信息 jmap -heap PID jinfo -flag UsePSAdaptiveSurvivorSizePolicy 35352 -XX:SurvivorRatio=8 G1调优策略 （1）不要手动设置新生代和老年代的大小，只要设置整个堆的大小 why：https://blogs.oracle.com/poonam/increased-heap-usage-with-g1-gc G1收集器在运行过程中，会自己调整新生代和老年代的大小 其实是通过adapt代的大小来调整对象晋升的速度和年龄，从而达到为收集器设置的暂停时间目标 如果手动设置了大小就意味着放弃了G1的自动调优 （2）不断调优暂停时间目标 一般情况下这个值设置到100ms或者200ms都是可以的(不同情况下会不一样)，但如果设置成50ms就不太合理。暂停时间设置的太短，就会导致出现G1跟不上垃圾产生的速度。最终退化成Full GC。所以对这个参数的调优是一个持续的过程，逐步调整到最佳状态。暂停时间只是一个目标，并不能总是得到满足。 （3）使用-XX:ConcGCThreads=n来增加标记线程的数量 IHOP如果阀值设置过高，可能会遇到转移失败的风险，比如对象进行转移时空间不足。如果阀值设置过低，就会使标记周期运行过于频繁，并且有可能混合收集期回收不到空间。 IHOP值如果设置合理，但是在并发周期时间过长时，可以尝试增加并发线程数，调高ConcGCThreads。 **（4）MixedGC调优 ** -XX:InitiatingHeapOccupancyPercent -XX:G1MixedGCLiveThresholdPercent -XX:G1MixedGCCountTarger -XX:G1OldCSetRegionThresholdPercent （5）适当增加堆内存大小 （6）不正常的Full GC 有时候会发现系统刚刚启动的时候，就会发生一次Full GC，但是老年代空间比较充足，一般是由Metaspace区域引起的。可以通过MetaspaceSize适当增加其大家，比如256M。 JVM性能优化指南","categories":[{"name":"JVM(Java虚拟机)","slug":"JVM-Java虚拟机","permalink":"http://example.com/categories/JVM-Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"JVM java","slug":"JVM-java","permalink":"http://example.com/tags/JVM-java/"}]},{"title":"分布式基础","slug":"23-分布式基础","date":"2023-09-11T06:54:30.000Z","updated":"2023-09-11T08:24:53.465Z","comments":true,"path":"posts/23.html","link":"","permalink":"http://example.com/posts/23.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) 分布式幂等性如何设计？在高并发场景的架构里，幂等性是必须得保证的。比如说支付功能，用户发起支付，如果后台没有 做幂等校验，刚好用户手抖多点了几下，于是后台就可能多次受到同一个订单请求，不做幂等很容 易就让用户重复支付了，这样用户是肯定不能忍的。 解决方案：。 1，查询和删除不在幂等讨论范围，查询肯定没有幂等的说，删除：第一次删除成功后，后面来删 除直接返回0，也是返回成功。 2，建唯一索引：唯一索引或唯一组合索引来防止新增数据存在脏数据 （当表存在唯一索引，并发 时新增异常时，再查询一次就可以了，数据应该已经存在了，返回结果即可）。 3，token机制：由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交。前端 在数据提交前要向后端服务的申请token，token放到 Redis 或 JVM 内存，token有效时间。提交后 后台校验token，同时删除token，生成新的token返回。redis要用删除操作来判断token，删除成 功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用。 4，悲观锁悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用（另外还要考 虑id是否为主键，如果id不是主键或者不是 InnoDB 存储引擎，那么就会出现锁全表）。for update 5，乐观锁，给数据库表增加一个version字段，可以通过这个字段来判断是否已经被修改了 6，分布式锁，比如 Redis 、 Zookeeper 的分布式锁。单号为key，然后给Key设置有效期（防止支 付失败后，锁一直不释放），来一个请求使用订单号生成一把锁，业务代码执行完成后再释放锁。 7，保底方案，先查询是否存在此单，不存在进行支付，存在就直接返回支付结果。 说说你对分布式事务的理解？场景：多个服务或者多个库，要保持在一个事务中。 分布式事务是企业集成中的一个技术难点，也是每一个分布式系统架构中都会涉及到的一个东西， 特别是在微服务架构中，几乎可以说是无法避免。 理论：ACID、CAP、BASE。 ACID 指数据库事务正确执行的四个基本要素： 原子性（Atomicity） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） CAP：cp，ap。 CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性 （Availability）、分区容忍性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同 时实现两点，不可能三者兼顾。一致性：在分布式系统中的所有数据备份，在同一时刻是否同样的值。 可用性：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。 分区容忍性：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数 据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 BASE理论 BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到 强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） 解决方案：seata，消息队列+本地事件表，事务消息，最大努力通知方案，tcc 什么是两阶段提交？两阶段提交2PC是分布式事务中最强大的事务类型之一， 流程： 两段提交就是分两个阶段提交： 第一阶段询问各个事务数据源是否准备好，投票阶段。 第二阶段才真正将数据提交给事务数据源。 为了保证该事务可以满足ACID，就要引入一个协调者（Cooradinator）。其他的节点被称为参与者 （Participant）。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务进行提交。 处理流程如下： 阶段一 a) 协调者向所有参与者发送事务内容，询问是否可以提交事务，并等待答复。 b) 各参与者执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务）。 c) 如参与者执行成功，给协调者反馈 yes，否则反馈 no。 阶段二 如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(rollback)消息；否则， 发送提交(commit)消息。两种情况处理如下： 情况1：当所有参与者均反馈 yes，提交事务 a) 协调者向所有参与者发出正式提交事务的请求（即 commit 请求）。 b) 参与者执行 commit 请求，并释放整个事务期间占用的资源。 c) 各参与者向协调者反馈 ack(应答)完成的消息。 d) 协调者收到所有参与者反馈的 ack 消息后，即完成事务提交。 情况2：当有一个参与者反馈 no，回滚事务 a) 协调者向所有参与者发出回滚请求（即 rollback 请求）。 b) 参与者使用阶段 1 中的 undo 信息执行回滚操作，并释放整个事务期间占用的资源。 c) 各参与者向协调者反馈 ack 完成的消息。 d) 协调者收到所有参与者反馈的 ack 消息后，即完成事务。 问题： 性能问题：所有参与者在事务提交阶段处于同步阻塞状态，占用系统资源，容易导致性能瓶颈。 可靠性问题：如果协调者存在单点故障问题，或出现故障，提供者将一直处于锁定状态。 数据一致性问题：在阶段 2 中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一 致。 优点：尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证 强一致）。 缺点：实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景。 优化： 举例子。 案例：seata，lcn，tcc。 什么是补偿性事务？补偿性事务是什么： TCC （Try Confifirm Cancel）是服务化的二阶段编程模型，采用的补偿机制： TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补 偿（撤销）操作。 它分为三个步骤： 1。Try 阶段主要是对业务系统做检测及资源预留。 2。Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默 认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 3。Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 使用场景： 业务需要： 举个例子，假入你要向A 转账，思路大概是： 我们有一个本地方法，里面依次调用步骤： 1、首先在 Try 阶段，要先调用远程接口把 你 和 A 的钱给冻结起来。 2、在 Confifirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 技术需要： 不同组件，无法在一个事务中完成。 优点： 性能提升：具体业务来实现控制资源锁的粒度变小，不会锁定整个资源。 数据最终一致性：基于 Confirm 和 Cancel 的幂等性，保证事务最终完成确认或者取消，保证数据 的一致性。 可靠性：解决了 XA 协议的协调者单点故障问题，由主业务方发起并控制整个业务活动，业务活动 管理器也变成多点，引入集群。 缺点： TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。 消息队列和事件表实现分布式事务 分布式ID生成有几种方案？分布式ID的特性唯一性：确保生成的ID是全网唯一的。 有序递增性：确保生成的ID是对于某个用户或者业务是按一定的数字有序递增的。 高可用性：确保任何时候都能正确的生成ID。 带时间：ID里面包含时间，不容易重复。 UUID算法的核心思想是结合机器的网卡、当地时间、一个随记数来生成UUID。 优点：本地生成，生成简单，性能好，没有高可用风险 缺点：长度过长，存储冗余，且无序不可读，查询效率低 数据库自增ID使用数据库的id自增策略，如 MySQL 的 auto_increment。并且可以使用两台数据库分别设置不同 步长，生成不重复ID的策略来实现高可用。 优点：数据库生成的ID绝对有序，高可用实现方式简单 缺点：需要独立部署数据库实例，成本高，有性能瓶颈 批量生成ID一次按需批量生成多个ID，每次生成都需要访问数据库，将数据库修改为最大的ID值，并在内存中 记录当前值及最大值。 优点：避免了每次生成ID都要访问数据库并带来压力，提高性能 缺点：属于本地生成策略，存在单点故障，服务重启造成ID不连续 Redis生成IDRedis的所有命令操作都是单线程的，本身提供像 incr 和 increby 这样的自增原子命令，所以能保 证生成的 ID 肯定是唯一有序的。 优点：不依赖于数据库，灵活方便，且性能优于数据库；数字ID天然排序，对分页或者需要排 序的结果很有帮助。 缺点：如果系统中没有Redis，还需要引入新的组件，增加系统复杂度；需要编码和配置的工作 量比较大。 考虑到单节点的性能瓶颈，可以使用 Redis 集群来获取更高的吞吐量。假如一个集群中有5台 Redis。可以初始化每台 Redis 的值分别是1, 2, 3, 4, 5，然后步长都是 5。 Twitter的snowflflake算法（重点）Twitter 利用 zookeeper 实现了一个全局ID生成的服务 Snowflflake 如上图的所示，Twitter 的 Snowflflake 算法由下面几部分组成： 1位符号位： 由于 long 类型在 java 中带符号的，最高位为符号位，正数为 0，负数为 1，且实际系统中所使用 的ID一般都是正数，所以最高位为 0。 41位时间戳（毫秒级）： 需要注意的是此处的 41 位时间戳并非存储当前时间的时间戳，而是存储时间戳的差值（当前时间 戳 - 起始时间戳），这里的起始时间戳一般是ID生成器开始使用的时间戳，由程序来指定，所以41 位毫秒时间戳最多可以使用 (1 &lt;&lt; 41) / (1000x60x60x24x365) = 69年 。 10位数据机器位：包括5位数据标识位和5位机器标识位，这10位决定了分布式系统中最多可以部署 1 &lt;&lt; 10 = 1024个节点。超过这个数量，生成的ID就有可能会冲突。 12位毫秒内的序列： 这 12 位计数支持每个节点每毫秒（同一台机器，同一时刻）最多生成 1 &lt;&lt; 12 = 4096个ID 加起来刚好64位，为一个Long型。 优点：高性能，低延迟，按时间有序，一般不会造成ID碰撞 缺点：需要独立的开发和部署，依赖于机器的时钟 百度UidGeneratorUidGenerator是百度开源的分布式ID生成器，基于于snowflflake算法的实现，看起来感觉还行。不 过，国内开源的项目维护性真是担忧。 美团LeafLeaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，但也需要依赖关系数据库、Zookeeper等中间件。 常见的负载均衡算法有哪些？轮询负载均衡算法：RR ，Round Robin，挨个发，适合于所有服务器硬件都相同的场景。一个一个来，如果有两个服务器，第一次是你，第二次就是我。 代码实现：用i 保存 取 服务器的 下标。第一次来 取0，第二次来取1 ，第三次来 取 0。 加权轮询算法：weighted round robin ，wrr，按照权重不同来分发，基本上是基于配置。 代码实现： 比如：两个服务权重分别是6和4，我们的方法，在1-10之间取 随机数，比如取到 1-6，就走6的权重，取到7-10，就走4权重的服务。 随机轮询算法：Random 代码实现：这个就随意了。 最少链接：Least connections，记录每个服务器正在处理的 连接数 （请求数），将新的请求&nbsp;分发到最少连接的服务器上，这是最符合负载均衡的算法 代码实现：放到redis里，做个hash。 可以用：incr。 （redis&gt; SET key 20 OK redis&gt; INCR key(integer) 21 redis&gt; GET key # 数字值在 Redis 中以字符串的形式保存”21” ） 每次调用一次，服务次数+1。 原地址散列：Source Hashing，根据请求来源的ip地址&nbsp;进行hash计算，只要原地址不变，每次请求映射来的后面提供服务的节点也不变。这有利于进行session信息的维护。 说说什么是计数器（固定窗口）算法计数器算法，是指在指定的时间周期内，累加访问的次数，达到设定的阈值时，触发限流策略。下一个时间周期进行访问时，访问次数清零。此算法无论在单机还是分布式环境下实现都非常简单，使用redis的incr原子自增性，再结合key的过期时间，即可轻松实现,计算器算法如图所示。 从上图看出，设置一分钟的阈值是100，在0:00到1:00内请求数是60，当到1:00时，请求数清零，从0开始计算，这时在1:00到2:00之间能处理的最大的请求为100，超过100个的请求，系统都拒绝。 这个算法有一个临界问题，例如在图中，在0:00到1:00内，只在0:50有60个请求，而在1:00到2:00之间，只在1:10有60个请求，虽然在两个一分钟的时间内，都没有超过100个请求，但是在0:50到1:10这20秒内，确有120个请求，虽然在每个周期内，都没超过阈值，但是在这20秒内，已经远远超过了原来设置的1分钟内100个请求的阈值。 说说什么是滑动窗口算法为了解决计数器算法的临界值的问题，发明了滑动窗口算法。在TCP网络通信协议中，就采用滑动时间窗口算法来解决网络拥堵问题。 滑动时间窗口是将计数器算法中的时间周期切分成多个小的时间窗口，分别在每个小的时间窗口中记录访问次数，然后根据时间将窗口往前滑动并删除过期的小时间窗口。最终只需要统计滑动窗口范围内的小时间窗口的总的请求数即可，如图所示。 在图中，假设设置一分钟的请求阈值是100，将一分钟拆分成4个小时间窗口，这样，在每个小的时间窗口内，只能处理25个请求，用虚线方框表示滑动时间窗口，当前窗口的大小是2，也就是在窗口内最多能处理50个请求。随着时间的推移，滑动窗口也随着时间往前移动，例如图开始时，窗口是0:00到0:30的这个范围，过了15秒后，窗口是0:15到0:45的这个范围，窗口中的请求重新清零，这样就很好的解决了计数器算法的临界值问题。 在滑动时间窗口算法中，小窗口划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。 说说什么是漏桶算法漏桶算法的原理就像它的名字一样，维持一个漏斗，它有恒定的流出速度，不管水流流入的速度有多快，漏斗出水的速度始终保持不变，类似于消息中间件，不管消息的生产者请求量有多大，消息的处理能力取决于消费者。 漏桶的容量=漏桶的流出速度*可接受的等待时长。在这个容量范围内的请求可以排队等待系统的处理，超过这个容量的请求，才会被抛弃。 在漏桶限流算法中，存在下面几种情况。 （1）当请求速度大于漏桶的流出速度时，也就是请求量大于当前服务所能处理的最大极限值时，触发限流策略。 （2）请求速度小于或等于漏桶的流出速度时，也就是服务的处理能力大于或等于请求量时，正常执行。 漏桶算法有一个缺点，当系统在短时间内有突发的大流量时，漏桶算法处理不了。 令牌桶算法令牌桶算法，是增加一个大小固定的容器，也就是令牌桶，系统以恒定的速率向令牌桶中放入令牌，如果有客户端来请求，先需要从令牌桶中拿一个令牌，拿到令牌，才有资格访问系统，这时令牌桶中少一个令牌。当令牌桶满的时候，再向令牌桶生成令牌时，令牌会被抛弃。 在令牌桶算法中，存在以下几种情况。 （1）请求速度大于令牌的生成速度：那么令牌桶中的令牌会被取完，后续再进来的请求，由于拿不到令牌，会被限流。 （2）请求速度等于令牌的生成速度：那么此时系统处于平稳状态。 （3）请求速度小于令牌的生成速度：那么此时系统的访问量远远低于系统的并发能力，请求可以被正常处理。 令牌桶算法，由于有一个桶的存在，可以处理短时间大流量的场景。 数据库如何处理大数据量对数据库进行：分区、分库分表，主从架构（读写分离）。 分区：隔离数据访问。 水平分库/表，各个库和表的结构一模一样，数据量不一样。 垂直分库/表，各个库和表的结构不一样，数据量一样。 读写分离：主机负责写，从机负责读。 什么是CAP定理？CAP定理，又叫布鲁尔定理。指的是在一个分布式系统中，最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 l C：一致性（Consistency），数据在多个副本中保持一致，可以理解成两个用户访问两个系统A和B，当A系统数据有变化时，及时同步给B系统，让两个用户看到的数据是一致的。 l A：可用性（Availability），系统对外提供服务必须一直处于可用状态，在任何故障下，客户端都能在合理时间内获得服务端非错误的响应。 l P：分区容错性（Partition tolerance），在分布式系统中遇到任何网络分区故障，系统仍然能对外提供服务。网络分区，可以这样理解，在分布式系统中，不同的节点分布在不同的子网络中，有可能子网络中只有一个节点，在所有网络正常的情况下，由于某些原因导致这些子节点之间的网络出现故障，导致整个节点环境被切分成了不同的独立区域，这就是网络分区。 来详细分析一下CAP，为什么只能满足两个，如图所示。 用户1和用户2分别访问系统A和系统B，系统A和系统B通过网络进行同步数据。理想情况是用户1访问系统A对数据进行修改，将data1改成了data2，同时用户2访问系统B，拿到的是data2数据。 但是实际中，由于分布式系统具有八大谬论。 （1）网络相当可靠。 （2）延迟为零。 （3）传输带宽是无限的。 （4）网络相当安全。 （5）拓扑结构不会改变。 （6）必须要有一名管理员。 （7）传输成本为零。 （8）网络同质化。 只要有网络调用，网络总是不可靠的，来一一分析。 （1）当网络发生故障时，系统A和系统B没法进行数据同步，也就是不满足P，同时两个系统依然可以访问，那么此时其实相当于是两个单机系统，就不是分布式系统了，所以既然是分布式系统，P必须满足。 （2）当P满足时，如果用户1通过系统A对数据进行了修改将data1改成了data2，也要让用户2通过系统B正确的拿到data2，那么此时是满足C，就必须等待网络将系统A和系统B的数据同步好，并且在同步期间，任何人不能访问系统B（让系统不可用），否则数据就不是一致的。此时满足的是CP，牺牲的是可用性。 （3）当P满足时，如果用户1通过系统A对数据进行了修改将data1改成了data2，也要让系统B能继续提供服务，那么此时，只能接受系统A没有将data2同步给系统B（牺牲了一致性）。此时满足的就是AP，牺牲了数据的一致性。 注册中心Eureka就是满足 的AP，它并不保证C。而Zookeeper是保证CP，它不保证A。在生产中，A和C的选择，没有标准的规定，是取决于自己的业务的。例如12306，是满足CP，因为买票必须满足数据的一致性，不然一个座位多卖了，对铁路运输都是不可以接受的。 什么是BASE理论？由于CAP中一致性C和可用性A无法兼得，eBay的架构师，提出了BASE理论，它是通过牺牲数据的强一致性，来获得可用性。它由于如下3种特征。 l Basically Available（基本可用）：分布式系统在出现不可预知故障的时候，允许损失部分可用性，保证核心功能的可用。 l Soft state（软状态）：软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 l Eventually consistent（最终一致性）：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 BASE理论并没有要求数据的强一致性，而是允许数据在一定的时间段内是不一致的，但在最终某个状态会达到一致。在生产环境中，很多公司，会采用BASE理论来实现数据的一致，因为产品的可用性相比强一致性来说，更加重要。例如在电商平台中，当用户对一个订单发起支付时，往往会调用第三方支付平台，例如支付宝支付或者微信支付，调用第三方成功后，第三方并不能及时通知我方系统，在第三方没有通知我方系统的这段时间内，给用户的订单状态显示支付中，等到第三方回调之后，再将状态改成已支付。虽然订单状态在短期内存在不一致，但是用户却获得了更好的产品体验。 什么是可靠消息最终一致性方案？可靠消息最终一致性方案。它是保证事务最终一致性的一种方案，允许数据在业务中出现短暂的不一致状态。 可靠消息最终一致性方案是指，当事务的发起方（事务参与者，也就是消息发送者）执行完本地事务后，同时发出一条消息，事务参与方（事务参与者，也就是消息的消费者）一定能够接收消息并可以成功处理自己的事务，如图所示。 这里面强调两点。 l 可靠消息：发起方一定得把消息传递到消费者。 l 最终一致性：最终发起方的业务处理和消费方的业务处理得完成，达成最终一致。 可靠消息最终一致性问题分析上图是一个可靠消息服务最终一致性方案的流程图，从图中可以看出事务发起方将消息发送给消息中间件，事务消费方从消息中间件接收消息，事务发起方和消息中间件之间，事务消费方和消息中间件之间，都有网络通信，由于网络通信的不确定性，这块会导致数据的问题。下面针对导致的问题来分别进行解决。 （1）事务发起方本地事务和消息发送之间的原子性问题。 此问题是本地事务执行成功，消息必须发出去，否则丢弃消息，即本地事务执行成功和消息的发送成功，要么都成功，要么都失败。 来一段伪代码参考一下。 begin transaction; 发送消息； 操作数据库； commit transaction; 这种情况下，如果发送消息成功，数据库操作失败，则无法保证原子性。调换一下顺序。 begin transaction; 操作数据库； 发送消息； commit transaction; 这种情况下，如果操作数据库出错，回滚，不影响数据；如果发送消息出错，也回滚，不影响数据。这么一看似乎可以保证原子性，但是会有一种情况，发送消息响应超时，导致数据库回滚，但是消息已经发送成功了。这时原子性还是无法保证的，这个时候就需要人工补偿了。 （2）事务消费方和消息消费的原子性问题。 此时要保证事务消费方必须能接受到消息，如果由于程序故障，导致事务消费方重启，那么需要消息中间件要有消息重发机制；由于网络延时的存在，当事务消费方消费消息成功，没有向消息中间件响应时，而消息中间件由于重发机制，会再次投递消息，就导致了消息重复消费的问题。此时在消费方要有幂等性解决方案。 RocketMQ在分布式事务中如何应用的？在此方案中，需要借助一个消息中间件RocketMQ，它是阿里巴巴的一个开源消息中间件，阿里集团内部的消息都运行在RocketMQ之上，可见它的性能之强。它在4.3之后的版本支持了事务消息，为解决分布式事务提供了便利 。 RocketMQ的事务消息，主要为了解决事务生产方执行业务和消息发送的原子性问题。事务消息流程如图所示。 图事务消息流程 介绍流程之前，先介绍一下事务消息的几种状态： l TransactionStatus.CommitTransaction: 提交状态，它允许消费者消费此消息。 l TransactionStatus.RollbackTransaction: 回滚状态，它代表该消息将被删除，不允许被消费者消费。 l TransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。 具体流程如下。 （1）发送half message。在执行本地业务之前，先向消息队列发送一条事务消息，此时叫做half message，此时消息被标记为（Prepared预备状态），此时的消息是无法被消费者消费的，需要生产者对消息进行二次确认后，消费者才能去消费它。 （2）消息队列回应half message发送成功。 （3）当事务发起方收到消息队列的成功响应之后，开始执行本地业务。 （4）如果本地事务执行成功，则向消息队列发送half message的确认，这样事务消费方就可以消费消息了。 （5）如果本地事务执行失败，则向消息队列发送half message的回滚，删除half message。事务消费方就无法消费消息。 （6）回查机制。当第4步无论是提交还是回滚，由于网络闪断，生产者应用重启等原因，导致生产者无法对消息队列中的half message进行二次确认（即上面的第4步骤，发送提交或者回滚消息）时，消息队列中的half message就不知道应该怎么办了。此时消息队列会定时扫描长期处于half message的消息，并发起一个回查机制，来确认此时的half message应该是提交还是回滚。此时，消息队列主动询问生产者该消息的最终状态（提交还是回滚），即为消息的回查机制。 请说说微服务注册中心的存储结构为什么需要注册中心？服务太多，无法管理。 还是类比一下通讯录。 生活中的通讯录，存储的信息如表所示。 表通讯录示例 姓 名 电话 张三 139xxxxxxxx 同理，注册中心是不是也应该这么存？ 注册中心示例 服务名 服务信息 order-service(订单服务) 服务的IP地址，服务端口，服务对外提供的URL等 那在程序中如何存储呢？从上面的存储内容和存储方式，是不是能想到Java中常用的数据结构Map？ 打开Eureka的源码，印证一下，在AbstractInstanceRegistry.java 中,有一个属性。 private final ConcurrentHashMap&lt;String, Map&lt;String, Lease&gt;&gt; registry = new ConcurrentHashMap&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt;(); 它的存储结构是用的ConcurrentHashMap，它的键值是服务名，值是个Map。而值的Map中键是服务实例ID，值是租约（租约里面包括服务信息）。 key： value &lt;服务名：&lt;服务实例：ip+port，服务配置等等。&gt;&gt; 设计一个注册中心需要写哪些接口？大家想一下，服务启动后，是否需要向注册中心进行注册？所以注册中心需要提供一个接口，让服务调用它来进行服务的登记注册，这就是注册中心的第一个功能， 接受服务注册 。 服务注册完成后，注册中心得知道这个服务是否还是有效服务？所以此时需要服务定期地告诉注册中心，自己的工作状态（是否可用）。此时需要注册中心提供第二个功能， 接受服务心跳 。 当服务下线时，要通知注册中心自己要下线，注册中心需要提供对应的接口，来让服务调用。此时需要注册中心的第三个功能， 接受服务下线 。 如果服务挂了，没有及时通知注册中心，此时注册中心也发现服务，最近没有发送心跳。注册中心要主动剔除挂了的服务。此时需要注册中心第四个功能， 服务剔除 。 注册表中存储的信息，是要供其他服务查询的，就像通讯录一样，是要供主人查阅的，所以注册中心还需要第五个功能， 查询注册表中的服务信息 。 一般微服务中，每个服务都要避免单点故障，注册中心也要做集群，所以还要涉及到注册中心间，注册信息的同步。这就是注册中心的第六个功能， 注册中心集群间注册表的同步 。 如果让你开发，是不是也能开发出一个注册中心呢？其实本质就是一个web服务，提供上面分析的5个接口，供服务调用。这就是平时所说的 注册中心服务端 。那对应的调用注册中心的服务（业务服务），一般称之为 注册中心客户端 。 谈谈你对RESTful规范的理解？很多求职者对RESTful风格有误解，认为它是一个新的协议，其实他就是普通的http请求。其中REST表示Resource Representational State Transfer，直接翻译即“资源表现层状态转移”。 Resource代表互联网资源。所谓“资源”是网络上的一个实体，或者说网上的一个具体信息。它可以是一段文本、一首歌曲、一种服务，可以使用一个URI指向它，每种“资源”对应一个URI（统一资源标识符Uniform Resource Identifier）。 Representational是“表现层”意思。“资源”是一种消息实体，它可以有多种外在的表现形式，把“资源”具体呈现出来的形式叫作它的“表现层”。例如说文本可以用TXT格式进行表现，也可以使用XML格式、JSON格式和二进制格式；视频可以用MP4格式表现，也可以用AVI格式表现。URI只代表资源的实体，不代表它的形式。它的具体表现形式，应该由HTTP请求的头信息Accept和Content-Type字段指定，这两个字段是对“表现层”的描述。 State Transfer是指“状态转移”。客户端访问服务的过程中必然涉及数据和状态的转化。如果客户端想要操作服务端资源，必须通过某种手段，让服务器端资源发生“状态转移”。而这种转化是建立在表现层之上的，所以被称为“表现层状态转移”。客户端通过使用HTTP协议中的常用的四个动词来实现上述操作，它们分别是获取资源的GET、新建或更新资源的POST、更新资源的PUT和删除资源的DELETE。 RestTemplate是由Spring提供，用于封装HTTP调用，它可以简化客户端与HTTP服务器之间的交互，并且它强制使用RESTful风格。它会处理HTTP连接和关闭，只需要使用者提供服务器的地址(URL)和模板参数。 RESTful其实是一种风格并不是一种协议。http协议。 /user post put delete get 分布式系统中为什么引入熔断？大家看一下下面的服务调用场景。C服务和D服务调用B服务，B服务调用A服务。在下面情况1中，服务正常调用。服务在运行过程中，A服务发生故障（网络延时，服务异常，负载过大无法及时响应），系统变成了情况2。由于B服务调用A服务，A服务出故障，导致B服务调用A的代码处也出故障，此时B服务也出故障了，系统变成了情况3。以此类推，系统最终发展成A、B、C、D所有的服务都出错了，整个系统崩塌了。这就是雪崩，如图所示。 雪崩效应 微服务系统之间通过互相调用来实现业务功能，但每个系统都无法百分之百保证自身运行不出问题。在服务调用中，很可能面临依赖服务失效的问题（网络延时，服务异常，负载过大无法及时响应），导致服务雪崩，这对于一个系统来说是灾难性的。因此需要一个组件，能提供强大的容错能力，当服务发生异常时，能提供保护和控制，把影响控制在较小范围内，不要造成所有服务的雪崩。 什么时候恢复系统？ 熔断开关状态：开（走降级的方法），关闭（正常的调用），半开（） 请描述一下熔断和降级的区别？从两方面来阐述。 （1）相似性： ①目的一致：都是从可用性和可靠性着想，为防止系统的整体响应缓慢甚至崩溃，而采用的技术手段。 ②最终表现类似：对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用。 ③粒度一致：都是服务级别的。 ④自治性要求很高：熔断模式一般都是服务基于策略的自动触发，降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段。 （2）区别： ①触发原因不一样：服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑。 ②管理目标的层次不一样：熔断时一个框架级的处理，每个服务都需要，而降级一般有业务层级之分，例如，降级一般在服务调用的上层处理。 如何提升系统的并发能力？分流：负载均衡，消息队列，数据库拆分 导流：缓存，cdn 并行/发：看具体业务。 你是依据什么来进行服务划分的？进行微服务设计时，服务的数量相对于单体应用来说，会比较多（你在公司中，有多少个服务）。考虑的重点就是如何准确识别系统的隔离点，也就是系统的边界。只有每个服务的边界确定了，才能在以后的开发中做到更好的协作。识别系统的隔离点。 结合具体业务。电商。负载均衡。不得不拆。 问的抽象的问题，一定举例子。 微服务设计一般都遵循了什么原则？（1）单一职责原则。让每个服务能独立，有界限的工作，每个服务只关注自己的业务。做到高内聚，服务和服务之间做到低耦合。 （2）服务自治原则。每个服务要能做到独立开发、独立测试、独立构建、独立部署，独立运行，与其他服务进行解耦。 （3）轻量级通信原则。让每个服务之间的调用是轻量级，并且能够跨平台、跨语言。例如采用RESTful风格，利用消息队列进行通信等。 （4）粒度进化原则。对每个服务的粒度把控，其实没有统一的标准，这个得结合解决的具体业务问题。不要过度设计。服务的粒度随着业务和用户的发展而发展。 还有一句话别忘了：软件是为业务服务的，好的系统不是设计出来的，而是进化出来的。 什么是最大努力通知方案？如果接入过支付宝或者微信的支付接口，会遇到这样一种流程。例如，APP调用支付宝或微信的SDK（Software Development Kit软件开发工具包）进行了支付，钱已经从用户的支付宝或微信账户，转到了公司（开发APP的公司）的支付宝或微信账户上，但是支付系统，并不知道钱是否已经支付成功，需要支付宝或者微信回调公司的支付系统，才能进行后续的业务，如图所示。 这其实就是一个最大努力通知的解决方案。在方案中主要保证两点： （1）有一定的消息重复通知机制。因为接收通知方（图中的我方支付系统）可能没有接收到通知，此时要有一定的机制对消息进行重复通知。 （2）消息校对机制。如果尽最大努力也没有通知到接收方，或者接收方消费消息后要再次消费，此时可由接收方主动向通知方查询消息信息来满足需求。 落地： 这种解决方案，其实针对内部系统和外部系统，有不同的做法。 （1）公司内部系统。针对公司内部系统来做的话，可以通过系统直接订阅消息队列来完成。因为都是自己的系统，直接订阅就可以。 （2）公司外部系统。针对公司外部系统来做的话，直接让消费方订阅消息队列就有点不合适了，毕竟不能让两家公司同时对一个消息队列进行操作，所以此时，可以在内部写一个程序来订阅消息队列，通过RPC的方式，调用消费方，使其被动的的接受通知消息。在接支付宝和微信时，一般都是采用这种方式。 spring cloud和Dubbo你是如何做选择的？采用微服务会带来更清晰的业务划分和更好的可扩展性，在很多企业中十分流行。支持微服务的技术栈也是多种多样。当前主流的是Spring Cloud和Dubbo。简单做一下对比，如表所示。 Spring Cloud和Dubbo对比 Spring Cloud Dubbo 注册中心 Eureka，Nacos，Consul，ETCD，Zookeeper Zookeeper 服务间调用方式 RESTful API 基于 http协议 RPC基于dubbo协议 服务监控 Spring Boot Admin Dubbo-Monitor 熔断器 Spring Cloud Circuit Breaker 不完善 网关 Zuul，Gateway 无 配置中心 Config，Nacos 无 服务追踪 Sleuth+Zipkin 无 数据流 Spring Cloud Stream 无 批量任务 Spring Cloud Task 无 消息总线 Spring Cloud Bus 无 从上面的比较可以看出，Spring Cloud的功能比Dubbo更全面，更完善，并且作为Spring 的旗舰项目，它可以与Spring其他项目无缝结合，完美对接，整个软件生态环境比较好。 Spring Cloud就像品牌机，整合在Spring的大家庭中，并做了大量的兼容性测试，保证了机器各部件的稳定。 Dubbo就像组装机，每个组件的选择自由度很高，但是如果你不是高手，如果你选择的某个组件出问题，就会导致整个机器的宕机，造成整体服务的可不用。 Ribbon的原理是什么？@LoadBalanced RestTemplate 重点答以下三点。 （1）通过拦截器对被注解@LoadBalanced修饰的RestTemplate进行拦截。 （2）将RestTemplate中调用的服务名，解析成具体的IP地址，由于一个服务名会对应多个地址，那么在选择具体服务地址的时候，需要做负载均衡。 （3）确定目标服务的IP和PORT后，通过Httpclient进行http的调用。 能说一下你对认证和授权的认识吗？（1）Authentication（认证） 是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 （2）Authorization（授权） 发生在 Authentication（认证） 之后。授权，它主要掌管访问系统的权限。例如有些特定资源，只能是具有特定权限的人才能访问例如admin，有些对系统资源操作例如删除、添加、更新只能特定人才具有。 什么是Cache Aside机制？双写一致性。db，cache。 首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。 会有问题吗？ 假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生。 （1）缓存没有。 （2）请求A查询数据库，得一个旧值 （3）请求B将新值写入数据库 （4）请求B删除缓存 （5）请求A将查到的旧值写入缓存。 ok，如果发生上述情况，确实是会发生脏数据。 这件事情会发生，那么发生的概率多大呢？ 发生上述情况有一个先天性条件： 读操作时，缓存中无数据，这样它才会去数据提供方取数据。 读操作进行的同时，存在一个写操作。 读操作在数据提供方中读取数据的时长大于写操作。这个就很难了。 在读写操作并发时，读取到的是旧值，算是一半一半的概率吧。 就一条，读操作正常情况下，比写操作快很多，所以上述4条同时满足的概率是极低的。 （PS:就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。 可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。 ） 真出现了怎么办？延时双删。 Cache aside 机制是一种 简单有效的缓存更新机制，应用非常广泛，所以叫做cache aside，缓存在边上。就是说以数据库为主，写完有空再处理边上的 缓存。 什么是Read/Write Through机制在Cache Aside中，有概率虽然很低出现数据不一致的情况，我们也用了延迟双删，但还是比较复杂。但要想避免缓存不一致的出现也很简单即进行写入操作时，直接将结果写入缓存，而再从缓存同步写入到数据提供方。等写入数据提供方操作结束后，写入操作才被返回。 这就是Read/Write Through写入机制。 在这种机制下，调用方只需要和缓存打交道，而不需要关心缓存后方的数据提供方。而由缓存来保证自身数据和数据提供方的一致性。 结论：读操作只和缓存打交道，直接读取缓存的结果；写操作的话，调用方写入缓存，再由缓存同步写入数据提供方。 和Cache-Aside的区别？ 在Cache Aside机制中，数据写入缓存的操作，是由调用方的查询结果触发的， 而在Read/write through 机制中，则需要缓存在启动时，自身完成将所有数据从数据提供方读入缓存的过程（在项目启动的时候，其实初始化什么也没有，也没有什么需要读取的，一会有修改，缓存就是新的数据，也不用读）。 比较一下Cache Aside和Read/write through机制。在Cache aside中，缓存只是一个辅助的存在，即使缓存不工作，调用方也可以通过数据提供方完成所有的读写操作，正如其名，缓存在边上，像胯子。 而在Read/write through中，缓存直接对接了调用方，屏蔽了数据提供方，这就意味着缓存系统不可或缺，要求十分可靠。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"并发编程(03)","slug":"22-并发编程-03","date":"2023-09-11T01:16:30.000Z","updated":"2023-09-11T06:40:43.997Z","comments":true,"path":"posts/22.html","link":"","permalink":"http://example.com/posts/22.html","excerpt":"","text":"(上传仅供个人学习交流使用 如有侵权立刻删除) JUC并发工具一、CountDownLatch应用&amp;源码分析1.1 CountDownLatch介绍CountDownLatch就是JUC包下的一个工具，整个工具最核心的功能就是计数器。 如果有三个业务需要并行处理，并且需要知道三个业务全部都处理完毕了。 需要一个并发安全的计数器来操作。 CountDownLatch就可以实现。 给CountDownLatch设置一个数值。可以设置3。 每个业务处理完毕之后，执行一次countDown方法，指定的3每次在执行countDown方法时，对3进行-1。 主线程可以在业务处理时，执行await，主线程会阻塞等待任务处理完毕。 当设置的3基于countDown方法减为0之后，主线程就会被唤醒，继续处理后续业务。 当咱们的业务中，出现2个以上允许并行处理的任务，并且需要在任务都处理完毕后，再做其他处理时，可以采用CountDownLatch去实现这个功能。 1.2 CountDownLatch应用模拟有三个任务需要并行处理，在三个任务全部处理完毕后，再执行后续操作 CountDownLatch中，执行countDown方法，代表一个任务结束，对计数器 - 1 执行await方法，代表等待计数器变为0时，再继续执行 执行await(time,unit)方法，代表等待time时长，如果计数器不为0，返回false，如果在等待期间，计数器为0，方法就返回true 一般CountDownLatch更多的是基于业务去构建，不采用成员变量。 static ThreadPoolExecutor executor = (ThreadPoolExecutor) Executors.newFixedThreadPool(3); static CountDownLatch countDownLatch = new CountDownLatch(3); public static void main(String[] args) throws InterruptedException { System.out.println(\"主业务开始执行\"); sleep(1000); executor.execute(CompanyTest::a); executor.execute(CompanyTest::b); executor.execute(CompanyTest::c); System.out.println(\"三个任务并行执行,主业务线程等待\"); // 死等任务结束 // countDownLatch.await(); // 如果在规定时间内，任务没有结束，返回false if (countDownLatch.await(10, TimeUnit.SECONDS)) { System.out.println(\"三个任务处理完毕，主业务线程继续执行\"); }else{ System.out.println(\"三个任务没有全部处理完毕，执行其他的操作\"); } } private static void a() { System.out.println(\"A任务开始\"); sleep(1000); System.out.println(\"A任务结束\"); countDownLatch.countDown(); } private static void b() { System.out.println(\"B任务开始\"); sleep(1500); System.out.println(\"B任务结束\"); countDownLatch.countDown(); } private static void c() { System.out.println(\"C任务开始\"); sleep(2000); System.out.println(\"C任务结束\"); countDownLatch.countDown(); } private static void sleep(long timeout){ try { Thread.sleep(timeout); } catch (InterruptedException e) { e.printStackTrace(); } } 1.3 CountDownLatch源码分析保证CountDownLatch就是一个计数器，没有什么特殊的功能，查看源码也只是查看计数器实现的方式 发现CountDownLatch的内部类Sync继承了AQS，CountDownLatch就是基于AQS实现的计数器。 AQS就是一个state属性，以及AQS双向链表 猜测计数器的数值实现就是基于state去玩的。 主线程阻塞的方式，也是阻塞在了AQS双向链表中。 1.3.1 有参构造就是构建内部类Sync，并且给AQS中的state赋值 // CountDownLatch的有参构造 public CountDownLatch(int count) { // 健壮性校验 if (count &lt; 0) throw new IllegalArgumentException(\"count &lt; 0\"); // 构建内部类，Sync传入count this.sync = new Sync(count); } // AQS子类，Sync的有参构造 Sync(int count) { // 就是给AQS中的state赋值 setState(count); } 1.3.2 await方法await方法就时判断当前CountDownLatch中的state是否为0，如果为0，直接正常执行后续任务 如果不为0，以共享锁的方式，插入到AQS的双向链表，并且挂起线程 // 一般主线程await的方法，阻塞主线程，等待state为0 public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1); } // 执行了AQS的acquireSharedInterruptibly方法 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { // 判断线程是否中断，如果中断标记位是true，直接抛出异常 if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) // 共享锁挂起的操作 doAcquireSharedInterruptibly(arg); } // tryAcquireShared在CountDownLatch中的实现 protected int tryAcquireShared(int acquires) { // 查看state是否为0，如果为0，返回1，不为0，返回-1 return (getState() == 0) ? 1 : -1; } private void doAcquireSharedInterruptibly(int arg) throws InterruptedException { // 封装当前先成为Node，属性为共享锁 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; } } // 在这，就需要挂起当前线程。 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } 1.3.3 countDown方法countDown方法本质就是对state - 1，如果state - 1后变为0，需要去AQS的链表中唤醒挂起的节点 // countDown对计数器-1 public void countDown() { // 是-1。 sync.releaseShared(1); } // AQS提供的功能 public final boolean releaseShared(int arg) { // 对state - 1 if (tryReleaseShared(arg)) { // state - 1后，变为0，执行doReleaseShared doReleaseShared(); return true; } return false; } // CountDownLatch的tryReleaseShared实现 protected boolean tryReleaseShared(int releases) { // 死循环是为了避免CAS并发问题 for (;;) { // 获取state int c = getState(); // state已经为0，直接返回false if (c == 0) return false; // 对获取到的state - 1 int nextc = c-1; // 基于CAS的方式，将值赋值给state if (compareAndSetState(c, nextc)) // 赋值完，发现state为0了。此时可能会有线程在await方法处挂起，那边挂起，需要这边唤醒 return nextc == 0; } } // 如何唤醒在await方法处挂起的线程 private void doReleaseShared() { // 死循环 for (;;) { // 拿到head Node h = head; // head不为null，有值，并且head != tail，代表至少2个节点 // 一个虚拟的head，加上一个实质性的Node if (h != null &amp;&amp; h != tail) { // 说明AQS队列中有节点 int ws = h.waitStatus; // 如果head节点的状态为 -1. if (ws == Node.SIGNAL) { // 先对head节点将状态从-1，修改为0，避免重复唤醒的情况 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 正常唤醒节点即可，先看head.next,能唤醒就唤醒，如果head.next有问题，从后往前找有效节点 unparkSuccessor(h); } // 会在Semaphore中谈到这个位置 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; } // 会在Semaphore中谈到这个位置 if (h == head) break; } } 二、CyclicBarrier应用&amp;源码分析2.1 CyclicBarrier介绍从名字上来看CyclicBarrier，就是代表循环屏障 Barrier屏障：让一个或多个线程达到一个屏障点，会被阻塞。屏障点会有一个数值，当达到一个线程阻塞在屏障点时，就会对屏障点的数值进行-1操作，当屏障点数值减为0时，屏障就会打开，唤醒所有阻塞在屏障点的线程。在释放屏障点之后，可以先执行一个任务，再让所有阻塞被唤醒的线程继续之后后续任务。 Cyclic循环：所有线程被释放后，屏障点的数值可以再次被重置。 CyclicBarrier一般被称为栅栏。 CyclicBarrier是一种同步机制，允许一组线程互相等待。现成的达到屏障点其实是基于await方法在屏障点阻塞。 CyclicBarrier并没有基于AQS实现，他是基于ReentrantLock锁的机制去实现了对屏障点–，以及线程挂起的操作。（CountDownLatch本身是基于AQS，对state进行release操作后，可以-1） CyclicBarrier没来一个线程执行await，都会对屏障数值进行-1操作，每次-1后，立即查看数值是否为0，如果为0，直接唤醒所有的互相等待线程。 CyclicBarrier对比CountDownLatch区别 底层实现不同。CyclicBarrier基于ReentrantLock做的。CountDownLatch直接基于AQS做的。 应用场景不同。CountDownLatch的计数器只能使用一次。而CyclicBarrier在计数器达到0之后，可以重置计数器。CyclicBarrier可以实现相比CountDownLatch更复杂的业务，执行业务时出现了错误，可以重置CyclicBarrier计数器，再次执行一次。 CyclicBarrier还提供了很多其他的功能： 可以获取到阻塞的现成有多少 在线程互相等待时，如果有等待的线程中断，可以抛出异常，避免无限等待的问题。 CountDownLatch一般是让主线程等待，让子线程对计数器–。CyclicBarrier更多的让子线程也一起计数和等待，等待的线程达到数值后，再统一唤醒 CyclicBarrier：多个线程互相等待，直到到达同一个同步点，再一次执行。 2.2 CyclicBarrier应用出国旅游。 导游小姐姐需要等待所有乘客都到位后，发送护照，签证等等文件，再一起出发 比如Tom，Jack，Rose三个人组个团出门旅游 在构建CyclicBarrier可以指定barrierAction，可以选择性指定，如果指定了，那么会在barrier归0后，优先执行barrierAction任务，然后再去唤醒所有阻塞挂起的线程，并行去处理后续任务。 所有互相等待的线程，可以指定等待时间，并且在等待的过程中，如果有线程中断，所有互相的等待的线程都会被唤醒。 如果在等待期间，有线程中断了，唤醒所有线程后，CyclicBarrier无法继续使用。 如果线程中断后，需要继续使用当前的CyclicBarrier，需要调用reset方法，让CyclicBarrier重置。 如果CyclicBarrier的屏障数值到达0之后，他默认会重置屏障数值，CyclicBarrier在没有线程中断时，是可以重复使用的。 public static void main(String[] args) throws InterruptedException { CyclicBarrier barrier = new CyclicBarrier(3,() -&gt; { System.out.println(\"等到各位大佬都到位之后，分发护照和签证等内容！\"); }); new Thread(() -&gt; { System.out.println(\"Tom到位！！！\"); try { barrier.await(); } catch (Exception e) { System.out.println(\"悲剧，人没到齐！\"); return; } System.out.println(\"Tom出发！！！\"); }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(\"Jack到位！！！\"); try { barrier.await(); } catch (Exception e) { System.out.println(\"悲剧，人没到齐！\"); return; } System.out.println(\"Jack出发！！！\"); }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(\"Rose到位！！！\"); try { barrier.await(); } catch (Exception e) { System.out.println(\"悲剧，人没到齐！\"); return; } System.out.println(\"Rose出发！！！\"); }).start(); /* tom到位，jack到位，rose到位 导游发签证 tom出发，jack出发，rose出发 */ } 2.3 CyclicBarrier源码分析分成两块内容去查看，首先查看CyclicBarrier的一些核心属性，然后再查看CyclicBarrier的核心方法 2.3.1 CyclicBarrier的核心属性public class CyclicBarrier { // 这个静态内部类是用来标记是否中断的 private static class Generation { boolean broken = false; } /** CyclicBarrier是基于ReentrantLock实现的互斥操作，以及计数原子性操作 */ private final ReentrantLock lock = new ReentrantLock(); /** 基于当前的Condition实现线程的挂起和唤醒 */ private final Condition trip = lock.newCondition(); /** 记录有参构造传入的屏障数值，不会对这个数值做操作 */ private final int parties; /** 当屏障数值达到0之后，优先执行当前任务 */ private final Runnable barrierCommand; /** 初始化默认的Generation，用来标记线程中断情况 */ private Generation generation = new Generation(); /** 每来一个线程等待，就对count进行-- */ private int count; } 2.3.2 CyclicBarrier的有参构造掌握构建CyclicBarrier之后，内部属性的情况 // 这个是CyclicBarrier的有参构造 // 在内部传入了parties，屏障点的数值 // 还传入了barrierAction，屏障点的数值达到0，优先执行barrierAction任务 public CyclicBarrier(int parties, Runnable barrierAction) { // 健壮性判 if (parties &lt;= 0) throw new IllegalArgumentException(); // 当前类中的属性parties是保存屏障点数值的 this.parties = parties; // 将parties赋值给属性count，每来一个线程，继续count做-1操作。 this.count = parties; // 优先执行的任务 this.barrierCommand = barrierAction; } 2.3.3 CyclicBarrier中的await方法在CyclicBarrier中，提供了2个await方法 第一个是无参的方式，线程要死等，直屏障点数值为0，或者有线程中断 第二个是有参方式，传入等待的时间，要么时间到位了，要不就是直屏障点数值为0，或者有线程中断 无论是哪种await方法，核心都在于内部调用的dowait方法 dowait方法主要包含了线程互相等待的逻辑，以及屏障点数值到达0之后的操作 三、Semaphone应用&amp;源码分析3.1 Semaphore介绍sync，ReentrantLock是互斥锁，保证一个资源同一时间只允许被一个线程访问 Semaphore（信号量）保证1个或多个资源可以被指定数量的线程同时访问 底层实现是基于AQS去做的。 Semaphore底层也是基于AQS的state属性做一个计数器的维护。state的值就代表当前共享资源的个数。如果一个线程需要获取的1或多个资源，直接查看state的标识的资源个数是否足够，如果足够的，直接对state - 1拿到当前资源。如果资源不够，当前线程就需要挂起等待。知道持有资源的线程释放资源后，会归还给Semaphore中的state属性，挂起的线程就可以被唤醒。 Semaphore也分为公平和非公平的概念。 使用场景：连接池对象就可以基础信号量去实现管理。在一些流量控制上，也可以采用信号量去实现。再比如去迪士尼或者是环球影城，每天接受的人流量是固定的，指定一个具体的人流量，可能接受10000人，每有一个人购票后，就对信号量进行–操作，如果信号量已经达到了0，或者是资源不足，此时就不能买票。 3.2 Semaphore应用以上面环球影城每日人流量为例子去测试一下。 public static void main(String[] args) throws InterruptedException { // 今天环球影城还有人个人流量 Semaphore semaphore = new Semaphore(10); new Thread(() -&gt; { System.out.println(\"一家三口要去~~\"); try { semaphore.acquire(3); System.out.println(\"一家三口进去了~~~\"); Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); }finally { System.out.println(\"一家三口走了~~~\"); semaphore.release(3); } }).start(); for (int i = 0; i &lt; 7; i++) { int j = i; new Thread(() -&gt; { System.out.println(j + \"大哥来了。\"); try { semaphore.acquire(); System.out.println(j + \"大哥进去了~~~\"); Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); }finally { System.out.println(j + \"大哥走了~~~\"); semaphore.release(); } }).start(); } Thread.sleep(10); System.out.println(\"main大哥来了。\"); if (semaphore.tryAcquire()) { System.out.println(\"main大哥进来了。\"); }else{ System.out.println(\"资源不够，main大哥进来了。\"); } Thread.sleep(10000); System.out.println(\"main大哥又来了。\"); if (semaphore.tryAcquire()) { System.out.println(\"main大哥进来了。\"); semaphore.release(); }else{ System.out.println(\"资源不够，main大哥进来了。\"); } } 其实Semaphore整体就是对构建Semaphore时，指定的资源数的获取和释放操作 获取资源方式： acquire()：获取一个资源，没有资源就挂起等待，如果中断，直接抛异常 acquire(int)：获取指定个数资源，资源不够，或者没有资源就挂起等待，如果中断，直接抛异常 tryAcquire()：获取一个资源，没有资源返回false，有资源返回true tryAcquire(int)：获取指定个数资源，没有资源返回false，有资源返回true tryAcquire(time,unit)：获取一个资源，如果没有资源，等待time.unit，如果还没有，就返回false tryAcquire(int，time,unit)：获取指定个数资源，如果没有资源，等待time.unit，如果还没有，就返回false acquireUninterruptibly()：获取一个资源，没有资源就挂起等待，中断线程不结束，继续等 acquireUninterruptibly(int)：获取指定个数资源，没有资源就挂起等待，中断线程不结束，继续等 归还资源方式： release()：归还一个资源 release(int)：归还指定个数资源 3.3 Semaphore源码分析先查看Semaphore的整体结构，然后基于获取资源，以及归还资源的方式去查看源码 3.3.1 Semaphore的整体结构Semaphore内部有3个静态内类。 首先是向上抽取的Sync 其次还有两个Sync的子类NonFairSync以及FairSync两个静态内部类 Sync内部主要提供了一些公共的方法，并且将有参构造传入的资源个数，直接基于AQS提供的setState方法设置了state属性。 NonFairSync以及FairSync区别就是tryAcquireShared方法的实现是不一样。 3.3.2 Semaphore的非公平的获取资源在构建Semaphore的时候，如果只设置资源个数，默认情况下是非公平。 如果在构建Semaphore，传入了资源个数以及一个boolean时，可以选择非公平还是公平。 public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 从非公平的acquire方法入手 首先确认默认获取资源数是1个，并且acquire是允许中断线程时，抛出异常的。获取资源的方式，就是直接用state - 需要的资源数，只要资源足够，就CAS的将state做修改。如果没有拿到锁资源，就基于共享锁的方式去将当前线程挂起在AQS双向链表中。如果基于doAcquireSharedInterruptibly拿锁成功，会做一个事情。会执行setHeadAndPropagate方法。一会说 // 信号量的获取资源方法（默认获取一个资源） public void acquire() throws InterruptedException { // 跳转到了AQS中提供共享锁的方法 sync.acquireSharedInterruptibly(1); } // AQS提供的 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { // 判断线程的中断标记位，如果已经中断，直接抛出异常 if (Thread.interrupted()) throw new InterruptedException(); // 先看非公平的tryAcquireShared实现。 // tryAcquireShared： // 返回小于0，代表获取资源失败，需要排队。 // 返回大于等于0，代表获取资源成功，直接执行业务代码 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); } // 信号量的非公平获取资源方法 final int nonfairTryAcquireShared(int acquires) { // 死循环。 for (;;) { // 获取state的数值，剩余的资源个数 int available = getState(); // 剩余的资源个数 - 需要的资源个数 int remaining = available - acquires; // 如果-完后，资源个数小于0，直接返回这个负数 if (remaining &lt; 0 || // 说明资源足够，基于CAS的方式，将state从原值，改为remaining compareAndSetState(available, remaining)) return remaining; } } // 获取资源失败，资源不够，当前线程需要挂起等待 private void doAcquireSharedInterruptibly(int arg) throws InterruptedException { // 构建Node节点，线程和共享锁标记，并且到AQS双向链表中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { for (;;) { // 拿到上一个节点 final Node p = node.predecessor(); // 如果是head.next，就抢一手 if (p == head) { // 再次基于非公平的方式去获取一次资源 int r = tryAcquireShared(arg); // 到这，说明拿到了锁资源 if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; failed = false; return; } } // 如果上面没拿到，或者不是head的next节点，将前继节点的状态改为-1，并挂起当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 如果线程中断会抛出异常 throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } acquire()以及acquire(int)的方式，都是执行acquireSharedInterruptibly方法去尝试获取资源，区别只在于是否传入了需要获取的资源个数。 tryAcquire()以及tryAcquire(int因为这两种方法是直接执行tryAcquire，只使用非公平的实现，只有非公平的情况下，才有可能在有线程排队的时候获取到资源 但是tryAcquire(int,time,unit)这种方法是正常走的AQS提供的acquire。因为这个tryAcquire可以排队一会，即便是公平锁也有可能拿到资源。这里的挂起和acquire挂起的区别仅仅是挂起的时间问题。 acquire是一直挂起直到线程中断，或者线程被唤醒。 tryAcquire(int,time,unit)是挂起一段时间，直到线程中断，要么线程被唤醒，要么阻塞时间到了 还有acquireUninterruptibly()以及acquireUninterruptibly(int)只是在挂起线程后，不会因为线程的中断而去抛出异常 3.3.3 Semaphore公平实现公平与非公平只是差了一个方法的实现tryAcquireShared实现 这个方法的实现中，如果是公平实现，需要先查看AQS中排队的情况 // 信号量公平实现 protected int tryAcquireShared(int acquires) { // 死循环。 for (;;) { // 公平实现在走下述逻辑前，先判断队列中排队的情况 // 如果没有排队的节点，直接不走if逻辑 // 如果有排队的节点，发现当前节点处在head.next位置，直接不走if逻辑 if (hasQueuedPredecessors()) return -1; // 下面这套逻辑和公平实现是一模一样的。 int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; } } 3.3.4 Semaphore释放资源因为信号量从头到尾都是共享锁的实现…… 释放资源操作，不区分公平和非公平 // 信号量释放资源的方法入口 public void release() { sync.releaseShared(1); } // 释放资源不分公平和非公平，都走AQS的releaseShared public final boolean releaseShared(int arg) { // 优先查看tryReleaseShared，这个方法是信号量自行实现的。 if (tryReleaseShared(arg)) { // 只要释放资源成功，执行doReleaseShared，唤醒AQS中排队的线程，去竞争Semaphore的资源 doReleaseShared(); return true; } return false; } // 信号量实现的释放资源方法 protected final boolean tryReleaseShared(int releases) { // 死循环 for (;;) { // 拿到当前的state int current = getState(); // 将state + 归还的资源个数，新的state要被设置为next int next = current + releases; // 如果归还后的资源个数，小于之前的资源数。 // 避免出现归还资源后，导致next为负数，需要做健壮性判断 if (next &lt; current) throw new Error(\"Maximum permit count exceeded\"); // CAS操作，保证原子性，只会有一个线程成功的就之前的state修改为next if (compareAndSetState(current, next)) return true; } } 3.4 AQS中PROPAGATE节点为了更好的了解PROPAGATE节点状态的意义，优先从JDK1.5去分析一下释放资源以及排队后获取资源的后置操作 3.4.1 掌握JDK1.5-Semaphore执行流程图首先查看4个线程获取信号量资源的情况 往下查看释放资源的过程会触发什么问题 首先t1释放资源，做了进一步处理 当线程3获取锁资源后，线程2再次释放资源，因为执行点问题，导致线程4无法被唤醒 3.4.2 分析JDK1.8的变化 ====================================JDK1.5实现============================================. public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } private void setHeadAndPropagate(Node node, int propagate) { setHead(node); if (propagate &gt; 0 &amp;&amp; node.waitStatus != 0) { Node s = node.next; if (s == null || s.isShared()) unparkSuccessor(node); } } ====================================JDK1.8实现============================================. public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } private void doReleaseShared() { for (;;) { // 拿到head节点 Node h = head; // 判断AQS中有排队的Node节点 if (h != null &amp;&amp; h != tail) { // 拿到head节点的状态 int ws = h.waitStatus; // 状态为-1 if (ws == Node.SIGNAL) { // 将head节点的状态从-1，改为0 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒后继节点 unparkSuccessor(h); } // 发现head状态为0，将head状态从0改为-3，目的是为了往后面传播 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } // 没有并发的时候。head节点没变化，正常完成释放排队的线程 if (h == head) break; } } private void setHeadAndPropagate(Node node, int propagate) { // 拿到head Node h = head; // 将线程3的Node设置为新的head setHead(node); // 如果propagate 大于0，代表还有剩余资源，直接唤醒后续节点，如果不满足，也需要继续往后判断看下是否需要传播 // h == null：看成健壮性判断即可 // 之前的head节点状态为负数，说明并发情况下，可能还有资源，需要继续向后唤醒Node // 如果当前新head节点的状态为负数，继续释放后续节点 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { // 唤醒当前节点的后继节点 Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); } } 异步编程一、FutureTask应用&amp;源码分析1.1 FutureTask介绍FutureTask是一个可以取消异步任务的类。FutureTask对Future做的一个基本实现。可以调用方法区开始和取消一个任务。 一般是配合Callable去使用。 异步任务启动之后，可以获取一个绑定当前异步任务的FutureTask。 可以基于FutureTask的方法去取消任务，查看任务是否结果，以及获取任务的返回结果。 FutureTask内部的整体结构中，实现了RunnableFuture的接口，这个接口又继承了Runnable, Future这个两个接口。所以FutureTask也可以作为任务直接交给线程池去处理。 1.2 FutureTask应用大方向是FutureTask对任务的控制： 任务执行过程中状态的控制 任务执行完毕后，返回结果的获取 FutureTask的任务在执行run方法后，是无法被再次运行，需要使用runAndReset方法才可以。 public static void main(String[] args) throws InterruptedException { // 构建FutureTask，基于泛型执行返回结果类型 // 在有参构造中，声明Callable或者Runnable指定任务 FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; { System.out.println(\"任务开始执行……\"); Thread.sleep(2000); System.out.println(\"任务执行完毕……\"); return \"OK!\"; }); // 构建线程池 ExecutorService service = Executors.newFixedThreadPool(10); // 线程池执行任务 service.execute(futureTask); // futureTask提供了run方法，一般不会自己去调用run方法，让线程池去执行任务，由线程池去执行run方法 // run方法在执行时，是有任务状态的。任务已经执行了，再次调用run方法无效的。 // 如果希望任务可以反复被执行，需要去调用runAndReset方法 // futureTask.run(); // 对返回结果的获取，类似阻塞队列的poll方法 // 如果在指定时间内，没有拿到方法的返回结果，直接扔TimeoutException // try { // String s = futureTask.get(3000, TimeUnit.MILLISECONDS); // System.out.println(\"返回结果：\" + s); // } catch (Exception e) { // System.out.println(\"异常返回：\" + e.getMessage()); // e.printStackTrace(); // } // 对返回结果的获取，类似阻塞队列的take方法，死等结果 // try { // String s = futureTask.get(); // System.out.println(\"任务结果：\" + s); // } catch (ExecutionException e) { // e.printStackTrace(); // } // 对任务状态的控制 // System.out.println(\"任务结束了么？：\" + futureTask.isDone()); // Thread.sleep(1000); // System.out.println(\"任务结束了么？：\" + futureTask.isDone()); // Thread.sleep(1000); // System.out.println(\"任务结束了么？：\" + futureTask.isDone()); } 1.3 FutureTask源码分析看FutureTask的源码，要从几个方向去看： 先查看FutureTask中提供的一些状态 在查看任务的执行过程 1.3.1 FutureTask中的核心属性清楚任务的流转流转状态是怎样的，其次对于核心属性要追到是干嘛的。 /** FutureTask的核心属性 FutureTask任务的状态流转 * NEW -&gt; COMPLETING -&gt; NORMAL 任务正常执行，并且返回结果也正常返回 * NEW -&gt; COMPLETING -&gt; EXCEPTIONAL 任务正常执行，但是结果是异常 * NEW -&gt; CANCELLED 任务被取消 * NEW -&gt; INTERRUPTING -&gt; INTERRUPTED 任务被中断 */ // 记录任务的状态 private volatile int state; // 任务被构建之后的初始状态 private static final int NEW = 0; private static final int COMPLETING = 1; private static final int NORMAL = 2; private static final int EXCEPTIONAL = 3; private static final int CANCELLED = 4; private static final int INTERRUPTING = 5; private static final int INTERRUPTED = 6; /** 需要执行任务，会被赋值到这个属性 */ private Callable&lt;V&gt; callable; /** 任务的任务结果要存储在这几个属性中 */ private Object outcome; // non-volatile, protected by state reads/writes /** 执行任务的线程 */ private volatile Thread runner; /** 等待返回结果的线程Node对象， */ private volatile WaitNode waiters; static final class WaitNode { volatile Thread thread; volatile WaitNode next; WaitNode() { thread = Thread.currentThread(); } } 1.3.2 FutureTask的run方法任务执行前的一些判断，以及调用任务封装结果的方式，还有最后的一些后续处理 // 当线程池执行FutureTask任务时，会调用的方法 public void run() { // 如果当前任务状态不是NEW，直接return告辞 if (state != NEW || // 如果状态正确是NEW，这边需要基于CAS将runner属性设置为当前线程 // 如果CAS失败，直接return告辞 !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try { // 将要执行的任务拿到 Callable&lt;V&gt; c = callable; // 健壮性判断，保证任务不是null // 再次判断任务的状态是NEW（DCL） if (c != null &amp;&amp; state == NEW) { // 执行任务 // result：任务的返回结果 // ran：如果为true，任务正常结束。 如果为false，任务异常结束。 V result; boolean ran; try { // 执行任务 result = c.call(); // 正常结果，ran设置为true ran = true; } catch (Throwable ex) { // 如果任务执行期间出了异常 // 返回结果置位null result = null; // ran设置为false ran = false; // 封装异常结果 setException(ex); } if (ran) // 封装正常结果 set(result); } } finally { // 将执行任务的线程置位null runner = null; // 拿到任务的状态 int s = state; // 如果状态大于等于INTERRUPTING if (s &gt;= INTERRUPTING) // 进来代表任务中断，做一些后续处理 handlePossibleCancellationInterrupt(s); } } 1.3.3 FutureTask的set&amp;setException方法任务执行完毕后，修改任务的状态以及封装任务的结果 // 没有异常的时候，正常返回结果 protected void set(V v) { // 因为任务执行完毕，需要将任务的状态从NEW，修改为COMPLETING if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { // 将返回结果赋值给 outcome 属性 outcome = v; // 将任务状态变为NORMAL，正常结束 UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // 一会再说…… finishCompletion(); } } // 任务执行期间出现了异常，这边要封装结果 protected void setException(Throwable t) { // 因为任务执行完毕，需要将任务的状态从NEW，修改为COMPLETING if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { // 将异常信息封装到 outcome 属性 outcome = t; // 将任务状态变为EXCEPTIONAL，异常结束 UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // 一会再说…… finishCompletion(); } } 1.3.4 FutureTask的cancel方法任务取消的一个方式 任务直接从NEW状态转换为CANCEL 任务从NEW状态变成INTERRUPTING，然后再转换为INTERRUPTED // 取消任务操作 public boolean cancel(boolean mayInterruptIfRunning) { // 查看任务的状态是否是NEW，如果NEW状态，就基于传入的参数mayInterruptIfRunning // 决定任务是直接从NEW转换为CANCEL，还是从NEW转换为INTERRUPTING if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try { // 如果mayInterruptIfRunning为true // 就需要中断线程 if (mayInterruptIfRunning) { try { // 拿到任务线程 Thread t = runner; if (t != null) // 如果线程不为null，直接interrupt t.interrupt(); } finally { // 将任务状态设置为INTERRUPTED UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); } } } finally { // 任务结束后的一些处理~~ 一会看~~ finishCompletion(); } return true; } 1.3.5 FutureTask的get方法这个是线程获取FutureTask任务执行结果的方法 // 拿任务结果 public V get() throws InterruptedException, ExecutionException { // 获取任务的状态 int s = state; // 要么是NEW，任务还没执行完 // 要么COMPLETING，任务执行完了，结果还没封装好。 if (s &lt;= COMPLETING) // 让当前线程阻塞，等待结果 s = awaitDone(false, 0L); // 最终想要获取结果，需要执行report方法 return report(s); } // 线程等待FutureTask结果的过程 private int awaitDone(boolean timed, long nanos) throws InterruptedException { // 针对get方法传入了等待时长时，需要计算等到什么时间点 final long deadline = timed ? System.nanoTime() + nanos : 0L; // 声明好需要的Node，queued：放到链表中了么？ WaitNode q = null; boolean queued = false; for (;;) { // 查看线程是否中断，如果中断，从等待链表中移除，甩个异常 if (Thread.interrupted()) { removeWaiter(q); throw new InterruptedException(); } // 拿到状态 int s = state; // 到这，说明任务结束了。 if (s &gt; COMPLETING) { if (q != null) // 如果之前封装了WaitNode，现在要清空 q.thread = null; return s; } // 如果任务状态是COMPLETING，这就不需要去阻塞线程，让步一下，等待一小会，结果就有了 else if (s == COMPLETING) Thread.yield(); // 如果还没初始化WaitNode，初始化 else if (q == null) q = new WaitNode(); // 没放队列的话，直接放到waiters的前面 else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 准备挂起线程，如果timed为true，挂起一段时间 else if (timed) { // 计算出最多可以等待多久 nanos = deadline - System.nanoTime(); // 如果等待的时间没了 if (nanos &lt;= 0L) { // 移除当前的Node，返回任务状态 removeWaiter(q); return state; } // 等一会 LockSupport.parkNanos(this, nanos); } else // 死等 LockSupport.park(this); } } // get的线程已经可以阻塞结束了，基于状态查看能否拿到返回结果 private V report(int s) throws ExecutionException { // 拿到outcome 返回结果 Object x = outcome; // 如果任务状态是NORMAL，任务正常结束，返回结果 if (s == NORMAL) return (V)x; // 如果任务状态大于等于取消 if (s &gt;= CANCELLED) // 直接抛出异常 throw new CancellationException(); // 到这就是异常结束 throw new ExecutionException((Throwable)x); } 1.3.6 FutureTask的finishCompletion方法只要任务结束了，无论是正常返回，异常返回，还是任务被取消都会执行这个方法 而这个方法其实就是唤醒那些执行get方法等待任务结果的线程 // 任务结束后触发 private void finishCompletion() { // 在任务结束后，需要唤醒 for (WaitNode q; (q = waiters) != null;) { // 第一步直接以CAS的方式将WaitNode置为null if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) { for (;;) { // 拿到了Node中的线程 Thread t = q.thread; // 如果线程不为null if (t != null) { // 第一步先置位null q.thread = null; // 直接唤醒这个线程 LockSupport.unpark(t); } // 拿到当前Node的next WaitNode next = q.next; // next为null，代表已经将全部节点唤醒了吗，跳出循环 if (next == null) break; // 将next置位null q.next = null; // q的引用指向next q = next; } break; } } // 任务结束后，可以基于这个扩展方法，记录一些信息 done(); // 任务执行完，把callable具体任务置位null callable = null; } 二、CompletableFuture应用&amp;源码分析2.1 CompletableFuture介绍平时多线程开发一般就是使用Runnable，Callable，Thread，FutureTask，ThreadPoolExecutor这些内容和并发编程息息相关。相对来对来说成本都不高，多多使用是可以熟悉这些内容。这些内容组合在一起去解决一些并发编程的问题时，很多时候没有办法很方便的去完成异步编程的操作。 Thread + Runnable：执行异步任务，但是没有返回结果 Thread + Callable + FutureTask：完整一个可以有返回结果的异步任务 获取返回结果，如果基于get方法获取，线程需要挂起在WaitNode里 获取返回结果，也可以基于isDone判断任务的状态，但是这里需要不断轮询 上述的方式都是有一定的局限性的。 比如说任务A，任务B，还有任务C。其中任务B还有任务C执行的前提是任务A先完成，再执行任务B和任务C。 如果任务的执行方式逻辑比较复杂，可能需要业务线程导出阻塞等待，或者是大量的任务线程去编一些任务执行的业务逻辑。对开发成本来说比较高。 CompletableFuture就是帮你处理这些任务之间的逻辑关系，编排好任务的执行方式后，任务会按照规划好的方式一步一步执行，不需要让业务线程去频繁的等待 2.2 CompletableFuture应用CompletableFuture应用还是需要一内内的成本的。 首先对CompletableFuture提供的函数式编程中三个函数有一个掌握 Supplier&lt;U&gt; // 生产者，没有入参，有返回结果 Consumer&lt;T&gt; // 消费者，有入参，但是没有返回结果 Function&lt;T,U&gt;// 函数，有入参，又有返回结果 2.2.1 supplyAsyncCompletableFuture如果不提供线程池的话，默认使用的ForkJoinPool，而ForkJoinPool内部是守护线程，如果main线程结束了，守护线程会跟着一起结束。 public static void main(String[] args) { // 生产者，可以指定返回结果 CompletableFuture&lt;String&gt; firstTask = CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"异步任务开始执行\"); System.out.println(\"异步任务执行结束\"); return \"返回结果\"; }); String result1 = firstTask.join(); String result2 = null; try { result2 = firstTask.get(); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } System.out.println(result1 + \",\" + result2); } 2.2.2 runAsync当前方式既不会接收参数，也不会返回任何结果，非常基础的任务编排方式 public static void main(String[] args) throws IOException { CompletableFuture.runAsync(() -&gt; { System.out.println(\"任务go\"); System.out.println(\"任务done\"); }); System.in.read(); } 2.2.3 thenApply，thenApplyAsync有任务A，还有任务B。 任务B需要在任务A执行完毕后再执行。 而且任务B需要任务A的返回结果。 任务B自身也有返回结果。 thenApply可以拼接异步任务，前置任务处理完之后，将返回结果交给后置任务，然后后置任务再执行 thenApply提供了带有Async的方法，可以指定每个任务使用的具体线程池。 public static void main(String[] args) throws IOException { ExecutorService executor = Executors.newFixedThreadPool(10); /*CompletableFuture&lt;String&gt; taskA = CompletableFuture.supplyAsync(() -&gt; { String id = UUID.randomUUID().toString(); System.out.println(\"执行任务A：\" + id); return id; }); CompletableFuture&lt;String&gt; taskB = taskA.thenApply(result -&gt; { System.out.println(\"任务B获取到任务A结果：\" + result); result = result.replace(\"-\", \"\"); return result; }); System.out.println(\"main线程拿到结果：\" + taskB.join());*/ CompletableFuture&lt;String&gt; taskB = CompletableFuture.supplyAsync(() -&gt; { String id = UUID.randomUUID().toString(); System.out.println(\"执行任务A：\" + id + \",\" + Thread.currentThread().getName()); return id; }).thenApplyAsync(result -&gt; { System.out.println(\"任务B获取到任务A结果：\" + result + \",\" + Thread.currentThread().getName()); result = result.replace(\"-\", \"\"); return result; },executor); System.out.println(\"main线程拿到结果：\" + taskB.join()); } 2.2.4 thenAccept，thenAcceptAsync套路和thenApply一样，都是任务A和任务B的拼接 前置任务需要有返回结果，后置任务会接收前置任务的结果，返回后置任务没有返回值 public static void main(String[] args) throws IOException { CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务A\"); return \"abcdefg\"; }).thenAccept(result -&gt; { System.out.println(\"任务b，拿到结果处理：\" + result); }); System.in.read(); } 2.2.5 thenRun，thenRunAsync套路和thenApply，thenAccept一样，都是任务A和任务B的拼接 前置任务没有返回结果，后置任务不接收前置任务结果，后置任务也会有返回结果 public static void main(String[] args) throws IOException { CompletableFuture.runAsync(() -&gt; { System.out.println(\"任务A！！\"); }).thenRun(() -&gt; { System.out.println(\"任务B！！\"); }); System.in.read(); } 2.2.6 thenCombine，thenAcceptBoth，runAfterBoth比如有任务A，任务B，任务C。任务A和任务B并行执行，等到任务A和任务B全部执行完毕后，再执行任务C。 A+B —— C 基于前面thenApply，thenAccept，thenRun知道了一般情况三种任务的概念 thenCombine以及thenAcceptBoth还有runAfterBoth的区别是一样的。 public static void main(String[] args) throws IOException { CompletableFuture&lt;Integer&gt; taskC = CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务A\"); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } return 78; }).thenCombine(CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务B\"); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } return 66; }), (resultA, resultB) -&gt; { System.out.println(\"任务C\"); int resultC = resultA + resultB; return resultC; }); System.out.println(taskC.join()); System.in.read(); } 2.2.7 applyToEither，acceptEither，runAfterEither比如有任务A，任务B，任务C。任务A和任务B并行执行，只要任务A或者任务B执行完毕，开始执行任务C A or B —– C applyToEither，acceptEither，runAfterEither三个方法拼接任务的方式都是一样的 区别依然是，可以接收结果并且返回结果，可以接收结果没有返回结果，不接收结果也没返回结果 public static void main(String[] args) throws IOException { CompletableFuture&lt;Integer&gt; taskC = CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务A\"); return 78; }).applyToEither(CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务B\"); return 66; }), resultFirst -&gt; { System.out.println(\"任务C\"); return resultFirst; }); System.out.println(taskC.join()); System.in.read(); } 2.2.8 exceptionally，thenCompose，handleexceptionally 这个也是拼接任务的方式，但是只有前面业务执行时出现异常了，才会执行当前方法来处理 只有异常出现时，CompletableFuture的编排任务没有处理完时，才会触发 thenCompose，handle 这两个也是异常处理的套路，可以根据方法描述发现，他的功能方向比exceptionally要更加丰富 thenCompose可以拿到返回结果同时也可以拿到出现的异常信息，但是thenCompose本身是Consumer不能返回结果。无法帮你捕获异常，但是可以拿到异常返回的结果。 handle可以拿到返回结果同时也可以拿到出现的异常信息，并且也可以指定返回托底数据。可以捕获异常的，异常不会抛出去。 public static void main(String[] args) throws IOException { CompletableFuture&lt;Integer&gt; taskC = CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务A\"); // int i = 1 / 0; return 78; }).applyToEither(CompletableFuture.supplyAsync(() -&gt; { System.out.println(\"任务B\"); return 66; }), resultFirst -&gt; { System.out.println(\"任务C\"); return resultFirst; }).handle((r,ex) -&gt; { System.out.println(\"handle:\" + r); System.out.println(\"handle:\" + ex); return -1; }); /*.exceptionally(ex -&gt; { System.out.println(\"exceptionally:\" + ex); return -1; });*/ /*.whenComplete((r,ex) -&gt; { System.out.println(\"whenComplete:\" + r); System.out.println(\"whenComplete:\" + ex); });*/ System.out.println(taskC.join()); System.in.read(); } 2.2.9 allOf，anyOfallOf的方式是让内部编写多个CompletableFuture的任务，多个任务都执行完后，才会继续执行你后续拼接的任务 allOf返回的CompletableFuture是Void，没有返回结果 public static void main(String[] args) throws IOException { CompletableFuture.allOf( CompletableFuture.runAsync(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务A\"); }), CompletableFuture.runAsync(() -&gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务B\"); }), CompletableFuture.runAsync(() -&gt; { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务C\"); }) ).thenRun(() -&gt; { System.out.println(\"任务D\"); }); System.in.read(); } anyOf是基于多个CompletableFuture的任务，只要有一个任务执行完毕就继续执行后续，最先执行完的任务做作为返回结果的入参 public static void main(String[] args) throws IOException { CompletableFuture.anyOf( CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务A\"); return \"A\"; }), CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务B\"); return \"B\"; }), CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务C\"); return \"C\"; }) ).thenAccept(r -&gt; { System.out.println(\"任务D执行，\" + r + \"先执行完毕的\"); }); System.in.read(); } 2.3 CompletableFuture源码分析CompletableFuture的源码内容特别多。不需要把所有源码都看了，更多的是要掌握整个CompletableFuture的源码执行流程，以及任务的执行时机。 从CompletableFuture中比较简单的方法作为分析的入口，从而掌握整体执行的流程。 2.3.1 当前任务执行方式将任务和CompletableFuture封装到一起，再执行封住好的具体对象的run方法即可 // 提交任务到CompletableFuture public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable) { // asyncPool：执行任务的线程池 // runnable：具体任务。 return asyncRunStage(asyncPool, runnable); } // 内部执行的方法 static CompletableFuture&lt;Void&gt; asyncRunStage(Executor e, Runnable f) { // 对任务做非空校验 if (f == null) throw new NullPointerException(); // 直接构建了CompletableFuture的对象，作为最后的返回结果 CompletableFuture&lt;Void&gt; d = new CompletableFuture&lt;Void&gt;(); // 将任务和CompletableFuture对象封装为了AsyncRun的对象 // 将封装好的任务交给了线程池去执行 e.execute(new AsyncRun(d, f)); // 返回构建好的CompletableFuture return d; } // 封装任务的AsyncRun类信息 static final class AsyncRun extends ForkJoinTask&lt;Void&gt; implements Runnable, AsynchronousCompletionTask { // 声明存储CompletableFuture对象以及任务的成员变量 CompletableFuture&lt;Void&gt; dep; Runnable fn; // 将传入的属性赋值给成员变量 AsyncRun(CompletableFuture&lt;Void&gt; dep, Runnable fn) { this.dep = dep; this.fn = fn; } // 当前对象作为任务提交给线程池之后，必然会执行当前方法 public void run() { // 声明局部变量 CompletableFuture&lt;Void&gt; d; Runnable f; // 将成员变量赋值给局部变量，并且做非空判断 if ((d = dep) != null &amp;&amp; (f = fn) != null) { // help GC，将成员变量置位null，只要当前任务结束后，成员变量也拿不到引用。 dep = null; fn = null; // 先确认任务没有执行。 if (d.result == null) { try { // 直接执行任务 f.run(); // 当前方法是针对Runnable任务的，不能将结果置位null // 要给没有返回结果的Runnable做一个返回结果 d.completeNull(); } catch (Throwable ex) { // 异常结束！ d.completeThrowable(ex); } } d.postComplete(); } } } 2.3.2 任务编排的存储&amp;执行方式首先如果要在前继任务处理后，执行后置任务的话。 有两种情况： 前继任务如果没有执行完毕，后置任务需要先放在stack栈结构中存储 前继任务已经执行完毕了，后置任务就应该直接执行，不需要在往stack中存储了。 如果单独采用thenRun在一个任务后面指定多个后继任务，CompletableFuture无法保证具体的执行顺序，而影响执行顺序的是前继任务的执行时间，以及后置任务编排的时机。 2.3.3 任务编排流程// 编排任务，前继任务搞定，后继任务再执行 public CompletableFuture&lt;Void&gt; thenRun(Runnable action) { // 执行了内部的uniRunStage方法， // null：线程池，现在没给。 // action：具体要执行的任务 return uniRunStage(null, action); } // 内部编排任务方法 private CompletableFuture&lt;Void&gt; uniRunStage(Executor e, Runnable f) { // 后继任务不能为null，健壮性判断 if (f == null) throw new NullPointerException(); // 创建CompletableFuture对象d，与后继任务f绑定 CompletableFuture&lt;Void&gt; d = new CompletableFuture&lt;Void&gt;(); // 如果线程池不为null，代表异步执行，将任务压栈 // 如果线程池是null，先基于uniRun尝试下，看任务能否执行 if (e != null || !d.uniRun(this, f, null)) { // 如果传了线程池，这边需要走一下具体逻辑 // e：线程池 // d：后继任务的CompletableFuture // this：前继任务的CompletableFuture // f：后继任务 UniRun&lt;T&gt; c = new UniRun&lt;T&gt;(e, d, this, f); // 将封装好的任务，push到stack栈结构 // 只要前继任务没结束，这边就可以正常的将任务推到栈结构中 // 放入栈中可能会失败 push(c); // 无论压栈成功与否，都要尝试执行以下。 c.tryFire(SYNC); } // 无论任务执行完毕与否，都要返回后继任务的CompletableFuture return d; } 2.3.4 查看后置任务执行时机任务在编排到前继任务时，因为前继任务已经结束了，这边后置任务会主动的执行 // 后置任务无论压栈成功与否，都需要执行tryFire方法 static final class UniRun&lt;T&gt; extends UniCompletion&lt;T,Void&gt; { Runnable fn; // executor：线程池 // dep：后置任务的CompletableFuture // src：前继任务的CompletableFuture // fn：具体的任务 UniRun(Executor executor, CompletableFuture&lt;Void&gt; dep,CompletableFuture&lt;T&gt; src, Runnable fn) { super(executor, dep, src); this.fn = fn; } final CompletableFuture&lt;Void&gt; tryFire(int mode) { // 声明局部变量 CompletableFuture&lt;Void&gt; d; CompletableFuture&lt;T&gt; a; // 赋值局部变量 // (d = dep) == null：赋值加健壮性校验 if ((d = dep) == null || // 调用uniRun。 // a：前继任务的CompletableFuture // fn：后置任务 // 第三个参数：传入的是this，是UniRun对象 !d.uniRun(a = src, fn, mode &gt; 0 ? null : this)) // 进到这，说明前继任务没结束，等！ return null; dep = null; src = null; fn = null; return d.postFire(a, mode); } } // 是否要主动执行任务 final boolean uniRun(CompletableFuture&lt;?&gt; a, Runnable f, UniRun&lt;?&gt; c) { // 方法要么正常结束，要么异常结束 Object r; Throwable x; // a == null：健壮性校验 // (r = a.result) == null：判断前继任务结束了么？ // f == null：健壮性校验 if (a == null || (r = a.result) == null || f == null) // 到这代表任务没结束。 return false; // 后置任务执行了没？ == null，代表没执行 if (result == null) { // 如果前继任务的结果是异常结束。如果前继异常结束，直接告辞，封装异常结果 if (r instanceof AltResult &amp;&amp; (x = ((AltResult)r).ex) != null) completeThrowable(x, r); else // 到这，前继任务正常结束，后置任务正常执行 try { // 如果基于tryFire(SYNC)进来，这里的C不为null，执行c.claim // 如果是因为没有传递executor，c就是null，不会执行c.claim if (c != null &amp;&amp; !c.claim()) // 如果返回false，任务异步执行了，直接return false return false; // 如果claim没有基于线程池运行任务，那这里就是同步执行 // 直接f.run了。 f.run(); // 封装Null结果 completeNull(); } catch (Throwable ex) { // 封装异常结果 completeThrowable(ex); } } return true; } // 异步的线程池处理任务 final boolean claim() { Executor e = executor; if (compareAndSetForkJoinTaskTag((short)0, (short)1)) { // 只要有线程池对象，不为null if (e == null) return true; executor = null; // disable // 基于线程池的execute去执行任务 e.execute(this); } return false; } 前继任务执行完毕后，基于嵌套的方式执行后置。 // A：嵌套了B+C， B：嵌套了D+E // 前继任务搞定，遍历stack执行后置任务 // A任务处理完，解决嵌套的B和C final void postComplete() { // f：前继任务的CompletableFuture // h：存储后置任务的栈结构 CompletableFuture&lt;?&gt; f = this; Completion h; // (h = f.stack) != null：赋值加健壮性判断，要确保栈中有数据 while ((h = f.stack) != null || // 循环一次后，对后续节点的赋值以及健壮性判断，要确保栈中有数据 (f != this &amp;&amp; (h = (f = this).stack) != null)) { // t：当前栈中任务的后续任务 CompletableFuture&lt;?&gt; d; Completion t; // 拿到之前的栈顶h后，将栈顶换数据 if (f.casStack(h, t = h.next)) { if (t != null) { if (f != this) { pushStack(h); continue; } h.next = null; // detach } // 执行tryFire方法， f = (d = h.tryFire(NESTED)) == null ? this : d; } } } // 回来了 NESTED == -1 final CompletableFuture&lt;Void&gt; tryFire(int mode) { CompletableFuture&lt;Void&gt; d; CompletableFuture&lt;T&gt; a; if ((d = dep) == null || !d.uniRun(a = src, fn, mode &gt; 0 ? null : this)) return null; dep = null; src = null; fn = null; // 内部会执行postComplete，运行B内部嵌套的D和E return d.postFire(a, mode); } 2.4 CompletableFuture执行流程图","categories":[{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"java 并发","slug":"java-并发","permalink":"http://example.com/tags/java-%E5%B9%B6%E5%8F%91/"}]},{"title":"并发编程(02)","slug":"并发编程-02","date":"2023-09-11T01:16:21.000Z","updated":"2023-09-11T06:40:28.314Z","comments":true,"path":"posts/21.html","link":"","permalink":"http://example.com/posts/21.html","excerpt":"","text":"线程池 (上传仅供个人学习交流使用 如有侵权立刻删除) 一、什么是线程池为什么要使用线程池 在开发中，为了提升效率的操作，我们需要将一些业务采用多线程的方式去执行。 比如有一个比较大的任务，可以将任务分成几块，分别交给几个线程去执行，最终做一个汇总就可以了。 比如做业务操作时，需要发送短信或者是发送邮件，这种操作也可以基于异步的方式完成，这种异步的方式，其实就是再构建一个线程去执行。 但是，如果每次异步操作或者多线程操作都需要新创建一个线程，使用完毕后，线程再被销毁，这样的话，对系统造成一些额外的开销。在处理过程中到底由多线程处理了多少个任务，以及每个线程的开销无法统计和管理。 所以咱们需要一个线程池机制来管理这些内容。线程池的概念和连接池类似，都是在一个Java的集合中存储大量的线程对象，每次需要执行异步操作或者多线程操作时，不需要重新创建线程，直接从集合中拿到线程对象直接执行方法就可以了。 JDK中就提供了线程池的类。 在线程池构建初期，可以将任务提交到线程池中。会根据一定的机制来异步执行这个任务。 可能任务直接被执行 任务可以暂时被存储起来了。等到有空闲线程再来处理。 任务也可能被拒绝，无法被执行。 JDK提供的线程池中记录了每个线程处理了多少个任务，以及整个线程池处理了多少个任务。同时还可以针对任务执行前后做一些勾子函数的实现。可以在任务执行前后做一些日志信息，这样可以多记录信息方便后面统计线程池执行任务时的一些内容参数等等…… 二、JDK自带的构建线程池的方式JDK中基于Executors提供了很多种线程池 2.1 newFixedThreadPool这个线程池的特别是线程数是固定的。 在Executors中第一个方法就是构建newFixedThreadPool public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } 构建时，需要给newFixedThreadPool方法提供一个nThreads的属性，而这个属性其实就是当前线程池中线程的个数。当前线程池的本质其实就是使用ThreadPoolExecutor。 构建好当前线程池后，线程个数已经固定好（线程是懒加载，在构建之初，线程并没有构建出来，而是随着人任务的提交才会将线程在线程池中国构建出来）。如果线程没构建，线程会待着任务执行被创建和执行。如果线程都已经构建好了，此时任务会被放到LinkedBlockingQueue无界队列中存放，等待线程从LinkedBlockingQueue中去take出任务，然后执行。 测试功能效果 public static void main(String[] args) throws Exception { ExecutorService threadPool = Executors.newFixedThreadPool(3); threadPool.execute(() -&gt; { System.out.println(\"1号任务：\" + Thread.currentThread().getName() + System.currentTimeMillis()); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } }); threadPool.execute(() -&gt; { System.out.println(\"2号任务：\" + Thread.currentThread().getName() + System.currentTimeMillis()); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } }); threadPool.execute(() -&gt; { System.out.println(\"3号任务：\" + Thread.currentThread().getName() + System.currentTimeMillis()); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } }); } 2.2 newSingleThreadExecutor这个线程池看名字就知道是单例线程池，线程池中只有一个工作线程在处理任务 如果业务涉及到顺序消费，可以采用newSingleThreadExecutor // 当前这里就是构建单例线程池的方式 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService // 在内部依然是构建了ThreadPoolExecutor，设置的线程个数为1 // 当任务投递过来后，第一个任务会被工作线程处理，后续的任务会被扔到阻塞队列中 // 投递到阻塞队列中任务的顺序，就是工作线程处理的顺序 // 当前这种线程池可以用作顺序处理的一些业务中 (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } static class FinalizableDelegatedExecutorService extends DelegatedExecutorService { // 线程池的使用没有区别，跟正常的ThreadPoolExecutor没区别 FinalizableDelegatedExecutorService(ExecutorService executor) { super(executor); } // finalize是当前对象被GC干掉之前要执行的方法 // 当前FinalizableDelegatedExecutorService的目的是为了在当前线程池被GC回收之前 // 可以执行shutdown，shutdown方法是将当前线程池停止，并且干掉工作线程 // 但是不能基于这种方式保证线程池一定会执行shutdown // finalize在执行时，是守护线程，这种线程无法保证一定可以执行完毕。 // 在使用线程池时，如果线程池是基于一个业务构建的，在使用完毕之后，一定要手动执行shutdown， // 否则会造成JVM中一堆线程 protected void finalize() { super.shutdown(); } } 测试单例线程池效果： public static void main(String[] args) throws Exception { ExecutorService threadPool = Executors.newSingleThreadExecutor(); threadPool.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + \",\" + \"111\"); }); threadPool.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + \",\" + \"222\"); }); threadPool.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + \",\" + \"333\"); }); threadPool.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + \",\" + \"444\"); }); } 测试线程池使用完毕后，不执行shutdown的后果： 如果是局部变量仅限当前线程池使用的线程池，在使用完毕之后要记得执行shutdown，避免线程无法结束 如果是全局的线程池，很多业务都会到，使用完毕后不要shutdown，因为其他业务也要执行当前线程池 static ExecutorService threadPool = Executors.newFixedThreadPool(200); public static void main(String[] args) throws Exception { newThreadPool(); System.gc(); Thread.sleep(5000); System.out.println(\"线程池被回收了！！\"); System.in.read(); } private static void newThreadPool(){ for (int i = 0; i &lt; 200; i++) { final int a = i; threadPool.execute(() -&gt; { System.out.println(a); }); } threadPool.shutdown(); for (int i = 0; i &lt; 200; i++) { final int a = i; threadPool.execute(() -&gt; { System.out.println(a); }); } } 2.3 newCachedThreadPool看名字好像是一个缓存的线程池，查看一下构建的方式 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } 当第一次提交任务到线程池时，会直接构建一个工作线程 这个工作线程带执行完人后，60秒没有任务可以执行后，会结束 如果在等待60秒期间有任务进来，他会再次拿到这个任务去执行 如果后续提升任务时，没有线程是空闲的，那么就构建工作线程去执行。 最大的一个特点，任务只要提交到当前的newCachedThreadPool中，就必然有工作线程可以处理 代码测试效果 public static void main(String[] args) throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 1; i &lt;= 200; i++) { final int j = i; executorService.execute(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \":\" + j); }); } } 2.4 newScheduleThreadPool首先看到名字就可以猜到当前线程池是一个定时任务的线程池，而这个线程池就是可以以一定周期去执行一个任务，或者是延迟多久执行一个任务一次 查看一下如何构建的。 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } 基于这个方法可以看到，构建的是ScheduledThreadPoolExecutor线程池 public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor{ //.... } 所以本质上还是正常线程池，只不过在原来的线程池基础上实现了定时任务的功能 原理是基于DelayQueue实现的延迟执行。周期性执行是任务执行完毕后，再次扔回到阻塞队列。 代码查看使用的方式和效果 public static void main(String[] args) throws Exception { ScheduledExecutorService pool = Executors.newScheduledThreadPool(10); // 正常执行 // pool.execute(() -&gt; { // System.out.println(Thread.currentThread().getName() + \"：1\"); // }); // 延迟执行，执行当前任务延迟5s后再执行 // pool.schedule(() -&gt; { // System.out.println(Thread.currentThread().getName() + \"：2\"); // },5,TimeUnit.SECONDS); // 周期执行，当前任务第一次延迟5s执行，然后没3s执行一次 // 这个方法在计算下次执行时间时，是从任务刚刚开始时就计算。 // pool.scheduleAtFixedRate(() -&gt; { // try { // Thread.sleep(3000); // } catch (InterruptedException e) { // e.printStackTrace(); // } // System.out.println(System.currentTimeMillis() + \"：3\"); // },2,1,TimeUnit.SECONDS); // 周期执行，当前任务第一次延迟5s执行，然后没3s执行一次 // 这个方法在计算下次执行时间时，会等待任务结束后，再计算时间 pool.scheduleWithFixedDelay(() -&gt; { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(System.currentTimeMillis() + \"：3\"); },2,1,TimeUnit.SECONDS); } 至于Executors提供的newSingleThreadScheduledExecutor单例的定时任务线程池就不说了。 一个线程的线程池可以延迟或者以一定的周期执行一个任务。 2.5 newWorkStealingPool当前JDK提供构建线程池的方式newWorkStealingPool和之前的线程池很非常大的区别 之前定长，单例，缓存，定时任务都基于ThreadPoolExecutor去实现的。 newWorkStealingPool是基于ForkJoinPool构建出来的 ThreadPoolExecutor的核心点： 在ThreadPoolExecutor中只有一个阻塞队列存放当前任务 ForkJoinPool的核心特点： ForkJoinPool从名字上就能看出一些东西。当有一个特别大的任务时，如果采用上述方式，这个大任务只能会某一个线程去执行。ForkJoin第一个特点是可以将一个大任务拆分成多个小任务，放到当前线程的阻塞队列中。其他的空闲线程就可以去处理有任务的线程的阻塞队列中的任务 来一个比较大的数组，里面存满值，计算总和 单线程处理一个任务： /** 非常大的数组 */ static int[] nums = new int[1_000_000_000]; // 填充值 static{ for (int i = 0; i &lt; nums.length; i++) { nums[i] = (int) ((Math.random()) * 1000); } } public static void main(String[] args) { // ===================单线程累加10亿数据================================ System.out.println(\"单线程计算数组总和！\"); long start = System.nanoTime(); int sum = 0; for (int num : nums) { sum += num; } long end = System.nanoTime(); System.out.println(\"单线程运算结果为：\" + sum + \"，计算时间为：\" + (end - start)); } 多线程分而治之的方式处理： /** 非常大的数组 */ static int[] nums = new int[1_000_000_000]; // 填充值 static{ for (int i = 0; i &lt; nums.length; i++) { nums[i] = (int) ((Math.random()) * 1000); } } public static void main(String[] args) { // ===================单线程累加10亿数据================================ System.out.println(\"单线程计算数组总和！\"); long start = System.nanoTime(); int sum = 0; for (int num : nums) { sum += num; } long end = System.nanoTime(); System.out.println(\"单线程运算结果为：\" + sum + \"，计算时间为：\" + (end - start)); // ===================多线程分而治之累加10亿数据================================ // 在使用forkJoinPool时，不推荐使用Runnable和Callable // 可以使用提供的另外两种任务的描述方式 // Runnable(没有返回结果) -&gt; RecursiveAction // Callable(有返回结果) -&gt; RecursiveTask ForkJoinPool forkJoinPool = (ForkJoinPool) Executors.newWorkStealingPool(); System.out.println(\"分而治之计算数组总和！\"); long forkJoinStart = System.nanoTime(); ForkJoinTask&lt;Integer&gt; task = forkJoinPool.submit(new SumRecursiveTask(0, nums.length - 1)); Integer result = task.join(); long forkJoinEnd = System.nanoTime(); System.out.println(\"分而治之运算结果为：\" + result + \"，计算时间为：\" + (forkJoinEnd - forkJoinStart)); } private static class SumRecursiveTask extends RecursiveTask&lt;Integer&gt;{ /** 指定一个线程处理哪个位置的数据 */ private int start,end; private final int MAX_STRIDE = 100_000_000; // 200_000_000： 147964900 // 100_000_000： 145942100 public SumRecursiveTask(int start, int end) { this.start = start; this.end = end; } @Override protected Integer compute() { // 在这个方法中，需要设置好任务拆分的逻辑以及聚合的逻辑 int sum = 0; int stride = end - start; if(stride &lt;= MAX_STRIDE){ // 可以处理任务 for (int i = start; i &lt;= end; i++) { sum += nums[i]; } }else{ // 将任务拆分，分而治之。 int middle = (start + end) / 2; // 声明为2个任务 SumRecursiveTask left = new SumRecursiveTask(start, middle); SumRecursiveTask right = new SumRecursiveTask(middle + 1, end); // 分别执行两个任务 left.fork(); right.fork(); // 等待结果，并且获取sum sum = left.join() + right.join(); } return sum; } } 最终可以发现，这种累加的操作中，采用分而治之的方式效率提升了2倍多。 但是也不是所有任务都能拆分提升效率，首先任务得大，耗时要长。 三、ThreadPoolExecutor应用&amp;源码剖析前面讲到的Executors中的构建线程池的方式，大多数还是基于ThreadPoolExecutor去new出来的。 3.1 为什么要自定义线程池首先ThreadPoolExecutor中，一共提供了7个参数，每个参数都是非常核心的属性，在线程池去执行任务时，每个参数都有决定性的作用。 但是如果直接采用JDK提供的方式去构建，可以设置的核心参数最多就两个，这样就会导致对线程池的控制粒度很粗。所以在阿里规范中也推荐自己去自定义线程池。手动的去new ThreadPoolExecutor设置他的一些核心属性。 自定义构建线程池，可以细粒度的控制线程池，去管理内存的属性，并且针对一些参数的设置可能更好的在后期排查问题。 查看一下ThreadPoolExecutor提供的七个核心参数 public ThreadPoolExecutor( int corePoolSize, // 核心工作线程（当前任务执行结束后，不会被销毁） int maximumPoolSize, // 最大工作线程（代表当前线程池中，一共可以有多少个工作线程） long keepAliveTime, // 非核心工作线程在阻塞队列位置等待的时间 TimeUnit unit, // 非核心工作线程在阻塞队列位置等待时间的单位 BlockingQueue&lt;Runnable&gt; workQueue, // 任务在没有核心工作线程处理时，任务先扔到阻塞队列中 ThreadFactory threadFactory, // 构建线程的线程工作，可以设置thread的一些信息 RejectedExecutionHandler handler) { // 当线程池无法处理投递过来的任务时，执行当前的拒绝策略 // 初始化线程池的操作 } 3.2 ThreadPoolExecutor应用手动new一下，处理的方式还是执行execute或者submit方法。 JDK提供的几种拒绝策略： AbortPolicy：当前拒绝策略会在无法处理任务时，直接抛出一个异常 public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); } CallerRunsPolicy：当前拒绝策略会在线程池无法处理任务时，将任务交给调用者处理 public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { r.run(); } } DiscardPolicy：当前拒绝策略会在线程池无法处理任务时，直接将任务丢弃掉 public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { } DiscardOldestPolicy：当前拒绝策略会在线程池无法处理任务时，将队列中最早的任务丢弃掉，将当前任务再次尝试交给线程池处理 public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { e.getQueue().poll(); e.execute(r); } } 自定义Policy：根据自己的业务，可以将任务扔到数据库，也可以做其他操作。 private static class MyRejectedExecution implements RejectedExecutionHandler{ @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.out.println(\"根据自己的业务情况，决定编写的代码！\"); } } 代码构建线程池，并处理有无返回结果的任务 public static void main(String[] args) throws ExecutionException, InterruptedException { //1. 构建线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor( 2, 5, 10, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(5), new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setName(\"test-ThreadPoolExecutor\"); return thread; } }, new MyRejectedExecution() ); //2. 让线程池处理任务,没返回结果 threadPool.execute(() -&gt; { System.out.println(\"没有返回结果的任务\"); }); //3. 让线程池处理有返回结果的任务 Future&lt;Object&gt; future = threadPool.submit(new Callable&lt;Object&gt;() { @Override public Object call() throws Exception { System.out.println(\"我有返回结果！\"); return \"返回结果\"; } }); Object result = future.get(); System.out.println(result); //4. 如果是局部变量的线程池，记得用完要shutdown threadPool.shutdown(); } private static class MyRejectedExecution implements RejectedExecutionHandler{ @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.out.println(\"根据自己的业务情况，决定编写的代码！\"); } } 3.3 ThreadPoolExecutor源码剖析线程池的源码内容会比较多一点，需要一点一点的去查看，内部比较多。 3.3.1 ThreadPoolExecutor的核心属性核心属性主要就是ctl，基于ctl拿到线程池的状态以及工作线程个数 在整个线程池的执行流程中，会基于ctl判断上述两个内容 // 当前是线程池的核心属性 // 当前的ctl其实就是一个int类型的数值，内部是基于AtomicInteger套了一层，进行运算时，是原子性的。 // ctl表示着线程池中的2个核心状态： // 线程池的状态：ctl的高3位，表示线程池状态 // 工作线程的数量：ctl的低29位，表示工作线程的个数 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); // Integer.SIZE：在获取Integer的bit位个数 // 声明了一个常量：COUNT_BITS = 29 private static final int COUNT_BITS = Integer.SIZE - 3; 00000000 00000000 00000000 00000001 00100000 00000000 00000000 00000000 00011111 11111111 11111111 11111111 // CAPACITY就是当前工作线程能记录的工作线程的最大个数 private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; // 线程池状态的表示 // 当前五个状态中，只有RUNNING状态代表线程池没问题，可以正常接收任务处理 // 111：代表RUNNING状态，RUNNING可以处理任务，并且处理阻塞队列中的任务。 private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; // 000：代表SHUTDOWN状态，不会接收新任务，正在处理的任务正常进行，阻塞队列的任务也会做完。 private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; // 001：代表STOP状态，不会接收新任务，正在处理任务的线程会被中断，阻塞队列的任务一个不管。 private static final int STOP = 1 &lt;&lt; COUNT_BITS; // 010：代表TIDYING状态，这个状态是否SHUTDOWN或者STOP转换过来的，代表当前线程池马上关闭，就是过渡状态。 private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; // 011：代表TERMINATED状态，这个状态是TIDYING状态转换过来的，转换过来只需要执行一个terminated方法。 private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 在使用下面这几个方法时，需要传递ctl进来 // 基于&amp;运算的特点，保证只会拿到ctl高三位的值。 private static int runStateOf(int c) { return c &amp; ~CAPACITY; } // 基于&amp;运算的特点，保证只会拿到ctl低29位的值。 private static int workerCountOf(int c) { return c &amp; CAPACITY; } 线程池状态的特点以及转换的方式 3.3.2 ThreadPoolExecutor的有参构造有参构造没啥说的，记住核心线程个数是允许为0的。 // 有参构造。无论调用哪个有参构造，最终都会执行当前的有参构造 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { // 健壮性校验 // 核心线程个数是允许为0个的。 // 最大线程数必须大于0，最大线程数要大于等于核心线程数 // 非核心线程的最大空闲时间，可以等于0 if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) // 不满足要求就抛出参数异常 throw new IllegalArgumentException(); // 阻塞队列，线程工厂，拒绝策略都不允许为null，为null就扔空指针异常 if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); // 不要关注当前内容，系统资源访问决策，和线程池核心业务关系不大。 this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); // 各种赋值，JUC包下，几乎所有涉及到线程挂起的操作，单位都用纳秒。 // 有参构造的值，都赋值给成员变量。 // Doug Lea的习惯就是将成员变量作为局部变量单独操作。 this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 3.3.3 ThreadPoolExecutor的execute方法execute方法是提交任务到线程池的核心方法，很重要 线程池的执行流程其实就是在说execute方法内部做了哪些判断 execute源码的分析 // 提交任务到线程池的核心方法 // command就是提交过来的任务 public void execute(Runnable command) { // 提交的任务不能为null if (command == null) throw new NullPointerException(); // 获取核心属性ctl，用于后面的判断 int c = ctl.get(); // 如果工作线程个数，小于核心线程数。 // 满足要求，添加核心工作线程 if (workerCountOf(c) &lt; corePoolSize) { // addWorker(任务,是核心线程吗) // addWorker返回true：代表添加工作线程成功 // addWorker返回false：代表添加工作线程失败 // addWorker中会基于线程池状态，以及工作线程个数做判断，查看能否添加工作线程 if (addWorker(command, true)) // 工作线程构建出来了，任务也交给command去处理了。 return; // 说明线程池状态或者是工作线程个数发生了变化，导致添加失败，重新获取一次ctl c = ctl.get(); } // 添加核心工作线程失败，往这走 // 判断线程池状态是否是RUNNING，如果是，正常基于阻塞队列的offer方法，将任务添加到阻塞队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { // 如果任务添加到阻塞队列成功，走if内部 // 如果任务在扔到阻塞队列之前，线程池状态突然改变了。 // 重新获取ctl int recheck = ctl.get(); // 如果线程池的状态不是RUNNING，将任务从阻塞队列移除， if (!isRunning(recheck) &amp;&amp; remove(command)) // 并且直接拒绝策略 reject(command); // 在这，说明阻塞队列有我刚刚放进去的任务 // 查看一下工作线程数是不是0个 // 如果工作线程为0个，需要添加一个非核心工作线程去处理阻塞队列中的任务 // 发生这种情况有两种： // 1. 构建线程池时，核心线程数是0个。 // 2. 即便有核心线程，可以设置核心线程也允许超时，设置allowCoreThreadTimeOut为true，代表核心线程也可以超时 else if (workerCountOf(recheck) == 0) // 为了避免阻塞队列中的任务饥饿，添加一个非核心工作线程去处理 addWorker(null, false); } // 任务添加到阻塞队列失败 // 构建一个非核心工作线程 // 如果添加非核心工作线程成功，直接完事，告辞 else if (!addWorker(command, false)) // 添加失败，执行决绝策略 reject(command); } execute方法的完整执行流程图 3.3.4 ThreadPoolExecutor的addWorker方法addWorker中主要分成两大块去看 第一块：校验线程池的状态以及工作线程个数 第二块：添加工作线程并且启动工作线程 校验线程池的状态以及工作线程个数 // 添加工作线程之校验源码 private boolean addWorker(Runnable firstTask, boolean core) { // 外层for循环在校验线程池的状态 // 内层for循环是在校验工作线程的个数 // retry是给外层for循环添加一个标记，是为了方便在内层for循坏跳出外层for循环 retry: for (;;) { // 获取ctl int c = ctl.get(); // 拿到ctl的高3位的值 int rs = runStateOf(c); //==========================线程池状态判断================================================== // 如果线程池状态是SHUTDOWN，并且此时阻塞队列有任务，工作线程个数为0，添加一个工作线程去处理阻塞队列的任务 // 判断线程池的状态是否大于等于SHUTDOWN，如果满足，说明线程池不是RUNNING if (rs &gt;= SHUTDOWN &amp;&amp; // 如果这三个条件都满足，就代表是要添加非核心工作线程去处理阻塞队列任务 // 如果三个条件有一个没满足，返回false，配合!，就代表不需要添加 !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) // 不需要添加工作线程 return false; for (;;) { //==========================工作线程个数判断================================================== // 基于ctl拿到低29位的值，代表当前工作线程个数 int wc = workerCountOf(c); // 如果工作线程个数大于最大值了，不可以添加了，返回false if (wc &gt;= CAPACITY || // 基于core来判断添加的是否是核心工作线程 // 如果是核心：基于corePoolSize去判断 // 如果是非核心：基于maximumPoolSize去判断 wc &gt;= (core ? corePoolSize : maximumPoolSize)) // 代表不能添加，工作线程个数不满足要求 return false; // 针对ctl进行 + 1，采用CAS的方式 if (compareAndIncrementWorkerCount(c)) // CAS成功后，直接退出外层循环，代表可以执行添加工作线程操作了。 break retry; // 重新获取一次ctl的值 c = ctl.get(); // 判断重新获取到的ctl中，表示的线程池状态跟之前的是否有区别 // 如果状态不一样，说明有变化，重新的去判断线程池状态 if (runStateOf(c) != rs) // 跳出一次外层for循环 continue retry; } } // 省略添加工作线程以及启动的过程 } 添加工作线程并且启动工作线程 private boolean addWorker(Runnable firstTask, boolean core) { // 省略校验部分的代码 // 添加工作线程以及启动工作线程~~~ // 声明了三个变量 // 工作线程启动了没，默认false boolean workerStarted = false; // 工作线程添加了没，默认false boolean workerAdded = false; // 工作线程，默认为null Worker w = null; try { // 构建工作线程，并且将任务传递进去 w = new Worker(firstTask); // 获取了Worker中的Thread对象 final Thread t = w.thread; // 判断Thread是否不为null，在new Worker时，内部会通过给予的ThreadFactory去构建Thread交给Worker // 一般如果为null，代表ThreadFactory有问题。 if (t != null) { // 加锁，保证使用workers成员变量以及对largestPoolSize赋值时，保证线程安全 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 再次获取线程池状态。 int rs = runStateOf(ctl.get()); // 再次判断 // 如果满足 rs &lt; SHUTDOWN 说明线程池是RUNNING，状态正常，执行if代码块 // 如果线程池状态为SHUTDOWN，并且firstTask为null，添加非核心工作处理阻塞队列任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { // 到这，可以添加工作线程。 // 校验ThreadFactory构建线程后，不能自己启动线程，如果启动了，抛出异常 if (t.isAlive()) throw new IllegalThreadStateException(); // private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); // 将new好的Worker添加到HashSet中。 workers.add(w); // 获取了HashSet的size，拿到工作线程个数 int s = workers.size(); // largestPoolSize在记录最大线程个数的记录 // 如果当前工作线程个数，大于最大线程个数的记录，就赋值 if (s &gt; largestPoolSize) largestPoolSize = s; // 添加工作线程成功 workerAdded = true; } } finally { mainLock.unlock(); } // 如果工作线程添加成功， if (workerAdded) { // 直接启动Worker中的线程 t.start(); // 启动工作线程成功 workerStarted = true; } } } finally { // 做补偿的操作，如果工作线程启动失败，将这个添加失败的工作线程处理掉 if (!workerStarted) addWorkerFailed(w); } // 返回工作线程是否启动成功 return workerStarted; } // 工作线程启动失败，需要不的步长操作 private void addWorkerFailed(Worker w) { // 因为操作了workers，需要加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 如果w不为null，之前Worker已经new出来了。 if (w != null) // 从HashSet中移除 workers.remove(w); // 同时对ctl进行 - 1，代表去掉了一个工作线程个数 decrementWorkerCount(); // 因为工作线程启动失败，判断一下状态的问题，是不是可以走TIDYING状态最终到TERMINATED状态了。 tryTerminate(); } finally { // 释放锁 mainLock.unlock(); } } 3.3.5 ThreadPoolExecutor的Worker工作线程Worker对象主要包含了两个内容 工作线程要执行任务 工作线程可能会被中断，控制中断 // Worker继承了AQS，目的就是为了控制工作线程的中断。 // Worker实现了Runnable，内部的Thread对象，在执行start时，必然要执行Worker中断额一些操作 private final class Worker extends AbstractQueuedSynchronizer implements Runnable{ // =======================Worker管理任务================================ // 线程工厂构建的线程 final Thread thread; // 当前Worker要执行的任务 Runnable firstTask; // 记录当前工作线程处理了多少个任务。 volatile long completedTasks; // 有参构造 Worker(Runnable firstTask) { // 将State设置为-1，代表当前不允许中断线程 setState(-1); // 任务赋值 this.firstTask = firstTask; // 基于线程工作构建Thread，并且传入的Runnable是Worker this.thread = getThreadFactory().newThread(this); } // 当thread执行start方法时，调用的是Worker的run方法， public void run() { // 任务执行时，执行的是runWorker方法 runWorker(this); } // =======================Worker管理中断================================ // 当前方法是中断工作线程时，执行的方法 void interruptIfStarted() { Thread t; // 只有Worker中的state &gt;= 0的时候，可以中断工作线程 if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) { try { // 如果状态正常，并且线程未中断，这边就中断线程 t.interrupt(); } catch (SecurityException ignore) { } } } protected boolean isHeldExclusively() { return getState() != 0; } protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } protected boolean tryRelease(int unused) { setExclusiveOwnerThread(null); setState(0); return true; } public void lock() { acquire(1); } public boolean tryLock() { return tryAcquire(1); } public void unlock() { release(1); } public boolean isLocked() { return isHeldExclusively(); } } 3.3.6 ThreadPoolExecutor的runWorker方法runWorker就是让工作线程拿到任务去执行即可。 并且在内部也处理了在工作线程正常结束和异常结束时的处理方案 // 工作线程启动后执行的任务。 final void runWorker(Worker w) { // 拿到当前线程 Thread wt = Thread.currentThread(); // 从worker对象中拿到任务 Runnable task = w.firstTask; // 将Worker中的firstTask置位空 w.firstTask = null; // 将Worker中的state置位0，代表当前线程可以中断的 w.unlock(); // allow interrupts // 判断工作线程是否是异常结束，默认就是异常结束 boolean completedAbruptly = true; try { // 获取任务 // 直接拿到第一个任务去执行 // 如果第一个任务为null，去阻塞队列中获取任务 while (task != null || (task = getTask()) != null) { // 执行了Worker的lock方法，当前在lock时，shutdown操作不能中断当前线程，因为当前线程正在处理任务 w.lock(); // 比较ctl &gt;= STOP,如果满足找个状态，说明线程池已经到了STOP状态甚至已经要凉凉了 // 线程池到STOP状态，并且当前线程还没有中断，确保线程是中断的，进到if内部执行中断方法 // if(runStateAtLeast(ctl.get(), STOP) &amp;&amp; !wt.isInterrupted()) {中断线程} // 如果线程池状态不是STOP，确保线程不是中断的。 // 如果发现线程中断标记位是true了，再次查看线程池状态是大于STOP了，再次中断线程 // 这里其实就是做了一个事情，如果线程池状态 &gt;= STOP，确保线程中断了。 if ( ( runStateAtLeast(ctl.get(), STOP) || ( Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP) ) ) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { // 勾子函数在线程池中没有做任何的实现，如果需要在线程池执行任务前后做一些额外的处理，可以重写勾子函数 // 前置勾子函数 beforeExecute(wt, task); Throwable thrown = null; try { // 执行任务。 task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { // 前后置勾子函数 afterExecute(task, thrown); } } finally { // 任务执行完，丢掉任务 task = null; // 当前工作线程处理的任务数+1 w.completedTasks++; // 执行unlock方法，此时shutdown方法才可以中断当前线程 w.unlock(); } } // 如果while循环结束，正常走到这，说明是正常结束 // 正常结束的话，在getTask中就会做一个额外的处理，将ctl - 1，代表工作线程没一个。 completedAbruptly = false; } finally { // 考虑干掉工作线程 processWorkerExit(w, completedAbruptly); } } // 工作线程结束前，要执行当前方法 private void processWorkerExit(Worker w, boolean completedAbruptly) { // 如果是异常结束 if (completedAbruptly) // 将ctl - 1，扣掉一个工作线程 decrementWorkerCount(); // 操作Worker，为了线程安全，加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 当前工作线程处理的任务个数累加到线程池处理任务的个数属性中 completedTaskCount += w.completedTasks; // 将工作线程从hashSet中移除 workers.remove(w); } finally { // 释放锁 mainLock.unlock(); } // 只要工作线程凉了，查看是不是线程池状态改变了。 tryTerminate(); // 获取ctl int c = ctl.get(); // 判断线程池状态，当前线程池要么是RUNNING，要么是SHUTDOWN if (runStateLessThan(c, STOP)) { // 如果正常结束工作线程 if (!completedAbruptly) { // 如果核心线程允许超时，min = 0，否则就是核心线程个数 int min = allowCoreThreadTimeOut ? 0 : corePoolSize; // 如果min == 0，可能会出现没有工作线程，并且阻塞队列有任务没有线程处理 if (min == 0 &amp;&amp; ! workQueue.isEmpty()) // 至少要有一个工作线程处理阻塞队列任务 min = 1; // 如果工作线程个数 大于等于1，不怕没线程处理，正常return if (workerCountOf(c) &gt;= min) return; } // 异常结束，为了避免出现问题，添加一个空任务的非核心线程来填补上刚刚异常结束的工作线程 addWorker(null, false); } } 3.3.7 ThreadPoolExecutor的getTask方法工作线程在去阻塞队列获取任务前，要先查看线程池状态 如果状态没问题，去阻塞队列take或者是poll任务 第二个循环时，不但要判断线程池状态，还要判断当前工作线程是否可以被干掉 // 当前方法就在阻塞队列中获取任务 // 前面半部分是判断当前工作线程是否可以返回null，结束。 // 后半部分就是从阻塞队列中拿任务 private Runnable getTask() { // timeOut默认值是false。 boolean timedOut = false; // 死循环 for (;;) { // 拿到ctl int c = ctl.get(); // 拿到线程池的状态 int rs = runStateOf(c); // 如果线程池状态是STOP，没有必要处理阻塞队列任务，直接返回null // 如果线程池状态是SHUTDOWN，并且阻塞队列是空的，直接返回null if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) { // 如果可以返回null，先扣减工作线程个数 decrementWorkerCount(); // 返回null，结束runWorker的while循环 return null; } // 基于ctl拿到工作线程个数 int wc = workerCountOf(c); // 核心线程允许超时，timed为true // 工作线程个数大于核心线程数，timed为true boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ( // 如果工作线程个数，大于最大线程数。（一般情况不会满足），把他看成false // 第二个判断代表，只要工作线程数小于等于核心线程数，必然为false // 即便工作线程个数大于核心线程数了，此时第一次循环也不会为true，因为timedOut默认值是false // 考虑第二次循环了，因为循环内部必然有修改timeOut的位置 (wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; // 要么工作线程还有，要么阻塞队列为空，并且满足上述条件后，工作线程才会走到if内部，结束工作线程 (wc &gt; 1 || workQueue.isEmpty()) ) { // 第二次循环才有可能到这。 // 正常结束，工作线程 - 1，因为是CAS操作，如果失败了，重新走for循环 if (compareAndDecrementWorkerCount(c)) return null; continue; } // 工作线程从阻塞队列拿任务 try { // 如果是核心线程，timed是false，如果是非核心线程，timed就是true Runnable r = timed ? // 如果是非核心，走poll方法，拿任务，等待一会 workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : // 如果是核心，走take方法，死等。 workQueue.take(); // 从阻塞队列拿到的任务不为null，这边就正常返回任务，去执行 if (r != null) return r; // 说明当前线程没拿到任务，将timeOut设置为true，在上面就可以返回null退出了。 timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } 3.3.8 ThreadPoolExecutor的关闭方法首先查看shutdownNow方法，可以从RUNNING状态转变为STOP // shutDownNow方法，shutdownNow不会处理阻塞队列的任务，将任务全部给你返回了。 public List&lt;Runnable&gt; shutdownNow() { // 声明返回结果 List&lt;Runnable&gt; tasks; // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 不关注这个方法…… checkShutdownAccess(); // 将线程池状态修改为STOP advanceRunState(STOP); // 无论怎么，直接中断工作线程。 interruptWorkers(); // 将阻塞队列的任务全部扔到List集合中。 tasks = drainQueue(); } finally { // 释放锁 mainLock.unlock(); } tryTerminate(); return tasks; } // 将线程池状态修改为STOP private void advanceRunState(int STOP) { // 死循环。 for (;;) { // 获取ctl属性的值 int c = ctl.get(); // 第一个判断：如果当前线程池状态已经大于等于STOP了，不管了，告辞。 if (runStateAtLeast(c, STOP) || // 基于CAS，将ctl从c修改为STOP状态，不修改工作线程个数，但是状态变为了STOP // 如果修改成功结束 ctl.compareAndSet(c, ctlOf(STOP, workerCountOf(c)))) break; } } // 无论怎么，直接中断工作线程。 private void interruptWorkers() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 遍历HashSet，拿到所有的工作线程，直接中断。 for (Worker w : workers) w.interruptIfStarted(); } finally { mainLock.unlock(); } } // 移除阻塞队列，内容全部扔到List集合中 private List&lt;Runnable&gt; drainQueue() { BlockingQueue&lt;Runnable&gt; q = workQueue; ArrayList&lt;Runnable&gt; taskList = new ArrayList&lt;Runnable&gt;(); // 阻塞队列自带的，直接清空阻塞队列，内容扔到List集合 q.drainTo(taskList); // 为了避免任务丢失，重新判断，是否需要编辑阻塞队列，重新扔到List if (!q.isEmpty()) { for (Runnable r : q.toArray(new Runnable[0])) { if (q.remove(r)) taskList.add(r); } } return taskList; } // 查看当前线程池是否可以变为TERMINATED状态 final void tryTerminate() { // 死循环。 for (;;) { // 拿到ctl int c = ctl.get(); // 如果是RUNNING，直接告辞。 // 如果状态已经大于等于TIDYING，马上就要凉凉，直接告辞。 // 如果状态是SHUTDOWN，但是阻塞队列还有任务，直接告辞。 if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 如果还有工作线程 if (workerCountOf(c) != 0) { // 再次中断工作线程 interruptIdleWorkers(ONLY_ONE); // 告辞，等你工作线程全完事，我这再尝试进入到TERMINATED状态 return; } // 加锁，为了可以执行Condition的释放操作 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 将线程池状态修改为TIDYING状态，如果成功，继续往下走 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) { try { // 这个方法是空的，如果你需要在线程池关闭后做一些额外操作，这里你可以自行实现 terminated(); } finally { // 最终修改为TERMINATED状态 ctl.set(ctlOf(TERMINATED, 0)); // 线程池提供了一个方法，主线程在提交任务到线程池后，是可以继续做其他操作的。 // 咱们也可以让主线程提交任务后，等待线程池处理完毕，再做后续操作 // 这里线程池凉凉后，要唤醒哪些调用了awaitTermination方法的线程 termination.signalAll(); } return; } } finally { mainLock.unlock(); } // else retry on failed CAS } } 再次shutdown方法，可以从RUNNING状态转变为SHUTDOWN shutdown状态下，不会中断正在干活的线程，而且会处理阻塞队列中的任务 public void shutdown() { // 加锁。。 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 不看。 checkShutdownAccess(); // 里面是一个死循环，将线程池状态修改为SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); // 说了，这个是为了ScheduleThreadPoolExecutor准备的，不管 onShutdown(); } finally { mainLock.unlock(); } // 尝试结束线程 tryTerminate(); } // 中断空闲线程 private void interruptIdleWorkers(boolean onlyOne) { // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { for (Worker w : workers) { Thread t = w.thread; // 如果线程没有中断，那么就去获取Worker的锁，基于tryLock可知，不会中断正在干活的线程 if (!t.isInterrupted() &amp;&amp; w.tryLock()) { try { // 会中断空闲线程 t.interrupt(); } catch (SecurityException ignore) { } finally { w.unlock(); } } if (onlyOne) break; } } finally { mainLock.unlock(); } } 3.4 线程池的核心参数设计规则线程池的使用难度不大，难度在于线程池的参数并不好配置。 主要难点在于任务类型无法控制，比如任务有CPU密集型，还有IO密集型，甚至还有混合型的。 因为IO咱们无法直接控制，所以很多时间按照一些书上提供的一些方法，是无法解决问题的。 《Java并发编程实践》 想调试出一个符合当前任务情况的核心参数，最好的方式就是测试。 需要将项目部署到测试环境或者是沙箱环境中，结果各种压测得到一个相对符合的参数。 如果每次修改项目都需要重新部署，成本太高了。 此时咱们可以实现一个动态监控以及修改线程池的方案。 因为线程池的核心参数无非就是： corePoolSize：核心线程数 maximumPoolSize：最大线程数 workQueue：工作队列 线程池中提供了获取核心信息的get方法，同时也提供了动态修改核心属性的set方法。 也可以采用一些开源项目提供的方式去做监控和修改 比如hippo4j就可以对线程池进行监控，而且可以和SpringBoot整合。 Github地址：https://github.com/opengoofy/hippo4j 官方文档：https://hippo4j.cn/docs/user_docs/intro 3.5 线程池处理任务的核心流程基于addWorker添加工作线程的流程切入到整体处理任务的位置 四、ScheduleThreadPoolExecutor应用&amp;源码4.1 ScheduleThreadPoolExecutor介绍从名字上就可以看出，当前线程池是用于执行定时任务的线程池。 Java比较早的定时任务工具是Timer类。但是Timer问题很多，串行的，不靠谱，会影响到其他的任务执行。 其实除了Timer以及ScheduleThreadPoolExecutor之外，正常在企业中一般会采用Quartz或者是SpringBoot提供的Schedule的方式去实现定时任务的功能。 ScheduleThreadPoolExecutor支持延迟执行以及周期性执行的功能。 4.2 ScheduleThreadPoolExecutor应用定时任务线程池的有参构造 public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory, RejectedExecutionHandler handler) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), threadFactory, handler); } 发现ScheduleThreadPoolExecutor在构建时，直接调用了父类的构造方法 ScheduleThreadPoolExecutor的父类就是ThreadPoolExecutor 首先ScheduleThreadPoolExecutor最多允许设置3个参数： 核心线程数 线程工厂 拒绝策略 首先没有设置阻塞队列，以及最大线程数和空闲时间以及单位 阻塞队列设置的是DelayedWorkQueue，其实本质就是DelayQueue，一个延迟队列。DelayQueue是一个无界队列。所以最大线程数以及非核心线程的空闲时间是不需要设置的。 代码落地使用 public static void main(String[] args) { //1. 构建定时任务线程池 ScheduledThreadPoolExecutor pool = new ScheduledThreadPoolExecutor( 5, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); return t; } }, new ThreadPoolExecutor.AbortPolicy() ); //2. 应用ScheduledThreadPoolExecutor // 跟直接执行线程池的execute没啥区别 pool.execute(() -&gt; { System.out.println(\"execute\"); }); // 指定延迟时间执行 System.out.println(System.currentTimeMillis()); pool.schedule(() -&gt; { System.out.println(\"schedule\"); System.out.println(System.currentTimeMillis()); },2, TimeUnit.SECONDS); // 指定第一次的延迟时间，并且确认后期的周期执行时间，周期时间是在任务开始时就计算 // 周期性执行就是将执行完毕的任务再次社会好延迟时间，并且重新扔到阻塞队列 // 计算的周期执行，也是在原有的时间上做累加，不关注任务的执行时长。 System.out.println(System.currentTimeMillis()); pool.scheduleAtFixedRate(() -&gt; { System.out.println(\"scheduleAtFixedRate\"); System.out.println(System.currentTimeMillis()); },2,3,TimeUnit.SECONDS); // // 指定第一次的延迟时间，并且确认后期的周期执行时间，周期时间是在任务结束后再计算下次的延迟时间 System.out.println(System.currentTimeMillis()); pool.scheduleWithFixedDelay(() -&gt; { System.out.println(\"scheduleWithFixedDelay\"); System.out.println(System.currentTimeMillis()); try { Thread.sleep(4000); } catch (InterruptedException e) { e.printStackTrace(); } },2,3,TimeUnit.SECONDS); } 4.3 ScheduleThreadPoolExecutor源码剖析4.3.1 核心属性后面的方法业务流程会涉及到这些属性。 // 这里是针对任务取消时的一些业务判断会用到的标记 private volatile boolean continueExistingPeriodicTasksAfterShutdown; private volatile boolean executeExistingDelayedTasksAfterShutdown = true; private volatile boolean removeOnCancel = false; // 计数器，如果两个任务的执行时间节点一模一样，根据这个序列来判断谁先执行 private static final AtomicLong sequencer = new AtomicLong(); // 这个方法是获取当前系统时间的毫秒值 final long now() { return System.nanoTime(); } // 内部类。核心类之一。 private class ScheduledFutureTask&lt;V&gt; extends FutureTask&lt;V&gt; implements RunnableScheduledFuture&lt;V&gt; { // 全局唯一的序列，如果两个任务时间一直，基于当前属性判断 private final long sequenceNumber; // 任务执行的时间，单位纳秒 private long time; /** * period == 0：执行一次的延迟任务 * period &gt; 0：代表是At * period &lt; 0：代表是With */ private final long period; // 周期性执行时，需要将任务重新扔回阻塞队列，基础当前属性拿到任务，方便扔回阻塞队列 RunnableScheduledFuture&lt;V&gt; outerTask = this; /** * 构建schedule方法的任务 */ ScheduledFutureTask(Runnable r, V result, long ns) { super(r, result); this.time = ns; this.period = 0; this.sequenceNumber = sequencer.getAndIncrement(); } /** * 构建At和With任务的有参构造 */ ScheduledFutureTask(Runnable r, V result, long ns, long period) { super(r, result); this.time = ns; this.period = period; this.sequenceNumber = sequencer.getAndIncrement(); } } // 内部类。核心类之一。 static class DelayedWorkQueue extends AbstractQueue&lt;Runnable&gt; implements BlockingQueue&lt;Runnable&gt; { // 这个类就是DelayQueue，不用过分关注，如果没看过，看阻塞队列中的优先级队列和延迟队列 4.3.2 schedule方法execute方法也是调用的schedule方法，只不过传入的延迟时间是0纳秒 schedule方法就是将任务和延迟时间封装到一起，并且将任务扔到阻塞队列中，再去创建工作线程去take阻塞队列。 // 延迟任务执行的方法。 // command：任务 // delay：延迟时间 // unit：延迟时间的单位 public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) { // 健壮性校验。 if (command == null || unit == null) throw new NullPointerException(); // 将任务和延迟时间封装到一起，最终组成ScheduledFutureTask // 要分成三个方法去看 // triggerTime：计算延迟时间。最终返回的是当前系统时间 + 延迟时间 // triggerTime就是将延迟时间转换为纳秒，并且+当前系统时间，再做一些健壮性校验 // ScheduledFutureTask有参构造：将任务以及延迟时间封装到一起，并且设置任务执行的方式 // decorateTask：当前方式是让用户基于自身情况可以动态修改任务的一个扩展口 RunnableScheduledFuture&lt;?&gt; t = decorateTask(command, new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(delay, unit))); // 任务封装好，执行delayedExecute方法，去执行任务 delayedExecute(t); // 返回FutureTask return t; } // triggerTime做的事情 // 外部方法，对延迟时间做校验，如果小于0，就直接设置为0 // 并且转换为纳秒单位 private long triggerTime(long delay, TimeUnit unit) { return triggerTime(unit.toNanos((delay &lt; 0) ? 0 : delay)); } // 将延迟时间+当前系统时间 // 后面的校验是为了避免延迟时间超过Long的取值范围 long triggerTime(long delay) { return now() + ((delay &lt; (Long.MAX_VALUE &gt;&gt; 1)) ? delay : overflowFree(delay)); } // ScheduledFutureTask有参构造 ScheduledFutureTask(Runnable r, V result, long ns) { super(r, result); // time就是任务要执行的时间 this.time = ns; // period,为0，代表任务是延迟执行，不是周期执行 this.period = 0; // 基于AtmoicLong生成的序列 this.sequenceNumber = sequencer.getAndIncrement(); } // delayedExecute 执行延迟任务的操作 private void delayedExecute(RunnableScheduledFuture&lt;?&gt; task) { // 查看当前线程池是否还是RUNNING状态，如果不是RUNNING，进到if if (isShutdown()) // 不是RUNNING。 // 执行拒绝策略。 reject(task); else { // 线程池状态是RUNNING // 直接让任务扔到延迟的阻塞队列中 super.getQueue().add(task); // DCL的操作，再次查看线程池状态 // 如果线程池在添加任务到阻塞队列后，状态不是RUNNING if (isShutdown() &amp;&amp; // task.isPeriodic()：现在反回的是false，因为任务是延迟执行，不是周期执行 // 默认情况，延迟队列中的延迟任务，可以执行 !canRunInCurrentRunState(task.isPeriodic()) &amp;&amp; // 从阻塞队列中移除任务。 remove(task)) task.cancel(false); else // 线程池状态正常，任务可以执行 ensurePrestart(); } } // 线程池状态不为RUNNING，查看任务是否可以执行 // 延迟执行：periodic==false // 周期执行：periodic==true // continueExistingPeriodicTasksAfterShutdown：周期执行任务，默认为false // executeExistingDelayedTasksAfterShutdown：延迟执行任务，默认为true boolean canRunInCurrentRunState(boolean periodic) { return isRunningOrShutdown(periodic ? continueExistingPeriodicTasksAfterShutdown : executeExistingDelayedTasksAfterShutdown); } // 当前情况，shutdownOK为true final boolean isRunningOrShutdown(boolean shutdownOK) { int rs = runStateOf(ctl.get()); // 如果状态是RUNNING，正常可以执行，返回true // 如果状态是SHUTDOWN，根据shutdownOK来决定 return rs == RUNNING || (rs == SHUTDOWN &amp;&amp; shutdownOK); } // 任务可以正常执行后，做的操作 void ensurePrestart() { // 拿到工作线程个数 int wc = workerCountOf(ctl.get()); // 如果工作线程个数小于核心线程数 if (wc &lt; corePoolSize) // 添加核心线程去处理阻塞队列中的任务 addWorker(null, true); else if (wc == 0) // 如果工作线程数为0，核心线程数也为0，这是添加一个非核心线程去处理阻塞队列任务 addWorker(null, false); } 4.3.3 At和With方法&amp;任务的run方法这两个方法在源码层面上的第一个区别，就是在计算周期时间时，需要将这个值传递给period，基于正负数在区别At和With 所以查看一个方法就ok，查看At方法 // At方法， // command：任务 // initialDelay：第一次执行的延迟时间 // period：任务的周期执行时间 // unit：上面两个时间的单位 public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) { // 健壮性校验 if (command == null || unit == null) throw new NullPointerException(); // 周期时间不能小于等于0. if (period &lt;= 0) throw new IllegalArgumentException(); // 将任务以及第一次的延迟时间，和后续的周期时间封装好。 ScheduledFutureTask&lt;Void&gt; sft = new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(period)); // 扩展口，可以对任务做修改。 RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); // 周期性任务，需要在任务执行完毕后，重新扔会到阻塞队列，为了方便拿任务，将任务设置到outerTask成员变量中 sft.outerTask = t; // 和schedule方法一样的方式 // 如果任务刚刚扔到阻塞队列，线程池状态变为SHUTDOWN，默认情况，当前任务不执行 delayedExecute(t); return t; } // 延迟任务以及周期任务在执行时，都会调用当前任务的run方法。 public void run() { // periodic == false：一次性延迟任务 // periodic == true：周期任务 boolean periodic = isPeriodic(); // 任务执行前，会再次判断状态，能否执行任务 if (!canRunInCurrentRunState(periodic)) cancel(false); // 判断是周期执行还是一次性任务 else if (!periodic) // 一次性任务，让工作线程直接执行command的逻辑 ScheduledFutureTask.super.run(); // 到这个else if，说明任务是周期执行 else if (ScheduledFutureTask.super.runAndReset()) { // 设置下次任务执行的时间 setNextRunTime(); // 将任务重新扔回线程池做处理 reExecutePeriodic(outerTask); } } // 设置下次任务执行的时间 private void setNextRunTime() { // 拿到period值，正数：At，负数：With long p = period; if (p &gt; 0) // 拿着之前的执行时间，直接追加上周期时间 time += p; else // 如果走到else，代表任务是With方式，这种方式要重新计算延迟时间 // 拿到当前系统时间，追加上延迟时间， time = triggerTime(-p); } // 将任务重新扔回线程池做处理 void reExecutePeriodic(RunnableScheduledFuture&lt;?&gt; task) { // 如果状态ok，可以执行 if (canRunInCurrentRunState(true)) { // 将任务扔到延迟队列 super.getQueue().add(task); // DCL，判断线程池状态 if (!canRunInCurrentRunState(true) &amp;&amp; remove(task)) task.cancel(false); else // 添加工作线程 ensurePrestart(); } } 并发集合一、ConcurrentHashMap1.1 存储结构ConcurrentHashMap是线程安全的HashMap ConcurrentHashMap在JDK1.8中是以CAS+synchronized实现的线程安全 CAS：在没有hash冲突时（Node要放在数组上时） synchronized：在出现hash冲突时（Node存放的位置已经有数据了） 存储的结构：数组+链表+红黑树 1.2 存储操作1.2.1 put方法public V put(K key, V value) { // 在调用put方法时，会调用putVal，第三个参数默认传递为false // 在调用putIfAbsent时，会调用putVal方法，第三个参数传递的为true // 如果传递为false，代表key一致时，直接覆盖数据 // 如果传递为true，代表key一致时，什么都不做，key不存在，正常添加（Redis，setnx） return putVal(key, value, false); } 1.2.2 putVal方法-散列算法final V putVal(K key, V value, boolean onlyIfAbsent) { // ConcurrentHashMap不允许key或者value出现为null的值，跟HashMap的区别 if (key == null || value == null) throw new NullPointerException(); // 根据key的hashCode计算出一个hash值，后期得出当前key-value要存储在哪个数组索引位置 int hash = spread(key.hashCode()); // 一个标识，在后面有用！ int binCount = 0; // 省略大量的代码…… } // 计算当前Node的hash值的方法 static final int spread(int h) { // 将key的hashCode值的高低16位进行^运算，最终又与HASH_BITS进行了&amp;运算 // 将高位的hash也参与到计算索引位置的运算当中 // 为什么HashMap、ConcurrentHashMap，都要求数组长度为2^n // HASH_BITS让hash值的最高位符号位肯定为0，代表当前hash值默认情况下一定是正数，因为hash值为负数时，有特殊的含义 // static final int MOVED = -1; // 代表当前hash位置的数据正在扩容！ // static final int TREEBIN = -2; // 代表当前hash位置下挂载的是一个红黑树 // static final int RESERVED = -3; // 预留当前索引位置…… return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; // 计算数组放到哪个索引位置的方法 (f = tabAt(tab, i = (n - 1) &amp; hash) // n：是数组的长度 } 00001101 00001101 00101111 10001111 - h = key.hashCode 运算方式 00000000 00000000 00000000 00001111 - 15 (n - 1) &amp; ( ( 00001101 00001101 00101111 10001111 - h ^ 00000000 00000000 00001101 00001101 - h &gt;&gt;&gt; 16 ) &amp; 01111111 11111111 11111111 11111111 - HASH_BITS ) 1.2.3 putVal方法-添加数据到数组&amp;初始化数组final V putVal(K key, V value, boolean onlyIfAbsent) { // 省略部分代码………… // 将Map的数组赋值给tab，死循环 for (Node&lt;K,V&gt;[] tab = table;;) { // 声明了一堆变量~~ // n:数组长度 // i:当前Node需要存放的索引位置 // f: 当前数组i索引位置的Node对象 // fn：当前数组i索引位置上数据的hash值 Node&lt;K,V&gt; f; int n, i, fh; // 判断当前数组是否还没有初始化 if (tab == null || (n = tab.length) == 0) // 将数组进行初始化。 tab = initTable(); // 基于 (n - 1) &amp; hash 计算出当前Node需要存放在哪个索引位置 // 基于tabAt获取到i位置的数据 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 现在数组的i位置上没有数据，基于CAS的方式将数据存在i位置上 if (casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) // 如果成功，执行break跳出循环，插入数据成功 break; } // 判断当前位置数据是否正在扩容…… else if ((fh = f.hash) == MOVED) // 让当前插入数据的线程协助扩容 tab = helpTransfer(tab, f); // 省略部分代码………… } // 省略部分代码………… } sizeCtl：是数组在初始化和扩容操作时的一个控制变量 -1：代表当前数组正在初始化 小于-1：低16位代表当前数组正在扩容的线程个数（如果1个线程扩容，值为-2，如果2个线程扩容，值为-3） 0：代表数组还没初始化 大于0：代表当前数组的扩容阈值，或者是当前数组的初始化大小 // 初始化数组方法 private final Node&lt;K,V&gt;[] initTable() { // 声明标识 Node&lt;K,V&gt;[] tab; int sc; // 再次判断数组没有初始化，并且完成tab的赋值 while ((tab = table) == null || tab.length == 0) { // 将sizeCtl赋值给sc变量，并判断是否小于0 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // 可以尝试初始化数组，线程会以CAS的方式，将sizeCtl修改为-1，代表当前线程可以初始化数组 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { // 尝试初始化！ try { // 再次判断当前数组是否已经初始化完毕。 if ((tab = table) == null || tab.length == 0) { // 开始初始化， // 如果sizeCtl &gt; 0，就初始化sizeCtl长度的数组 // 如果sizeCtl == 0，就初始化默认的长度 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组！ Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将初始化的数组nt，赋值给tab和table table = tab = nt; // sc赋值为了数组长度 - 数组长度 右移 2位 16 - 4 = 12 // 将sc赋值为下次扩容的阈值 sc = n - (n &gt;&gt;&gt; 2); } } finally { // 将赋值好的sc，设置给sizeCtl sizeCtl = sc; } break; } } return tab; } 1.2.4 putVal方法-添加数据到链表final V putVal(K key, V value, boolean onlyIfAbsent) { // 省略部分代码………… int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // n:数组长度 // i:当前Node需要存放的索引位置 // f: 当前数组i索引位置的Node对象 // fn：当前数组i索引位置上数据的hash值 // 省略部分代码………… else { // 声明变量为oldVal V oldVal = null; // 基于当前索引位置的Node，作为锁对象…… synchronized (f) { // 判断当前位置的数据还是之前的f么……（避免并发操作的安全问题） if (tabAt(tab, i) == f) { // 再次判断hash值是否大于0（不是树） if (fh &gt;= 0) { // binCount设置为1（在链表情况下，记录链表长度的一个标识） binCount = 1; // 死循环，每循环一次，对binCount for (Node&lt;K,V&gt; e = f;; ++binCount) { // 声明标识ek K ek; // 当前i索引位置的数据，是否和当前put的key的hash值一致 if (e.hash == hash &amp;&amp; // 如果当前i索引位置数据的key和put的key == 返回为true // 或者equals相等 ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { // key一致，可能需要覆盖数据！ // 当前i索引位置数据的value复制给oldVal oldVal = e.val; // 如果传入的是false，代表key一致，覆盖value // 如果传入的是true，代表key一致，什么都不做！ if (!onlyIfAbsent) // 覆盖value e.val = value; break; } // 拿到当前指定的Node对象 Node&lt;K,V&gt; pred = e; // 将e指向下一个Node对象,如果next指向的是一个null，可以挂在当前Node下面 if ((e = e.next) == null) { // 将hash，key，value封装为Node对象，挂在pred的next上 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } // 省略部分代码………… } } // binCount长度不为0 if (binCount != 0) { // binCount是否大于8（链表长度是否 &gt;= 8） if (binCount &gt;= TREEIFY_THRESHOLD) // 尝试转为红黑树或者扩容 // 基于treeifyBin方法和上面的if判断，可以得知链表想要转为红黑树，必须保证数组长度大于等于64，并且链表长度大于等于8 // 如果数组长度没有达到64的话，会首先将数组扩容 treeifyBin(tab, i); // 如果出现了数据覆盖的情况， if (oldVal != null) // 返回之前的值 return oldVal; break; } } } // 省略部分代码………… } // 为什么链表长度为8转换为红黑树，不是能其他数值嘛？ // 因为布松分布 The main disadvantage of per-bin locks is that other update * operations on other nodes in a bin list protected by the same * lock can stall, for example when user equals() or mapping * functions take a long time. However, statistically, under * random hash codes, this is not a common problem. Ideally, the * frequency of nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average, given the resizing threshold * of 0.75, although with a large variance because of resizing * granularity. Ignoring variance, the expected occurrences of * list size k are (exp(-0.5) * pow(0.5, k) / factorial(k)). The * first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million 1.3 扩容操作1.3.1 treeifyBin方法触发扩容// 在链表长度大于等于8时，尝试将链表转为红黑树 private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) { Node&lt;K,V&gt; b; int n, sc; // 数组不能为空 if (tab != null) { // 数组的长度n，是否小于64 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 如果数组长度小于64，不能将链表转为红黑树，先尝试扩容操作 tryPresize(n &lt;&lt; 1); // 省略部分代码…… } } 1.3.2 tryPreSize方法-针对putAll的初始化操作// size是将之前的数组长度 左移 1位得到的结果 private final void tryPresize(int size) { // 如果扩容的长度达到了最大值，就使用最大值 // 否则需要保证数组的长度为2的n次幂 // 这块的操作，是为了初始化操作准备的，因为调用putAll方法时，也会触发tryPresize方法 // 如果刚刚new的ConcurrentHashMap直接调用了putAll方法的话，会通过tryPresize方法进行初始化 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); // 这些代码和initTable一模一样 // 声明sc int sc; // 将sizeCtl的值赋值给sc，并判断是否大于0，这里代表没有初始化操作，也没有扩容操作 while ((sc = sizeCtl) &gt;= 0) { // 将ConcurrentHashMap的table赋值给tab，并声明数组长度n Node&lt;K,V&gt;[] tab = table; int n; // 数组是否需要初始化 if (tab == null || (n = tab.length) == 0) { // 进来执行初始化 // sc是初始化长度，初始化长度如果比计算出来的c要大的话，直接使用sc，如果没有sc大， // 说明sc无法容纳下putAll中传入的map，使用更大的数组长度 n = (sc &gt; c) ? sc : c; // 设置sizeCtl为-1，代表初始化操作 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { // 再次判断数组的引用有没有变化 if (table == tab) { // 初始化数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 数组赋值 table = nt; // 计算扩容阈值 sc = n - (n &gt;&gt;&gt; 2); } } finally { // 最终赋值给sizeCtl sizeCtl = sc; } } } // 如果计算出来的长度c如果小于等于sc，直接退出循环结束方法 // 数组长度大于等于最大长度了，直接退出循环结束方法 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; // 省略部分代码 } } // 将c这个长度设置到最近的2的n次幂的值， 15 - 16 17 - 32 // c == size + (size &gt;&gt;&gt; 1) + 1 // size = 17 00000000 00000000 00000000 00010001 + 00000000 00000000 00000000 00001000 + 00000000 00000000 00000000 00000001 // c = 26 00000000 00000000 00000000 00011010 private static final int tableSizeFor(int c) { // 00000000 00000000 00000000 00011001 int n = c - 1; // 00000000 00000000 00000000 00011001 // 00000000 00000000 00000000 00001100 // 00000000 00000000 00000000 00011101 n |= n &gt;&gt;&gt; 1; // 00000000 00000000 00000000 00011101 // 00000000 00000000 00000000 00000111 // 00000000 00000000 00000000 00011111 n |= n &gt;&gt;&gt; 2; // 00000000 00000000 00000000 00011111 // 00000000 00000000 00000000 00000001 // 00000000 00000000 00000000 00011111 n |= n &gt;&gt;&gt; 4; // 00000000 00000000 00000000 00011111 // 00000000 00000000 00000000 00000000 // 00000000 00000000 00000000 00011111 n |= n &gt;&gt;&gt; 8; // 00000000 00000000 00000000 00011111 n |= n &gt;&gt;&gt; 16; // 00000000 00000000 00000000 00100000 return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 1.3.3 tryPreSize方法-计算扩容戳并且查看BUGprivate final void tryPresize(int size) { // n：数组长度 while ((sc = sizeCtl) &gt;= 0) { // 判断当前的tab是否和table一致， else if (tab == table) { // 计算扩容表示戳，根据当前数组的长度计算一个16位的扩容戳 // 第一个作用是为了保证后面的sizeCtl赋值时，保证sizeCtl为小于-1的负数 // 第二个作用用来记录当前是从什么长度开始扩容的 int rs = resizeStamp(n); // BUG --- sc &lt; 0，永远进不去~ // 如果sc小于0，代表有线程正在扩容。 if (sc &lt; 0) { // 省略部分代码……协助扩容的代码（进不来~~~~） } // 代表没有线程正在扩容，我是第一个扩容的。 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 省略部分代码……第一个扩容的线程…… } } } // 计算扩容表示戳 // 32 = 00000000 00000000 00000000 00100000 // Integer.numberOfLeadingZeros(32) = 26 // 1 &lt;&lt; (RESIZE_STAMP_BITS - 1) // 00000000 00000000 10000000 00000000 // 00000000 00000000 00000000 00011010 // 00000000 00000000 10000000 00011010 static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1)); } 1.3.4 tryPreSize方法-对sizeCtl的修改以及条件判断的BUGprivate final void tryPresize(int size) { // sc默认为sizeCtl while ((sc = sizeCtl) &gt;= 0) { else if (tab == table) { // rs：扩容戳 00000000 00000000 10000000 00011010 int rs = resizeStamp(n); if (sc &lt; 0) { // 说明有线程正在扩容，过来帮助扩容 Node&lt;K,V&gt;[] nt; // 依然有BUG // 当前线程扩容时，老数组长度是否和我当前线程扩容时的老数组长度一致 // 00000000 00000000 10000000 00011010 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // 10000000 00011010 00000000 00000010 // 00000000 00000000 10000000 00011010 // 这两个判断都是有问题的，核心问题就应该先将rs左移16位，再追加当前值。 // 这两个判断是BUG // 判断当前扩容是否已经即将结束 || sc == rs + 1 // sc == rs &lt;&lt; 16 + 1 BUG // 判断当前扩容的线程是否达到了最大限度 || sc == rs + MAX_RESIZERS // sc == rs &lt;&lt; 16 + MAX_RESIZERS BUG // 扩容已经结束了。 || (nt = nextTable) == null // 记录迁移的索引位置，从高位往低位迁移，也代表扩容即将结束。 || transferIndex &lt;= 0) break; // 如果线程需要协助扩容，首先就是对sizeCtl进行+1操作，代表当前要进来一个线程协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) // 上面的判断没进去的话，nt就代表新数组 transfer(tab, nt); } // 是第一个来扩容的线程 // 基于CAS将sizeCtl修改为 10000000 00011010 00000000 00000010 // 将扩容戳左移16位之后，符号位是1，就代码这个值为负数 // 低16位在表示当前正在扩容的线程有多少个, // 为什么低位值为2时，代表有一个线程正在扩容 // 每一个线程扩容完毕后，会对低16位进行-1操作，当最后一个线程扩容完毕后，减1的结果还是-1， // 当值为-1时，要对老数组进行一波扫描，查看是否有遗漏的数据没有迁移到新数组 else if (U.compareAndSwapInt(this, SIZECTL, sc,(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 调用transfer方法，并且将第二个参数设置为null，就代表是第一次来扩容！ transfer(tab, null); } } } 1.3.5 transfer方法-计算每个线程迁移的长度// 开始扩容 tab=oldTable private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { // n = 数组长度 // stride = 每个线程一次性迁移多少数据到新数组 int n = tab.length, stride; // 基于CPU的内核数量来计算，每个线程一次性迁移多少长度的数据最合理 // NCPU = 4 // 举个栗子：数组长度为1024 - 512 - 256 - 128 / 4 = 32 // MIN_TRANSFER_STRIDE = 16,为每个线程迁移数据的最小长度 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // 根据CPU计算每个线程一次迁移多长的数据到新数组，如果结果大于16，使用计算结果。 如果结果小于16，就使用最小长度16 } 1.3.6 transfer方法-构建新数组并查看标识属性// 以32长度数组扩容到64位例子 private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { // n = 老数组长度 32 // stride = 步长 16 // 第一个进来扩容的线程需要把新数组构建出来 if (nextTab == null) { try { // 将原数组长度左移一位，构建新数组长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; // 赋值操作 nextTab = nt; } catch (Throwable ex) { // 到这说明已经达到数组长度的最大取值范围 sizeCtl = Integer.MAX_VALUE; // 设置sizeCtl后直接结束 return; } // 将成员变量的新数组赋值 nextTable = nextTab; // 迁移数据时，用到的标识，默认值为老数组长度 transferIndex = n; // 32 } // 新数组长度 int nextn = nextTab.length; // 64 // 在老数组迁移完数据后，做的标识 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 迁移数据时，需要用到的标识 boolean advance = true; boolean finishing = false; // 省略部分代码 } 1.3.7 transfer方法-线程领取迁移任务 // 以32长度扩容到64位为例子 private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { // n：32 // stride：16 int n = tab.length, stride; if (nextTab == null) { // 省略部分代码………… // nextTable：新数组 nextTable = nextTab; // transferIndex：0 transferIndex = n; } // nextn：64 int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // advance：true，代表当前线程需要接收任务，然后再执行迁移， 如果为false，代表已经接收完任务 boolean advance = true; // finishing：false，是否迁移结束！ boolean finishing = false; // 循环…… // i = 15 代表当前线程迁移数据的索引值！！ // bound = 0 for (int i = 0, bound = 0;;) { // f = null // fh = 0 Node&lt;K,V&gt; f; int fh; // 当前线程要接收任务 while (advance) { // nextIndex = 16 // nextBound = 16 int nextIndex, nextBound; // 第一次进来，这两个判断肯定进不去。 // 对i进行--，并且判断当前任务是否处理完毕！ if (--i &gt;= bound || finishing) advance = false; // 判断transferIndex是否小于等于0，代表没有任务可领取，结束了。 // 在线程领取任务会，会对transferIndex进行修改，修改为transferIndex - stride // 在任务都领取完之后，transferIndex肯定是小于等于0的，代表没有迁移数据的任务可以领取 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } // 当前线程尝试领取任务 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // 对bound赋值 bound = nextBound; // 对i赋值 i = nextIndex - 1; // 设置advance设置为false，代表当前线程领取到任务了。 advance = false; } } // 开始迁移数据，并且在迁移完毕后，会将advance设置为true } } 1.3.8 transfer方法-迁移结束操作// 以32长度扩容到64位为例子 private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { for (int i = 0, bound = 0;;) { while (advance) { // 判断扩容是否已经结束！ // i &lt; 0：当前线程没有接收到任务！ // i &gt;= n: 迁移的索引位置，不可能大于数组的长度，不会成立 // i + n &gt;= nextn：因为i最大值就是数组索引的最大值，不会成立 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { // 如果进来，代表当前线程没有接收到任务 int sc; // finishing为true，代表扩容结束 if (finishing) { // 将nextTable新数组设置为null nextTable = null; // 将当前数组的引用指向了新数组~ table = nextTab; // 重新计算扩容阈值 64 - 16 = 48 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 结束扩容 return; } // 当前线程没有接收到任务，让当前线程结束扩容操作。 // 采用CAS的方式，将sizeCtl - 1，代表当前并发扩容的线程数 - 1 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // sizeCtl的高16位是基于数组长度计算的扩容戳，低16位是当前正在扩容的线程个数 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) // 代表当前线程并不是最后一个退出扩容的线程，直接结束当前线程扩容 return; // 如果是最后一个退出扩容的线程，将finishing和advance设置为true finishing = advance = true; // 将i设置为老数组长度，让最后一个线程再从尾到头再次检查一下，是否数据全部迁移完毕。 i = n; } } // 开始迁移数据，并且在迁移完毕后，会将advance设置为true } } 1.3.9 transfer方法-迁移数据（链表）// 以32长度扩容到64位为例子 private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { // 省略部分代码………… for (int i = 0, bound = 0;;) { // 省略部分代码………… if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { // 省略部分代码………… } // 开始迁移数据，并且在迁移完毕后，会将advance设置为true // 获取指定i位置的Node对象，并且判断是否为null else if ((f = tabAt(tab, i)) == null) // 当前桶位置没有数据，无需迁移，直接将当前桶位置设置为fwd advance = casTabAt(tab, i, null, fwd); // 拿到当前i位置的hash值，如果为MOVED，证明数据已经迁移过了。 else if ((fh = f.hash) == MOVED) // 一般是给最后扫描时，使用的判断，如果迁移完毕，直接跳过当前位置。 advance = true; // already processed else { // 当前桶位置有数据，先锁住当前桶位置。 synchronized (f) { // 判断之前取出的数据是否为当前的数据。 if (tabAt(tab, i) == f) { // ln：null - lowNode // hn：null - highNode Node&lt;K,V&gt; ln, hn; // hash大于0，代表当前Node属于正常情况，不是红黑树，使用链表方式迁移数据 if (fh &gt;= 0) { // lastRun机制 // 000000000010000 // 这种运算结果只有两种，要么是0，要么是n int runBit = fh &amp; n; // 将f赋值给lastRun Node&lt;K,V&gt; lastRun = f; // 循环的目的就是为了得到链表下经过hash &amp; n结算，结果一致的最后一些数据 // 在迁移数据时，值需要迁移到lastRun即可，剩下的指针不需要变换。 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } // runBit == 0，赋值给ln if (runBit == 0) { ln = lastRun; hn = null; } // rubBit == n,赋值给hn else { hn = lastRun; ln = null; } // 循环到lastRun指向的数据即可，后续不需要再遍历 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { // 获取当前Node的hash值，key值，value值。 int ph = p.hash; K pk = p.key; V pv = p.val; // 如果hash&amp;n为0，挂到lowNode上 if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); // 如果hash&amp;n为n，挂到highNode上 else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } // 采用CAS的方式，将ln挂到新数组的原位置 setTabAt(nextTab, i, ln); // 采用CAS的方式，将hn挂到新数组的原位置 + 老数组长度 setTabAt(nextTab, i + n, hn); // 采用CAS的方式，将当前桶位置设置为fwd setTabAt(tab, i, fwd); // advance设置为true，保证可以进入到while循环，对i进行--操作 advance = true; } // 省略迁移红黑树的操作 } } } } } 1.3.10 helpTransfer方法-协助扩容// 在添加数据时，如果插入节点的位置的数据，hash值为-1，代表当前索引位置数据已经被迁移到了新数组 // tab：老数组 // f：数组上的Node节点 final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) { // nextTab：新数组 // sc：给sizeCtl做临时变量 Node&lt;K,V&gt;[] nextTab; int sc; // 第一个判断：老数组不为null // 第二个判断：新数组不为null （将新数组赋值给nextTab） if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) { // ConcurrentHashMap正在扩容 // 基于老数组长度计算扩容戳 int rs = resizeStamp(tab.length); // 第一个判断：fwd中的新数组，和当前正在扩容的新数组是否相等。 相等：可以协助扩容。不相等：要么扩容结束，要么开启了新的扩容 // 第二个判断：老数组是否改变了。 相等：可以协助扩容。不相等：扩容结束了 // 第三个判断：如果正在扩容，sizeCtl肯定为负数，并且给sc赋值 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) { // 第一个判断：将sc右移16位，判断是否与扩容戳一致。 如果不一致，说明扩容长度不一样，退出协助扩容 // 第二个、三个判断是BUG： /* sc == rs &lt;&lt; 16 + 1 || 如果+1和当前sc一致，说明扩容已经到了最后检查的阶段 sc == rs &lt;&lt; 16 + MAX_RESIZERS || 判断协助扩容的线程是否已经达到了最大值 */ // 第四个判断：transferIndex是从高索引位置到低索引位置领取数据的一个核心属性，如果满足 小于等于0，说明任务被领光了。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) // 不需要协助扩容 break; // 将sizeCtl + 1，进来协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { // 协助扩容 transfer(tab, nextTab); break; } } return nextTab; } return table; } 1.4 红黑树操作在前面搞定了关于数据+链表的添加和扩容操作，现在要搞定红黑树。因为红黑树的操作有点乱，先对红黑树结构有一定了解。 1.4.1 什么是红黑树红黑树是一种特殊的平衡二叉树，首选具备了平衡二叉树的特点：左子树和右子数的高度差不会超过1，如果超过了，平衡二叉树就会基于左旋和右旋的操作，实现自平衡。 红黑树在保证自平衡的前提下，还保证了自己的几个特性： 每个节点必须是红色或者黑色。 根节点必须是黑色。 如果当前节点是红色，子节点必须是黑色 所有叶子节点都是黑色。 从任意节点到每个叶子节点的路径中，黑色节点的数量是相同的。 当对红黑树进行增删操作时，可能会破坏平衡或者是特性，这是红黑树就需要基于左旋、右旋、变色来保证平衡和特性。 左旋操作： 右旋操作： 变色操作：节点的颜色从黑色变为红色，或者从红色变为黑色，就成为变色。变色操作是在增删数据之后，可能出现的操作。插入数据时，插入节点的颜色一般是红色，因为插入红色节点的破坏红黑树结构的可能性比较低的。如果破坏了红黑树特性，会通过变色来调整 红黑树相对比较复杂，完整的红黑树代码400~500行内容，没有必要全部记下来，或者首先红黑树。 如果向细粒度掌握红黑树的结构：https://www.mashibing.com/subject/21?courseNo=339 1.4.2 TreeifyBin方法-封装TreeNode和双向链表// 将链表转为红黑树的准备操作 private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) { // b：当前索引位置的Node Node&lt;K,V&gt; b; int sc; if (tab != null) { // 省略部分代码 // 开启链表转红黑树操作 // 当前桶内有数据，并且是链表结构 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) { // 加锁，保证线程安全 synchronized (b) { // 再次判断数据是否有变化，DCL if (tabAt(tab, index) == b) { // 开启准备操作，将之前的链表中的每一个Node，封装为TreeNode，作为双向链表 // hd：是整个双向链表的第一个节点。 // tl：是单向链表转换双向链表的临时存储变量 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) { TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } // hd就是整个双向链表 // TreeBin的有参构建，将双向链表转为了红黑树。 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); } } } } } 1.4.3 TreeBin有参构造-双向链表转为红黑树TreeBin中不但保存了红黑树结构，同时还保存在一套双向链表 // 将双向链表转为红黑树的操作。 b：双向链表的第一个节点 // TreeBin继承自Node，root：代表树的根节点，first：双向链表的头节点 TreeBin(TreeNode&lt;K,V&gt; b) { // 构建Node，并且将hash值设置为-2 super(TREEBIN, null, null, null); // 将双向链表的头节点赋值给first this.first = b; // 声明r的TreeNode，最后会被赋值为根节点 TreeNode&lt;K,V&gt; r = null; // 遍历之前封装好的双向链表 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) { next = (TreeNode&lt;K,V&gt;)x.next; // 先将左右子节点清空 x.left = x.right = null; // 如果根节点为null，第一次循环 if (r == null) { // 将第一个节点设置为当前红黑树的根节点 x.parent = null; // 根节点没父节点 x.red = false; // 不是红色，是黑色 r = x; // 将当前节点设置为r } // 已经有根节点，当前插入的节点要作为父节点的左子树或者右子树 else { // 拿到了当前节点key和hash值。 K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // 循环？ for (TreeNode&lt;K,V&gt; p = r;;) { // dir：如果为-1，代表要插入到父节点的左边，如果为1，代表要插入的父节点的右边 // ph：是父节点的hash值 int dir, ph; // pk：是父节点的key K pk = p.key; // 父节点的hash值，大于当前节点的hash值，就设置为-1，代表要插入到父节点的左边 if ((ph = p.hash) &gt; h) dir = -1; // 父节点的hash值，小于当前节点的hash值，就设置为1，代表要插入到父节点的右边 else if (ph &lt; h) dir = 1; // 父节点的hash值和当前节点hash值一致，基于compare方式判断到底放在左子树还是右子树 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // 拿到当前父节点。 TreeNode&lt;K,V&gt; xp = p; // 将p指向p的left、right，并且判断是否为null // 如果为null，代表可以插入到这位置。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) { // 进来就说明找到要存放当前节点的位置了 // 将当前节点的parent指向父节点 x.parent = xp; // 根据dir的值，将父节点的left、right指向当前节点 if (dir &lt;= 0) xp.left = x; else xp.right = x; // 插入一个节点后，做一波平衡操作 r = balanceInsertion(r, x); break; } } } } // 将根节点复制给root this.root = r; // 检查红黑树结构 assert checkInvariants(root); } 1.4.4 balanceInsertion方法-保证红黑树平衡以及特性// 红黑树的插入动画：https://www.cs.usfca.edu/~galles/visualization/RedBlack.html // 红黑树做自平衡以及保证特性的操作。 root：根节点， x：当前节点 static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) { // 先将节点置位红色 x.red = true; // xp：父节点 // xpp：爷爷节点 // xppl：爷爷节点的左子树 // xxpr：爷爷节点的右子树 for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) { // 拿到父节点，并且父节点为红 if ((xp = x.parent) == null) { // 当前节点为根节点，置位黑色 x.red = false; return x; } // 父节点不是红色，爷爷节点为null else if (!xp.red || (xpp = xp.parent) == null) // 什么都不做，直接返回 return root; // ===================================== // 左子树的操作 if (xp == (xppl = xpp.left)) { // 通过变色满足红黑树特性 if ((xppr = xpp.right) != null &amp;&amp; xppr.red) { // 叔叔节点和父节点变为黑色 xppr.red = false; xp.red = false; // 爷爷节点置位红色 xpp.red = true; // 让爷爷节点作为当前节点，再走一次循环 x = xpp; } else { // 如果当前节点是右子树，通过父节点的左旋，变为左子树的结构 if (x == xp.right) {、 // 父节点做左旋操作 root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { // 父节点变为黑色 xp.red = false; if (xpp != null) { // 爷爷节点变为红色 xpp.red = true; // 爷爷节点做右旋操作 root = rotateRight(root, xpp); } } } } // 右子树（只讲左子树就足够了，因为业务都是一样的） else { if (xppl != null &amp;&amp; xppl.red) { xppl.red = false; xp.red = false; xpp.red = true; x = xpp; } else { if (x == xp.left) { root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { xp.red = false; if (xpp != null) { xpp.red = true; root = rotateLeft(root, xpp); } } } } } } 1.4.5 putTreeVal方法-添加节点整体操作就是判断当前节点要插入到左子树，还是右子数，还是覆盖操作。 确定左子树和右子数之后，直接维护双向链表和红黑树结构，并且再判断是否需要自平衡。 TreeBin的双向链表用的头插法。 // 添加节点到红黑树内部 final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) { // Class对象 Class&lt;?&gt; kc = null; // 搜索节点 boolean searched = false; // 死循环，p节点是根节点的临时引用 for (TreeNode&lt;K,V&gt; p = root;;) { // dir：确定节点是插入到左子树还是右子数 // ph：父节点的hash值 // pk：父节点的key int dir, ph; K pk; // 根节点是否为诶null，把当前节点置位根节点 if (p == null) { first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; } // 判断当前节点要放在左子树还是右子数 else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; // 如果key一致，直接返回p，由putVal去修改数据 else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; // hash值一致，但是key的==和equals都不一样，基于Compare去判断 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || // 基于compare判断也是一致，就进到if判断 (dir = compareComparables(kc, k, pk)) == 0) { // 开启搜索，查看是否有相同的key，只有第一次循环会执行。 if (!searched) { TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) // 如果找到直接返回 return q; } // 再次判断hash大小，如果小于等于，返回-1 dir = tieBreakOrder(k, pk); } // xp是父节点的临时引用 TreeNode&lt;K,V&gt; xp = p; // 基于dir判断是插入左子树还有右子数，并且给p重新赋值 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) { // first引用拿到 TreeNode&lt;K,V&gt; x, f = first; // 将当前节点构建出来 first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); // 因为当前的TreeBin除了红黑树还维护这一个双向链表，维护双向链表的操作 if (f != null) f.prev = x; // 维护红黑树操作 if (dir &lt;= 0) xp.left = x; else xp.right = x; // 如果如节点是黑色的，当前节点红色即可，说明现在插入的节点没有影响红黑树的平衡 if (!xp.red) x.red = true; else { // 说明插入的节点是黑色的 // 加锁操作 lockRoot(); try { // 自平衡一波。 root = balanceInsertion(root, x); } finally { // 释放锁操作 unlockRoot(); } } break; } } // 检查一波红黑树结构 assert checkInvariants(root); // 代表插入了新节点 return null; } 1.4.6 TreeBin的锁操作TreeBin的锁操作，没有基于AQS，仅仅是对一个变量的CAS操作和一些业务判断实现的。 每次读线程操作，对lockState+4。 写线程操作，对lockState + 1，如果读操作占用着线程，就先+2，waiter是当前线程，并挂起当前线程 // TreeBin的锁操作 // 如果说有读线程在读取红黑树的数据，这时，写线程要阻塞（做平衡前） // 如果有写线程正在操作红黑树（做平衡），读线程不会阻塞，会读取双向链表 // 读读不会阻塞！ static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; { // waiter：等待获取写锁的线程 volatile Thread waiter; // lockState：当前TreeBin的锁状态 volatile int lockState; // 对锁状态进行运算的值 // 有线程拿着写锁 static final int WRITER = 1; // 有写线程，再等待获取写锁 static final int WAITER = 2; // 读线程，在红黑树中检索时，需要先对lockState + READER // 这个只会在读操作中遇到 static final int READER = 4; // 加锁-写锁 private final void lockRoot() { // 将lockState从0设置为1，代表拿到写锁成功 if (!U.compareAndSwapInt(this, LOCKSTATE, 0, WRITER)) // 如果写锁没拿到，执行contendedLock contendedLock(); } // 释放写锁 private final void unlockRoot() { lockState = 0; } // 写线程没有拿到写锁，执行当前方法 private final void contendedLock() { // 是否有线程正在等待 boolean waiting = false; // 死循环，s是lockState的临时变量 for (int s;;) { // // lockState &amp; 11111101 ,只要结果为0，说明当前写锁，和读锁都没线程获取 if (((s = lockState) &amp; ~WAITER) == 0) { // CAS一波，尝试将lockState再次修改为1， if (U.compareAndSwapInt(this, LOCKSTATE, s, WRITER)) { // 成功拿到锁资源，并判断是否在waiting if (waiting) // 如果当前线程挂起过，直接将之前等待的线程资源设置为null waiter = null; return; } } // 有读操作在占用资源 // lockState &amp; 00000010,代表当前没有写操作挂起等待。 else if ((s &amp; WAITER) == 0) { // 基于CAS，将LOCKSTATE的第二位设置为1 if (U.compareAndSwapInt(this, LOCKSTATE, s, s | WAITER)) { // 如果成功，代表当前线程可以waiting等待了 waiting = true; waiter = Thread.currentThread(); } } else if (waiting) // 挂起当前线程！会由写操作唤醒 LockSupport.park(this); } } } 1.4.7 transfer迁移数据首先红黑结构的数据迁移是基于双向链表封装的数据。 如果高低位的长度小于等于6，封装为链表迁移到新数组 如果高低位的长度大于6，依然封装为红黑树迁移到新数组 // 红黑树的迁移操作单独拿出来，TreeBin中不但有红黑树，还有双向链表，迁移的过程是基于双向链表迁移 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; // lo，hi扩容后要放到新数组的高低位的链表 TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; // lc，hc在记录高低位数据的长度 int lc = 0, hc = 0; // 遍历TreeBin中的双向链表 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(h, e.key, e.val, null, null); // 与老数组长度做&amp;运算，基于结果确定需要存放到低位还是高位 if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; // 低位长度++ ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; // 高位长度++ ++hc; } } // 封装低位节点，如果低位节点的长度小于等于6，转回成链表。 如果长度大于6，需要重新封装红黑树 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; // 封装高位节点 hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; // 低位数据设置到新数组 setTabAt(nextTab, i, ln); // 高位数据设置到新数组 setTabAt(nextTab, i + n, hn); // 当前位置数据迁移完毕，设置上fwd setTabAt(tab, i, fwd); // 开启前一个节点的数据迁移 advance = true; 1.5 查询数据1.5.1 get方法-查询数据的入口在查询数据时，会先判断当前key对应的value，是否在数组上。 其次会判断当前位置是否属于特殊情况：数据被迁移、位置被占用、红黑树结构 最后判断链表上是否有对应的数据。 找到返回指定的value，找不到返回null即可 // 基于key查询value public V get(Object key) { // tab：数组， e：查询指定位置的节点 n：数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 基于传入的key，计算hash值 int h = spread(key.hashCode()); // 数组不为null，数组上得有数据，拿到指定位置的数组上的数据 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { // 数组上数据恩地hash值，是否和查询条件key的hash一样 if ((eh = e.hash) == h) { // key的==或者equals是否一致，如果一致，数组上就是要查询的数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } // 如果数组上的数据的hash为负数，有特殊情况， else if (eh &lt; 0) // 三种情况，数据迁移走了，节点位置被占，红黑树 return (p = e.find(h, key)) != null ? p.val : null; // 肯定走链表操作 while ((e = e.next) != null) { // 如果hash值一致，并且key的==或者equals一致，返回当前链表位置的数据 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } // 如果上述三个流程都没有知道指定key对应的value，那就是key不存在，返回null即可 return null; } 1.5.2 ForwardingNode的find方法在查询数据时，如果发现已经扩容了，去新数组上查询数据 在数组和链表上正常找key对应的value 可能依然存在特殊情况： 再次是fwd，说明当前线程可能没有获取到CPU时间片，导致CHM再次触发扩容，重新走当前方法 可能是被占用或者是红黑树，再次走另外两种find方法的逻辑 // 在查询数据时，发现当前桶位置已经放置了fwd，代表已经被迁移到了新数组 Node&lt;K,V&gt; find(int h, Object k) { // key：get(key) h：key的hash tab：新数组 outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) { // n：新数组长度， e：新数组上定位的位置上的数组 Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; // 开始在新数组中走逻辑 for (;;) { // eh：新数组位置的数据的hash int eh; K ek; // 判断hash是否一致，如果一致，再判断==或者equals。 if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) // 在新数组找到了数据 return e; // 发现到了新数组，hash值又小于0 if (eh &lt; 0) { // 套娃，发现刚刚在扩容，到了新数组，发现又扩容 if (e instanceof ForwardingNode) { // 再次重新走最外层循环，拿到最新的nextTable tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; } else // 占了，红黑树 return e.find(h, k); } // 说明不在数组上，往下走链表 if ((e = e.next) == null) // 进来说明链表没找到，返回null return null; } } } 1.5.3 ReservationNode的find方法没什么说的，直接返回null 因为当前桶位置被占用的话，说明数据还没放到当前位置，当前位置可以理解为就是null Node&lt;K,V&gt; find(int h, Object k) { return null; } 1.5.4 TreeBin的find方法在红黑树中执行find方法后，会有两个情况 如果有线程在持有写锁或者等待获取写锁，当前查询就要在双向链表中锁检索 如果没有线程持有写锁或者等待获取写锁，完全可以对lockState + 4，然后去红黑树中检索，并且在检索完毕后，需要对lockState - 4，再判断是否需要唤醒等待写锁的线程 // 在红黑树中检索数据 final Node&lt;K,V&gt; find(int h, Object k) { // 非空判断 if (k != null) { // e：Treebin中的双向链表， for (Node&lt;K,V&gt; e = first; e != null; ) { int s; K ek; // s：TreeBin的锁状态 // 00000010 // 00000001 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) { // 如果进来if，说明要么有写线程在等待获取写锁，要么是由写线程持有者写锁 // 如果出现这个情况，他会去双向链表查询数据 if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; } // 说明没有线程等待写锁或者持有写锁，将lockState + 4，代表当前读线程可以去红黑树中检索数据 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) { TreeNode&lt;K,V&gt; r, p; try { // 基于findTreeNode在红黑树中检索数据 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); } finally { Thread w; // 会对lockState - 4，读线程拿到数据了，释放读锁 // 可以确认，如果-完4，等于WAITER，说明有写线程可能在等待，判断waiter是否为null if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) // 当前我是最后一个在红黑树中检索的线程，同时有线程在等待持有写锁，唤醒等待的写线程 LockSupport.unpark(w); } return p; } } } return null; } 1.5.6 TreeNode的findTreeNode方法红黑树的检索方式，套路很简单，及时基于hash值，来决定去找左子树还有右子数。 如果hash值一致，判断是否 == 、equals，满足就说明找到数据 如果hash值一致，并不是找的数据，基于compare方式，再次决定找左子树还是右子数，知道找到当前节点的子节点为null，停住。 // 红黑树中的检索方法 final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) { if (k != null) { TreeNode&lt;K,V&gt; p = this; do { int ph, dir; K pk; TreeNode&lt;K,V&gt; q; // 声明左子树和右子数 TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; // 直接比较hash值，来决决定走左子树还是右子数 if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; // 判断当前的子树是否和查询的k == 或者equals，直接返回 else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; // 递归继续往底层找 else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; } while (p != null); } return null; } 1.6 ConcurrentHashMap其他方法1.6.1 compute方法修改ConcurrentHashMap中指定key的value时，一般会选择先get出来，然后再拿到原value值，基于原value值做一些修改，最后再存放到咱们ConcurrentHashMap public static void main(String[] args) { ConcurrentHashMap&lt;String,Integer&gt; map = new ConcurrentHashMap(); map.put(\"key\",1); // 修改key对应的value，追加上1 // 之前的操作方式 Integer oldValue = (Integer) map.get(\"key\"); Integer newValue = oldValue + 1; map.put(\"key\",newValue); System.out.println(map); // 现在的操作方式 map.compute(\"key\",(key,computeOldValue) -&gt; { if(computeOldValue == null){ computeOldValue = 0; } return computeOldValue + 1; }); System.out.println(map); } 1.6.2 compute方法源码分析整个流程和putVal方法很类似，但是内部涉及到了占位的情况RESERVED 整个compute方法和putVal的区别就是，compute方法的value需要计算，如果key存在，基于oldValue计算出新结果，如果key不存在，直接基于oldValue为null的情况，去计算新的value。 // compute 方法 public V compute(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) { if (key == null || remappingFunction == null) throw new NullPointerException(); // 计算key的hash int h = spread(key.hashCode()); V val = null; int delta = 0; int binCount = 0; // 初始化，桶上赋值，链表插入值，红黑树插入值 for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 桶上赋值 else if ((f = tabAt(tab, i = (n - 1) &amp; h)) == null) { // 数组指定的索引位置是没有数据，当前数据必然要放到数组上。 // 因为value需要计算得到，计算的时间不可估计，所以这里并没有通过CAS的方式处理并发操作，直接添加临时占用节点， // 并占用当前临时节点的锁资源。 Node&lt;K,V&gt; r = new ReservationNode&lt;K,V&gt;(); synchronized (r) { // 以CAS的方式将数据放上去 if (casTabAt(tab, i, null, r)) { binCount = 1; Node&lt;K,V&gt; node = null; try { // 如果ReservationNode临时Node存放成功，直接开始计算value if ((val = remappingFunction.apply(key, null)) != null) { delta = 1; // 将计算的value和传入的key封装成一个新Node，通过CAS存储到当前数组上 node = new Node&lt;K,V&gt;(h, key, val, null); } } finally { setTabAt(tab, i, node); } } } if (binCount != 0) break; } else { // 省略部分代码。主要是针对在链表上的替换、添加，以及在红黑树上的替换、添加 } } if (delta != 0) addCount((long)delta, binCount); return val; } 1.6.3 computeIfPresent、computeIfAbsent、compute区别compute的BUG，如果在计算结果的函数中，又涉及到了当前的key，会造成死锁问题。 public static void main(String[] args) { ConcurrentHashMap&lt;String,Integer&gt; map = new ConcurrentHashMap(); map.compute(\"key\",(k,v) -&gt; { return map.compute(\"key\",(key,value) -&gt; { return 1111; }); }); System.out.println(map); } computeIfPresent和computeIfAbsent其实就是将compute方法拆开成了两个方法 compute会在key不存在时，正常存放结果，如果key存在，就基于oldValue计算newValue computeIfPresent：要求key在map中必须存在，需要基于oldValue计算newValue computeIfAbsent：要求key在map中不能存在，必须为null，才会基于函数得到value存储进去 computeIfPresent： // 如果key存在，才执行修改操作 public V computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) { for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); // 如果key不存在，什么事都不做~ else if ((f = tabAt(tab, i = (n - 1) &amp; h)) == null) break; else { synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f, pred = null;; ++binCount) { K ek; // 如果查看到有 == 或者equals的key，就直接修改即可 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { val = remappingFunction.apply(key, e.val); if (val != null) e.val = val; else { delta = -1; Node&lt;K,V&gt; en = e.next; if (pred != null) pred.next = en; else setTabAt(tab, i, en); } break; } pred = e; // 走完链表，还是没找到指定数据，直接break; if ((e = e.next) == null) break; } } // 省略部分代码 return val; } computeIfAbsent核心位置源码： // key必须不存在才会执行添加操作 public V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) { for (Node&lt;K,V&gt;[] tab = table;;) { else if ((f = tabAt(tab, i = (n - 1) &amp; h)) == null) { // 如果key不存在，正常添加; Node&lt;K,V&gt; r = new ReservationNode&lt;K,V&gt;(); synchronized (r) { if (casTabAt(tab, i, null, r)) { binCount = 1; Node&lt;K,V&gt; node = null; try { if ((val = mappingFunction.apply(key)) != null) node = new Node&lt;K,V&gt;(h, key, val, null); } finally { setTabAt(tab, i, node); } } } } else { boolean added = false; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; V ev; // 如果key存在，直接break; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { val = e.val; break; } // 如果没有找到一样的key，计算value结果接口 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { if ((val = mappingFunction.apply(key)) != null) { added = true; pred.next = new Node&lt;K,V&gt;(h, key, val, null); } break; } } } // 省略部分代码 return val; } 1.6.4 replace方法详解涉及到类似CAS的操作，需要将ConcurrentHashMap的value从val1改为val2的场景就可以使用replace实现。 replace内部要求key必须存在，替换value值之前，要先比较oldValue，只有oldValue一致时，才会完成替换操作。 // replace方法调用的replaceNode方法， value：newValue， cv：oldValue final V replaceNode(Object key, V value, Object cv) { int hash = spread(key.hashCode()); for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 在数组没有初始化时，或者key不存在时，什么都不干。 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { validated = true; for (Node&lt;K,V&gt; e = f, pred = null;;) { K ek; // 找到key一致的Node了。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { // 拿到当前节点的原值。 V ev = e.val; // 拿oldValue和原值做比较，如果一致， if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) { // 可以开始替换 oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } else if (f instanceof TreeBin) { validated = true; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r, p; if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } } if (validated) { if (oldVal != null) { if (value == null) addCount(-1L, -1); return oldVal; } break; } } } return null; } 1.6.5 merge方法详解merge(key,value,Function&lt;oldValue,value&gt;); 在使用merge时，有三种情况可能发生： 如果key不存在，就跟put(key,value); 如果key存在，就可以基于Function计算，得到最终结果 结果不为null，将key对应的value，替换为Function的结果 结果为null，删除当前key 分析merge源码 public V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) { if (key == null || value == null || remappingFunction == null) throw new NullPointerException(); int h = spread(key.hashCode()); V val = null; int delta = 0; int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); // key不存在，直接执行正常的添加操作，将value作为值，添加到hashMap else if ((f = tabAt(tab, i = (n - 1) &amp; h)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(h, key, value, null))) { delta = 1; val = value; break; } } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f, pred = null;; ++binCount) { K ek; // 判断链表中，有当前的key if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { // 基于函数，计算value val = remappingFunction.apply(e.val, value); // 如果计算的value不为null，正常替换 if (val != null) e.val = val; // 计算的value是null，直接让上一个指针指向我的next，绕过当前节点 else { delta = -1; Node&lt;K,V&gt; en = e.next; if (pred != null) pred.next = en; else setTabAt(tab, i, en); } break; } pred = e; if ((e = e.next) == null) { delta = 1; val = value; pred.next = new Node&lt;K,V&gt;(h, key, val, null); break; } } } else if (f instanceof TreeBin) { binCount = 2; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r = t.root; TreeNode&lt;K,V&gt; p = (r == null) ? null : r.findTreeNode(h, key, null); val = (p == null) ? value : remappingFunction.apply(p.val, value); if (val != null) { if (p != null) p.val = val; else { delta = 1; t.putTreeVal(h, key, val); } } else if (p != null) { delta = -1; if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); break; } } } if (delta != 0) addCount((long)delta, binCount); return val; } 1.7 ConcurrentHashMap计数器1.7.1 addCount方法分析addCount方法本身就是为了记录ConcurrentHashMap中元素的个数。 两个方向组成： 计数器，如果添加元素成功，对计数器 + 1 检验当前ConcurrentHashMap是否需要扩容 计数器选择的不是AtomicLong，而是类似LongAdder的一个功能 addCount源码分析 private final void addCount(long x, int check) { // ================================计数===================================== // as： CounterCell[] // s：是自增后的元素个数 // b：原来的baseCount CounterCell[] as; long b, s; // 判断CounterCell不为null，代表之前有冲突问题，有冲突直接进到if中 // 如果CounterCell[]为null，直接执行||后面的CAS操作，直接修改baseCount if ((as = counterCells) != null || // 如果对baseCount++成功。直接告辞。 如果CAS失败，直接进到if中 !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) { // 导致，说明有并发问题。 // 进来的方式有两种： // 1. counterCell[] 有值。 // 2. counterCell[] 无值，但是CAS失败。 // m：数组长度 - 1 // a：当前线程基于随机数，获得到的数组上的某一个CounterCell CounterCell a; long v; int m; // 是否有冲突，默认为true，代表没有冲突 boolean uncontended = true; // 判断CounterCell[]没有初始化，执行fullAddCount方法，初始化数组 if (as == null || (m = as.length - 1) &lt; 0 || // CounterCell[]已经初始化了，基于随机数拿到数组上的一个CounterCell，如果为null，执行fullAddCount方法，初始化CounterCell (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // CounterCell[]已经初始化了，并且指定索引位置上有CounterCell // 直接CAS修改指定的CounterCell上的value即可。 // CAS成功，直接告辞！ // CAS失败，代表有冲突，uncontended = false，执行fullAddCount方法 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) { fullAddCount(x, uncontended); return; } // 如果链表长度小于等于1，不去判断扩容 if (check &lt;= 1) return; // 将所有CounterCell中记录的信累加，得到最终的元素个数 s = sumCount(); } // ================================判断扩容======================================= // 判断check大于等于，remove的操作就是小于0的。 因为添加时，才需要去判断是否需要扩容 if (check &gt;= 0) { // 一堆小变量 Node&lt;K,V&gt;[] tab, nt; int n, sc; // 当前元素个数是否大于扩容阈值，并且数组不为null，数组长度没有达到最大值。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) { // 扩容表示戳 int rs = resizeStamp(n); // 正在扩容 if (sc &lt; 0) { // 判断是否可以协助扩容 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 没有线程执行扩容，我来扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); // 重新计数。 s = sumCount(); } } } // CounterCell的类，就类似于LongAdder的Cell @sun.misc.Contended static final class CounterCell { // volatile修饰的value，并且外部基于CAS的方式修改 volatile long value; CounterCell(long x) { value = x; } } @sun.misc.Contended（JDK1.8）： 这个注解是为了解决伪共享的问题（解决缓存行同步带来的性能问题）。 CPU在操作主内存变量前，会将主内存数据缓存到CPU缓存（L1,L2,L3）中， CPU缓存L1，是以缓存行为单位存储数据的，一般默认的大小为64字节。 缓存行同步操作，影响CPU一定的性能。 @Contented注解，会将当前类中的属性，会独占一个缓存行，从而避免缓存行失效造成的性能问题。 @Contented注解，就是将一个缓存行的后面7个位置，填充上7个没有意义的数据。 long value; long l1,l2,l3,l4,l5,l6,l7; // 整体CounterCell数组数据到baseCount final long sumCount() { // 拿到CounterCell[] CounterCell[] as = counterCells; CounterCell a; // 拿到baseCount long sum = baseCount; // 循环走你，遍历CounterCell[]，将值累加到sum中，最终返回sum if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } // CounterCell数组没有初始化 // CounterCell对象没有构建 // 什么都有，但是有并发问题，导致CAS失败 private final void fullAddCount(long x, boolean wasUncontended) { // h：当前线程的随机数 int h; // 判断当前线程的Probe是否初始化。 if ((h = ThreadLocalRandom.getProbe()) == 0) { // 初始化一波 ThreadLocalRandom.localInit(); // 生成随机数。 h = ThreadLocalRandom.getProbe(); // 标记，没有冲突 wasUncontended = true; } // 阿巴阿巴 boolean collide = false; // 死循环………… for (;;) { // as：CounterCell[] // a：CounterCell对 null // n：数组长度 // v：value值 CounterCell[] as; CounterCell a; int n; long v; // CounterCell[]不为null时，做CAS操作 if ((as = counterCells) != null &amp;&amp; (n = as.length) &gt; 0) { // 拿到当前线程随机数对应的CounterCell对象，为null // 第一个if：当前数组已经初始化，但是指定索引位置没有CounterCell对象，构建CounterCell对象放到数组上 if ((a = as[h &amp; (n - 1)]) == null) { // 判断cellsBusy是否为0， if (cellsBusy == 0) { // 构建CounterCell对象 CounterCell r = new CounterCell(x); // 在此判断cellsBusy为0，CAS从0修改为1，代表可以操作当前数组上的指定索引，构建CounterCell，赋值进去 if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { // 构建未完成 boolean created = false; try { // 阿巴阿巴 CounterCell[] rs; int m, j; // DCL，还包含复制 if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; // 再次拿到指定索引位置的值，如果为null，正常将前面构建的CounterCell对象，赋值给数组 rs[j = (m - 1) &amp; h] == null) { // 将CounterCell对象赋值到数组 rs[j] = r; // 构建完成 created = true; } } finally { // 归位 cellsBusy = 0; } if (created) // 跳出循环，告辞 break; continue; // Slot is now non-empty } } collide = false; } // 指定索引位置上有CounterCell对象，有冲突，修改冲突标识 else if (!wasUncontended) wasUncontended = true; // CAS，将数组上存在的CounterCell对象的value进行 + 1操作 else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) // 成功，告辞。 break; // 之前拿到的数组引用和成员变量的引用值不一样了， // CounterCell数组的长度是都大于CPU内核数，不让CounterCell数组长度大于CPU内核数。 else if (counterCells != as || n &gt;= NCPU) // 当前线程的循环失败，不进行扩容 collide = false; // 如果没并发问题，并且可以扩容，设置标示位，下次扩容 else if (!collide) collide = true; // 扩容操作 // 先判断cellsBusy为0，再基于CAS将cellsBusy从0修改为1。 else if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { try { // DCL! if (counterCells == as) { // 构建一个原来长度2倍的数组 CounterCell[] rs = new CounterCell[n &lt;&lt; 1]; // 将老数组数据迁移到新数组 for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; // 新数组复制给成员变量 counterCells = rs; } } finally { // 归位 cellsBusy = 0; } // 归位 collide = false; // 开启下次循环 continue; } // 重新设置当前线程的随机数，争取下次循环成功！ h = ThreadLocalRandom.advanceProbe(h); } // CounterCell[]没有初始化 // 判断cellsBusy为0.代表没有其他线程在初始化或者扩容当前CounterCell[] // 判断counterCells还是之前赋值的as，代表没有并发问题 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; // 修改cellsBusy，从0改为1，代表当前线程要开始初始化了 U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { // 标识，init未成功 boolean init = false; try { // DCL! if (counterCells == as) { // 构建CounterCell[]，默认长度为2 CounterCell[] rs = new CounterCell[2]; // 用当前线程的随机数，和数组长度 - 1，进行&amp;运算，将这个位置上构建一个CounterCell对象，赋值value为1 rs[h &amp; 1] = new CounterCell(x); // 将声明好的rs，赋值给成员变量 counterCells = rs; // init成功 init = true; } } finally { // cellsBusy归位。 cellsBusy = 0; } if (init) // 退出循环 break; } // 到这就直接在此操作baseCount。 else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) break; // Fall back on using base } } 1.7.2 size方法方法分析size获取ConcurrentHashMap中的元素个数 public int size() { // 基于sumCount方法获取元素个数 long n = sumCount(); // 做了一些简单的健壮性判断 return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } // 整体CounterCell数组数据到baseCount final long sumCount() { // 拿到CounterCell[] CounterCell[] as = counterCells; CounterCell a; // 拿到baseCount long sum = baseCount; // 循环走你，遍历CounterCell[]，将值累加到sum中，最终返回sum if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } 二、CopyOnWriteArrayList2.1 CopyOnWriteArrayList介绍CopyOnWriteArrayList是一个线程安全的ArrayList。 CopyOnWriteArrayList是基于lock锁和数组副本的形式去保证线程安全。 在写数据时，需要先获取lock锁，需要复制一个副本数组，将数据插入到副本数组中，将副本数组赋值给CopyOnWriteArrayList中的array。 因为CopyOnWriteArrayList每次写数据都要构建一个副本，如果你的业务是写多，并且数组中的数据量比较大，尽量避免去使用CopyOnWriteArrayList，因为这里会构建大量的数组副本，比较占用内存资源。 CopyOnWriteArrayList是弱一致性的，写操作先执行，但是副本还有落到CopyOnWriteArrayList的array属性中，此时读操作是无法查询到的。 2.2 核心属性&amp;方法主要查看2个核心属性，以及2个核心方法，还有无参构造 /** 写操作时，需要先获取到的锁资源，CopyOnWriteArrayList全局唯一的。 */ final transient ReentrantLock lock = new ReentrantLock(); /** CopyOnWriteArrayList真实存放数据的位置，查询也是查询当前array */ private transient volatile Object[] array; // 获取array属性 final Object[] getArray() { return array; } // 替换array属性 final void setArray(Object[] a) { array = a; } /** * 默认new的CopyOnWriteArrayList数组长度为0。 * 不像ArrayList，初始长度是10，每次扩容1/2, CopyOnWriteArrayList不存在这个概念 * 每次写的时候都会构建一个新的数组 */ public CopyOnWriteArrayList() { setArray(new Object[0]); } 2.3 读操作CopyOnWriteArrayList的读操作就是get方法，基于数组索引位置获取数据。 方法之所以要差分成两个，是因为CopyOnWriteArrayList中在获取数据时，不单单只有一个array的数组需要获取值，还有副本中数据的值。 // 查询数据时，只能通过get方法查询CopyOnWriteArrayList中的数据 public E get(int index) { // getArray拿到array数组，调用get方法的重载 return get(getArray(), index); } // 执行get(int)时，内部调用的方法 private E get(Object[] a, int index) { // 直接拿到数组上指定索引位置的值 return (E) a[index]; } 2.4 写操作CopyOnWriteArrayList是基于lock锁和副本数组的形式保证线程安全。 // 写入元素，不指定索引位置，直接放到最后的位置 public boolean add(E e) { // 获取全局锁，并执行lock final ReentrantLock lock = this.lock; lock.lock(); try { // 获取原数组，还获取了原数组的长度 Object[] elements = getArray(); int len = elements.length; // 基于原数组复制一份副本数组，并且长度比原来多了一个 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将添加的数据放到副本数组最后一个位置 newElements[len] = e; // 将副本数组，赋值给CopyOnWriteArrayList的原数组 setArray(newElements); // 添加成功，返回true return true; } finally { // 释放锁~ lock.unlock(); } } // 写入元素，指定索引位置。（不会覆盖数据） public void add(int index, E element) { // 拿锁，加锁~ final ReentrantLock lock = this.lock; lock.lock(); try { // 获取原数组，还获取了原数组的长度 Object[] elements = getArray(); int len = elements.length; // 如果索引位置大于原数组的长度，或者索引位置是小于0的。 if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(\"Index: \"+index+ \", Size: \"+len); // 声明了副本数组 Object[] newElements; // 原数组长度 - 索引位置等到numMoved int numMoved = len - index; // 如果numMoved为0，说明数据要放到最后面的位置 if (numMoved == 0) // 直接走了原生态的方式，正常复制一份副本数组 newElements = Arrays.copyOf(elements, len + 1); else { // 数组要插入的位置不是最后一个位置 // 副本数组长度依然是原长度 + 1 newElements = new Object[len + 1]; // 将原数组从0索引位置开始复制，复制到副本数组中的前置位置 System.arraycopy(elements, 0, newElements, 0, index); // 将原数组从index位置开始复制，复制到副本数组的index + 1往后放。 // 这时，index就空缺出来了。 System.arraycopy(elements, index, newElements, index + 1, numMoved); } // 数据正常放到指定的索引位置 newElements[index] = element; // 将副本数组，赋值给CopyOnWriteArrayList的原数组 setArray(newElements); } finally { // 释放锁 lock.unlock(); } } 2.5 移除数据关于remove操作，要分析两个方法 基于索引位置移除指定数据 基于具体元素删除数组中最靠前的数据 当前这种方式，嵌套了一层，导致如果元素存在话，成本是比较高的。 如果元素不存在，这种设计不需要加锁，提升写的效率 // 删除指定索引位置的数据 public E remove(int index) { // 拿锁，加锁 final ReentrantLock lock = this.lock; lock.lock(); try { // 获取原数组和原数组长度 Object[] elements = getArray(); int len = elements.length; // 通过get方法拿到index位置的数据 E oldValue = get(elements, index); // 声明numMoved int numMoved = len - index - 1; // 如果numMoved为0，说明删除的元素是最后的位置 if (numMoved == 0) // Arrays.copyOf复制一份新的副本数组，并且将最后一个数据不要了 // 基于setArray将副本数组赋值给array原数组 setArray(Arrays.copyOf(elements, len - 1)); else { // 删除的元素不在最后面的位置 // 声明副本数组，长度是原数组长度 - 1 Object[] newElements = new Object[len - 1]; // 从0开始复制的index前面 System.arraycopy(elements, 0, newElements, 0, index); // 从index后面复制到最后 System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); } // 返回被干掉的数据 return oldValue; } finally { // 释放锁 lock.unlock(); } } // 删除元素（最前面的） public boolean remove(Object o) { // 没加锁！！！！ // 获取原数组 Object[] snapshot = getArray(); // 用indexOf获取元素在数组的哪个索引位置 // 没找到的话，返回-1 int index = indexOf(o, snapshot, 0, snapshot.length); // 如果index &lt; 0,说明元素没找到，直接返回false，告辞 // 如果找到了元素的位置，直接执行remove方法的重载 return (index &lt; 0) ? false : remove(o, snapshot, index); } // 执行remove(Object o)，找到元素位置时，执行当前方法 private boolean remove(Object o, Object[] snapshot, int index) { // 拿锁，加锁 final ReentrantLock lock = this.lock; lock.lock(); try { // 拿到原数组和长度 Object[] current = getArray(); int len = current.length; // findIndex: 是给if起标识，break 标识的时候，直接跳出if的代码块~~ if (snapshot != current) findIndex: { // 如果没进到if，说明数组没变化，按照原来的index位置删除即可 // 进到这，说明数组有变化，之前的索引位置不一定对 // 拿到index位置和原数组长度的值 int prefix = Math.min(index, len); // 循环判断，数组变更后，是否影响到了要删除元素的位置 for (int i = 0; i &lt; prefix; i++) { // 如果不相等，说明元素变化了。 // 同时判断变化的元素是否是我要删除的元素o if (current[i] != snapshot[i] &amp;&amp; eq(o, current[i])) { // 如果满足条件，说明当前位置就是我要删除的元素 index = i; break findIndex; } } // 如果for循环结束，没有退出if，说明元素可能变化了，总之没找到要删除的元素 // 如果删除元素的位置，已经大于等于数组长度了。 if (index &gt;= len) // 超过索引范围了，没法删除了。 return false; // 索引还在范围内，判断是否是还是原位置，如果是，直接跳出if代码块 if (current[index] == o) break findIndex; // 重新找元素在数组中的位置 index = indexOf(o, current, index, len); // 找到直接跳出if代码块 // 没找到。执行下面的return false if (index &lt; 0) return false; } // 删除套路，构建新数组，复制index前的，复制index后的 Object[] newElements = new Object[len - 1]; System.arraycopy(current, 0, newElements, 0, index); System.arraycopy(current, index + 1, newElements, index, len - index - 1); // 复制到array setArray(newElements); // 返回true，删除成功 return true; } finally { lock.unlock(); } } 2.6 覆盖数据&amp;清空集合覆盖数据就是set方法，可以将指定位置的数据替换 // 覆盖数据 public E set(int index, E element) { // 拿锁，加锁 final ReentrantLock lock = this.lock; lock.lock(); try { // 获取原数组 Object[] elements = getArray(); // 获取原数组的原位置数据 E oldValue = get(elements, index); // 原数据和新数据不一样 if (oldValue != element) { // 拿到原数据的长度，复制一份副本。 int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len); // 将element替换掉副本数组中的数据 newElements[index] = element; // 写回原数组 setArray(newElements); } else { // 原数据和新数据一样，啥不干，把拿出来的数组再写回去 setArray(elements); } // 返回原值 return oldValue; } finally { // 释放锁 lock.unlock(); } } 清空就是清空了~~~ public void clear() { // 加锁 final ReentrantLock lock = this.lock; lock.lock(); try { // 扔一个长度为0的数组 setArray(new Object[0]); } finally { lock.unlock(); } } 2.7 迭代器用ArrayList时，如果想在遍历的过程中去移除或者修改元素，必须使用迭代器才可以。 但是CopyOnWriteArrayList这哥们即便用了迭代器也不让做写操作 不让在迭代时做写操作是因为不希望迭代操作时，会影响到写操作，还有，不希望迭代时，还需要加锁。 // 获取遍历CopyOnWriteArrayList的iterator。 public Iterator&lt;E&gt; iterator() { // 其实就是new了一个COWIterator对象，并且获取了array，指定从0开始遍历 return new COWIterator&lt;E&gt;(getArray(), 0); } static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; { /** 遍历的快照 */ private final Object[] snapshot; /** 游标，索引~~~ */ private int cursor; // 有参构造 private COWIterator(Object[] elements, int initialCursor) { cursor = initialCursor; snapshot = elements; } // 有没有下一个元素，基于遍历的索引位置和数组长度查看 public boolean hasNext() { return cursor &lt; snapshot.length; } // 有没有上一个元素 public boolean hasPrevious() { return cursor &gt; 0; } // 获取下一个值，游标动一下 public E next() { // 确保下个位置有数据 if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; } // 获取上一个值，游标往上移动 public E previous() { if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; } // 拿到下一个值的索引，返回游标 public int nextIndex() { return cursor; } // 拿到上一个值的索引，返回游标 public int previousIndex() { return cursor-1; } // 写操作全面禁止！！ public void remove() { throw new UnsupportedOperationException(); } public void set(E e) { throw new UnsupportedOperationException(); } public void add(E e) { throw new UnsupportedOperationException(); } // 兼容函数式编程 @Override public void forEachRemaining(Consumer&lt;? super E&gt; action) { Objects.requireNonNull(action); Object[] elements = snapshot; final int size = elements.length; for (int i = cursor; i &lt; size; i++) { @SuppressWarnings(\"unchecked\") E e = (E) elements[i]; action.accept(e); } cursor = size; } }","categories":[{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"java 并发","slug":"java-并发","permalink":"http://example.com/tags/java-%E5%B9%B6%E5%8F%91/"}]},{"title":"并发编程(01)","slug":"并发编程-01","date":"2023-09-11T01:16:13.000Z","updated":"2023-09-11T06:38:24.912Z","comments":true,"path":"posts/20.html","link":"","permalink":"http://example.com/posts/20.html","excerpt":"","text":"并发 @Author 郑金维 (上传仅供个人学习交流使用 如有侵权立刻删除) 一、线程的基础概念一、基础概念1.1 进程与线程A什么是进程？ 进程是指运行中的程序。 比如我们使用钉钉，浏览器，需要启动这个程序，操作系统会给这个程序分配一定的资源（占用内存资源）。 什么线程？ 线程是CPU调度的基本单位，每个线程执行的都是某一个进程的代码的某个片段。 举个栗子：房子与人 比如现在有一个100平的房子，这个方式可以看做是一个进程 房子里有人，人就可以看做成一个线程。 人在房子中做一个事情，比如吃饭，学习，睡觉。这个就好像线程在执行某个功能的代码。 所谓进程就是线程的容器，需要线程利用进程中的一些资源，处理一个代码、指令。最终实现进程锁预期的结果。 进程和线程的区别： 根本不同：进程是操作系统分配的资源，而线程是CPU调度的基本单位。 资源方面：同一个进程下的线程共享进程中的一些资源。线程同时拥有自身的独立存储空间。进程之间的资源通常是独立的。 数量不同：进程一般指的就是一个进程。而线程是依附于某个进程的，而且一个进程中至少会有一个或多个线程。 开销不同：毕竟进程和线程不是一个级别的内容，线程的创建和终止的时间是比较短的。而且线程之间的切换比进程之间的切换速度要快很多。而且进程之间的通讯很麻烦，一般要借助内核才可以实现，而线程之间通讯，相当方面。 ……………… 1.2 多线程什么是多线程？ 多线程是指：单个进程中同时运行多个线程。 多线程的不低是为了提高CPU的利用率。 可以通过避免一些网络IO或者磁盘IO等需要等待的操作，让CPU去调度其他线程。 这样可以大幅度的提升程序的效率，提高用户的体验。 比如Tomcat可以做并行处理，提升处理的效率，而不是一个一个排队。 不如要处理一个网络等待的操作，开启一个线程去处理需要网络等待的任务，让当前业务线程可以继续往下执行逻辑，效率是可以得到大幅度提升的。 多线程的局限 如果线程数量特别多，CPU在切换线程上下文时，会额外造成很大的消耗。 任务的拆分需要依赖业务场景，有一些异构化的任务，很难对任务拆分，还有很多业务并不是多线程处理更好。 线程安全问题：虽然多线程带来了一定的性能提升，但是再做一些操作时，多线程如果操作临界资源，可能会发生一些数据不一致的安全问题，甚至涉及到锁操作时，会造成死锁问题。 1.3 串行、并行、并发什么是串行： 串行就是一个一个排队，第一个做完，第二个才能上。 什么是并行： 并行就是同时处理。（一起上！！！） 什么是并发： 这里的并发并不是三高中的高并发问题，这里是多线程中的并发概念（CPU调度线程的概念）。CPU在极短的时间内，反复切换执行不同的线程，看似好像是并行，但是只是CPU高速的切换。 并行囊括并发。 并行就是多核CPU同时调度多个线程，是真正的多个线程同时执行。 单核CPU无法实现并行效果，单核CPU是并发。 1.4 同步异步、阻塞非阻塞同步与异步：执行某个功能后，被调用者是否会主动反馈信息 阻塞和非阻塞：执行某个功能后，调用者是否需要一直等待结果的反馈。 两个概念看似相似，但是侧重点是完全不一样的。 同步阻塞：比如用锅烧水，水开后，不会主动通知你。烧水开始执行后，需要一直等待水烧开。 同步非阻塞：比如用锅烧水，水开后，不会主动通知你。烧水开始执行后，不需要一直等待水烧开，可以去执行其他功能，但是需要时不时的查看水开了没。 异步阻塞：比如用水壶烧水，水开后，会主动通知你水烧开了。烧水开始执行后，需要一直等待水烧开。 异步非阻塞：比如用水壶烧水，水开后，会主动通知你水烧开了。烧水开始执行后，不需要一直等待水烧开，可以去执行其他功能。 异步非阻塞这个效果是最好的，平时开发时，提升效率最好的方式就是采用异步非阻塞的方式处理一些多线程的任务。 二、线程的创建线程的创建分为三种方式： 2.1 继承Thread类 重写run方法启动线程是调用start方法，这样会创建一个新的线程，并执行线程的任务。 如果直接调用run方法，这样会让当前线程执行run方法中的业务逻辑。 public class MiTest { public static void main(String[] args) { MyJob t1 = new MyJob(); t1.start(); for (int i = 0; i &lt; 100; i++) { System.out.println(\"main:\" + i); } } } class MyJob extends Thread{ @Override public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(\"MyJob:\" + i); } } } 2.2 实现Runnable接口 重写run方法public class MiTest { public static void main(String[] args) { MyRunnable myRunnable = new MyRunnable(); Thread t1 = new Thread(myRunnable); t1.start(); for (int i = 0; i &lt; 1000; i++) { System.out.println(\"main:\" + i); } } } class MyRunnable implements Runnable{ @Override public void run() { for (int i = 0; i &lt; 1000; i++) { System.out.println(\"MyRunnable:\" + i); } } } 最常用的方式： 匿名内部类方式： Thread t1 = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; 1000; i++) { System.out.println(\"匿名内部类:\" + i); } } }); lambda方式： Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { System.out.println(\"lambda:\" + i); } }); 2.3 实现Callable 重写call方法，配合FutureTaskCallable一般用于有返回结果的非阻塞的执行方法 同步非阻塞。 public class MiTest { public static void main(String[] args) throws ExecutionException, InterruptedException { //1. 创建MyCallable MyCallable myCallable = new MyCallable(); //2. 创建FutureTask，传入Callable FutureTask futureTask = new FutureTask(myCallable); //3. 创建Thread线程 Thread t1 = new Thread(futureTask); //4. 启动线程 t1.start(); //5. 做一些操作 //6. 要结果 Object count = futureTask.get(); System.out.println(\"总和为：\" + count); } } class MyCallable implements Callable{ @Override public Object call() throws Exception { int count = 0; for (int i = 0; i &lt; 100; i++) { count += i; } return count; } } 2.4 基于线程池构建线程追其底层，其实只有一种，实现Runnble 二、线程的使用2.1 线程的状态网上对线程状态的描述很多，有5种，6种，7种，都可以接受 5中状态一般是针对传统的线程状态来说（操作系统层面） Java中给线程准备的6种状态 NEW：Thread对象被创建出来了，但是还没有执行start方法。 RUNNABLE：Thread对象调用了start方法，就为RUNNABLE状态（CPU调度/没有调度） BLOCKED、WAITING、TIME_WAITING：都可以理解为是阻塞、等待状态，因为处在这三种状态下，CPU不会调度当前线程 BLOCKED：synchronized没有拿到同步锁，被阻塞的情况 WAITING：调用wait方法就会处于WAITING状态，需要被手动唤醒 TIME_WAITING：调用sleep方法或者join方法，会被自动唤醒，无需手动唤醒 TERMINATED：run方法执行完毕，线程生命周期到头了 在Java代码中验证一下效果 NEW： public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { }); System.out.println(t1.getState()); } RUNNABLE： public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while(true){ } }); t1.start(); Thread.sleep(500); System.out.println(t1.getState()); } BLOCKED： public static void main(String[] args) throws InterruptedException { Object obj = new Object(); Thread t1 = new Thread(() -&gt; { // t1线程拿不到锁资源，导致变为BLOCKED状态 synchronized (obj){ } }); // main线程拿到obj的锁资源 synchronized (obj) { t1.start(); Thread.sleep(500); System.out.println(t1.getState()); } } WAITING： public static void main(String[] args) throws InterruptedException { Object obj = new Object(); Thread t1 = new Thread(() -&gt; { synchronized (obj){ try { obj.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } }); t1.start(); Thread.sleep(500); System.out.println(t1.getState()); } TIMED_WAITING： public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } }); t1.start(); Thread.sleep(500); System.out.println(t1.getState()); } TERMINATED： public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } }); t1.start(); Thread.sleep(1000); System.out.println(t1.getState()); } 2.2 线程的常用方法2.2.1 获取当前线程Thread的静态方法获取当前线程对象 public static void main(String[] args) throws ExecutionException, InterruptedException { // 获取当前线程的方法 Thread main = Thread.currentThread(); System.out.println(main); // \"Thread[\" + getName() + \",\" + getPriority() + \",\" + group.getName() + \"]\"; // Thread[main,5,main] } 2.2.2 线程的名字在构建Thread对象完毕后，一定要设置一个有意义的名称，方面后期排查错误 public static void main(String[] args) throws ExecutionException, InterruptedException { Thread t1 = new Thread(() -&gt; { System.out.println(Thread.currentThread().getName()); }); t1.setName(\"模块-功能-计数器\"); t1.start(); } 2.2.3 线程的优先级其实就是CPU调度线程的优先级、 java中给线程设置的优先级别有10个级别，从1~10任取一个整数。 如果超出这个范围，会排除参数异常的错误 public static void main(String[] args) throws ExecutionException, InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 1000; i++) { System.out.println(\"t1:\" + i); } }); Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 1000; i++) { System.out.println(\"t2:\" + i); } }); t1.setPriority(1); t2.setPriority(10); t2.start(); t1.start(); } 2.2.4 线程的让步可以通过Thread的静态方法yield，让当前线程从运行状态转变为就绪状态。 public static void main(String[] args) throws ExecutionException, InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { if(i == 50){ Thread.yield(); } System.out.println(\"t1:\" + i); } }); Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { System.out.println(\"t2:\" + i); } }); t2.start(); t1.start(); } 2.2.5 线程的休眠Thread的静态方法，让线程从运行状态转变为等待状态 sleep有两个方法重载： 第一个就是native修饰的，让线程转为等待状态的效果 第二个是可以传入毫秒和一个纳秒的方法（如果纳秒值大于等于0.5毫秒，就给休眠的毫秒值+1。如果传入的毫秒值是0，纳秒值不为0，就休眠1毫秒） sleep会抛出一个InterruptedException public static void main(String[] args) throws InterruptedException { System.out.println(System.currentTimeMillis()); Thread.sleep(1000); System.out.println(System.currentTimeMillis()); } 2.2.6 线程的强占Thread的非静态方法join方法 需要在某一个线程下去调用这个方法 如果在main线程中调用了t1.join()，那么main线程会进入到等待状态，需要等待t1线程全部执行完毕，在恢复到就绪状态等待CPU调度。 如果在main线程中调用了t1.join(2000)，那么main线程会进入到等待状态，需要等待t1执行2s后，在恢复到就绪状态等待CPU调度。如果在等待期间，t1已经结束了，那么main线程自动变为就绪状态等待CPU调度。 public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 10; i++) { System.out.println(\"t1:\" + i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } }); t1.start(); for (int i = 0; i &lt; 10; i++) { System.out.println(\"main:\" + i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } if (i == 1){ try { t1.join(2000); } catch (InterruptedException e) { e.printStackTrace(); } } } } 2.2.7 守护线程默认情况下，线程都是非守护线程 JVM会在程序中没有非守护线程时，结束掉当前JVM 主线程默认是非守护线程，如果主线程执行结束，需要查看当前JVM内是否还有非守护线程，如果没有JVM直接停止 public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 10; i++) { System.out.println(\"t1:\" + i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } }); t1.setDaemon(true); t1.start(); } 2.2.8 线程的等待和唤醒可以让获取synchronized锁资源的线程通过wait方法进去到锁的等待池，并且会释放锁资源 可以让获取synchronized锁资源的线程，通过notify或者notifyAll方法，将等待池中的线程唤醒，添加到锁池中 notify随机的唤醒等待池中的一个线程到锁池 notifyAll将等待池中的全部线程都唤醒，并且添加到锁池 在调用wait方法和notify以及norifyAll方法时，必须在synchronized修饰的代码块或者方法内部才可以，因为要操作基于某个对象的锁的信息维护。 public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { sync(); },\"t1\"); Thread t2 = new Thread(() -&gt; { sync(); },\"t2\"); t1.start(); t2.start(); Thread.sleep(12000); synchronized (MiTest.class) { MiTest.class.notifyAll(); } } public static synchronized void sync() { try { for (int i = 0; i &lt; 10; i++) { if(i == 5) { MiTest.class.wait(); } Thread.sleep(1000); System.out.println(Thread.currentThread().getName()); } } catch (InterruptedException e) { e.printStackTrace(); } } 2.3 线程的结束方式线程结束方式很多，最常用就是让线程的run方法结束，无论是return结束，还是抛出异常结束，都可以 2.3.1 stop方法（不用）强制让线程结束，无论你在干嘛，不推荐使用当然当然方式，但是，他确实可以把线程干掉 public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } }); t1.start(); Thread.sleep(500); t1.stop(); System.out.println(t1.getState()); } 2.3.2 使用共享变量（很少会用）这种方式用的也不多，有的线程可能会通过死循环来保证一直运行。 咱们可以通过修改共享变量在破坏死循环，让线程退出循环，结束run方法 static volatile boolean flag = true; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while(flag){ // 处理任务 } System.out.println(\"任务结束\"); }); t1.start(); Thread.sleep(500); flag = false; } 2.3.3 interrupt方式共享变量方式 public static void main(String[] args) throws InterruptedException { // 线程默认情况下， interrupt标记位：false System.out.println(Thread.currentThread().isInterrupted()); // 执行interrupt之后，再次查看打断信息 Thread.currentThread().interrupt(); // interrupt标记位：ture System.out.println(Thread.currentThread().isInterrupted()); // 返回当前线程，并归位为false interrupt标记位：ture System.out.println(Thread.interrupted()); // 已经归位了 System.out.println(Thread.interrupted()); // ===================================================== Thread t1 = new Thread(() -&gt; { while(!Thread.currentThread().isInterrupted()){ // 处理业务 } System.out.println(\"t1结束\"); }); t1.start(); Thread.sleep(500); t1.interrupt(); } 通过打断WAITING或者TIMED_WAITING状态的线程，从而抛出异常自行处理 这种停止线程方式是最常用的一种，在框架和JUC中也是最常见的 public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while(true){ // 获取任务 // 拿到任务，执行任务 // 没有任务了，让线程休眠 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); System.out.println(\"基于打断形式结束当前线程\"); return; } } }); t1.start(); Thread.sleep(500); t1.interrupt(); } wait和sleep的区别？ 单词不一样。 sleep属于Thread类中的static方法、wait属于Object类的方法 sleep属于TIMED_WAITING，自动被唤醒、wait属于WAITING，需要手动唤醒。 sleep方法在持有锁时，执行，不会释放锁资源、wait在执行后，会释放锁资源。 sleep可以在持有锁或者不持有锁时，执行。 wait方法必须在只有锁时才可以执行。 wait方法会将持有锁的线程从owner扔到WaitSet集合中，这个操作是在修改ObjectMonitor对象，如果没有持有synchronized锁的话，是无法操作ObjectMonitor对象的。 二、并发编程的三大特性一、原子性1.1 什么是并发编程的原子性JMM（Java Memory Model）。不同的硬件和不同的操作系统在内存上的操作有一定差异的。Java为了解决相同代码在不同操作系统上出现的各种问题，用JMM屏蔽掉各种硬件和操作系统带来的差异。 让Java的并发编程可以做到跨平台。 JMM规定所有变量都会存储在主内存中，在操作的时候，需要从主内存中复制一份到线程内存（CPU内存），在线程内部做计算。然后再写回主内存中（不一定！）。 原子性的定义：原子性指一个操作是不可分割的，不可中断的，一个线程在执行时，另一个线程不会影响到他。 并发编程的原子性用代码阐述： private static int count; public static void increment(){ try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } count++; } public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { increment(); } }); Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { increment(); } }); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(count); } 当前程序：多线程操作共享数据时，预期的结果，与最终的结果不符。 原子性：多线程操作临界资源，预期的结果与最终结果一致。 通过对这个程序的分析，可以查看出，++的操作，一共分为了三部，首先是线程从主内存拿到数据保存到CPU的寄存器中，然后在寄存器中进行+1操作，最终将结果写回到主内存当中。 1.2 保证并发编程的原子性1.2.1 synchronized因为++操作可以从指令中查看到 可以在方法上追加synchronized关键字或者采用同步代码块的形式来保证原子性 synchronized可以让避免多线程同时操作临街资源，同一时间点，只会有一个线程正在操作临界资源 1.2.2 CAS到底什么是CAS compare and swap也就是比较和交换，他是一条CPU的并发原语。 他在替换内存的某个位置的值时，首先查看内存中的值与预期值是否一致，如果一致，执行替换操作。这个操作是一个原子性操作。 Java中基于Unsafe的类提供了对CAS的操作的方法，JVM会帮助我们将方法实现CAS汇编指令。 但是要清楚CAS只是比较和交换，在获取原值的这个操作上，需要你自己实现。 private static AtomicInteger count = new AtomicInteger(0); public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { count.incrementAndGet(); } }); Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { count.incrementAndGet(); } }); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(count); } Doug Lea在CAS的基础上帮助我们实现了一些原子类，其中就包括现在看到的AtomicInteger，还有其他很多原子类…… CAS的缺点：CAS只能保证对一个变量的操作是原子性的，无法实现对多行代码实现原子性。 CAS的问题： ABA问题：问题如下，可以引入版本号的方式，来解决ABA的问题。Java中提供了一个类在CAS时，针对各个版本追加版本号的操作。 AtomicStampeReference AtomicStampedReference在CAS时，不但会判断原值，还会比较版本信息。 public static void main(String[] args) { AtomicStampedReference&lt;String&gt; reference = new AtomicStampedReference&lt;&gt;(\"AAA\",1); String oldValue = reference.getReference(); int oldVersion = reference.getStamp(); boolean b = reference.compareAndSet(oldValue, \"B\", oldVersion, oldVersion + 1); System.out.println(\"修改1版本的：\" + b); boolean c = reference.compareAndSet(\"B\", \"C\", 1, 1 + 1); System.out.println(\"修改2版本的：\" + c); } * **自旋时间过长问题**： * 可以指定CAS一共循环多少次，如果超过这个次数，直接失败/或者挂起线程。（自旋锁、自适应自旋锁） * 可以在CAS一次失败后，将这个操作暂存起来，后面需要获取结果时，将暂存的操作全部执行，再返回最后的结果。 #### 1.2.3 Lock锁 Lock锁是在JDK1.5由Doug Lea研发的，他的性能相比synchronized在JDK1.5的时期，性能好了很多多，但是在JDK1.6对synchronized优化之后，性能相差不大，但是如果涉及并发比较多时，推荐ReentrantLock锁，性能会更好。 实现方式： ```java private static int count; private static ReentrantLock lock = new ReentrantLock(); public static void increment() { lock.lock(); try { count++; try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } } finally { lock.unlock(); } } public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { increment(); } }); Thread t2 = new Thread(() -&gt; { for (int i = 0; i &lt; 100; i++) { increment(); } }); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(count); } ReentrantLock可以直接对比synchronized，在功能上来说，都是锁。 但是ReentrantLock的功能性相比synchronized更丰富。 ReentrantLock底层是基于AQS实现的，有一个基于CAS维护的state变量来实现锁的操作。 1.2.4 ThreadLocalJava中的四种引用类型 Java中的使用引用类型分别是强，软，弱，虚。 User user = new User（）； 在 Java 中最常见的就是强引用，把一个对象赋给一个引用变量，这个引用变量就是一个强引用。当一个对象被强引用变量引用时，它始终处于可达状态，它是不可能被垃圾回收机制回收的，即使该对象以后永远都不会被用到 JVM 也不会回收。因此强引用是造成 Java 内存泄漏的主要原因之一。 SoftReference 其次是软引用，对于只有软引用的对象来说，当系统内存足够时它不会被回收，当系统内存空间不足时它会被回收。软引用通常用在对内存敏感的程序中，作为缓存使用。 然后是弱引用，它比软引用的生存期更短，对于只有弱引用的对象来说，只要垃圾回收机制一运行，不管 JVM 的内存空间是否足够，总会回收该对象占用的内存。可以解决内存泄漏问题，ThreadLocal就是基于弱引用解决内存泄漏的问题。 最后是虚引用，它不能单独使用，必须和引用队列联合使用。虚引用的主要作用是跟踪对象被垃圾回收的状态。不过在开发中，我们用的更多的还是强引用。 ThreadLocal保证原子性的方式，是不让多线程去操作临界资源，让每个线程去操作属于自己的数据 代码实现 static ThreadLocal tl1 = new ThreadLocal(); static ThreadLocal tl2 = new ThreadLocal(); public static void main(String[] args) { tl1.set(\"123\"); tl2.set(\"456\"); Thread t1 = new Thread(() -&gt; { System.out.println(\"t1:\" + tl1.get()); System.out.println(\"t1:\" + tl2.get()); }); t1.start(); System.out.println(\"main:\" + tl1.get()); System.out.println(\"main:\" + tl2.get()); } ThreadLocal实现原理： 每个Thread中都存储着一个成员变量，ThreadLocalMap ThreadLocal本身不存储数据，像是一个工具类，基于ThreadLocal去操作ThreadLocalMap ThreadLocalMap本身就是基于Entry[]实现的，因为一个线程可以绑定多个ThreadLocal，这样一来，可能需要存储多个数据，所以采用Entry[]的形式实现。 每一个现有都自己独立的ThreadLocalMap，再基于ThreadLocal对象本身作为key，对value进行存取 ThreadLocalMap的key是一个弱引用，弱引用的特点是，即便有弱引用，在GC时，也必须被回收。这里是为了在ThreadLocal对象失去引用后，如果key的引用是强引用，会导致ThreadLocal对象无法被回收 ThreadLocal内存泄漏问题： 如果ThreadLocal引用丢失，key因为弱引用会被GC回收掉，如果同时线程还没有被回收，就会导致内存泄漏，内存中的value无法被回收，同时也无法被获取到。 只需要在使用完毕ThreadLocal对象之后，及时的调用remove方法，移除Entry即可 二、可见性2.1 什么是可见性可见性问题是基于CPU位置出现的，CPU处理速度非常快，相对CPU来说，去主内存获取数据这个事情太慢了，CPU就提供了L1，L2，L3的三级缓存，每次去主内存拿完数据后，就会存储到CPU的三级缓存，每次去三级缓存拿数据，效率肯定会提升。 这就带来了问题，现在CPU都是多核，每个线程的工作内存（CPU三级缓存）都是独立的，会告知每个线程中做修改时，只改自己的工作内存，没有及时的同步到主内存，导致数据不一致问题。 可见性问题的代码逻辑 private static boolean flag = true; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while (flag) { // .... } System.out.println(\"t1线程结束\"); }); t1.start(); Thread.sleep(10); flag = false; System.out.println(\"主线程将flag改为false\"); } 2.2 解决可见性的方式2.2.1 volatilevolatile是一个关键字，用来修饰成员变量。 如果属性被volatile修饰，相当于会告诉CPU，对当前属性的操作，不允许使用CPU的缓存，必须去和主内存操作 volatile的内存语义： volatile属性被写：当写一个volatile变量，JMM会将当前线程对应的CPU缓存及时的刷新到主内存中 volatile属性被读：当读一个volatile变量，JMM会将对应的CPU缓存中的内存设置为无效，必须去主内存中重新读取共享变量 其实加了volatile就是告知CPU，对当前属性的读写操作，不允许使用CPU缓存，加了volatile修饰的属性，会在转为汇编之后后，追加一个lock的前缀，CPU执行这个指令时，如果带有lock前缀会做两个事情： 将当前处理器缓存行的数据写回到主内存 这个写回的数据，在其他的CPU内核的缓存中，直接无效。 总结：volatile就是让CPU每次操作这个数据时，必须立即同步到主内存，以及从主内存读取数据。 private volatile static boolean flag = true; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while (flag) { // .... } System.out.println(\"t1线程结束\"); }); t1.start(); Thread.sleep(10); flag = false; System.out.println(\"主线程将flag改为false\"); } 2.2.2 synchronizedsynchronized也是可以解决可见性问题的，synchronized的内存语义。 如果涉及到了synchronized的同步代码块或者是同步方法，获取锁资源之后，将内部涉及到的变量从CPU缓存中移除，必须去主内存中重新拿数据，而且在释放锁之后，会立即将CPU缓存中的数据同步到主内存。 private static boolean flag = true; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while (flag) { synchronized (MiTest.class){ //... } System.out.println(111); } System.out.println(\"t1线程结束\"); }); t1.start(); Thread.sleep(10); flag = false; System.out.println(\"主线程将flag改为false\"); } 2.2.3 LockLock锁保证可见性的方式和synchronized完全不同，synchronized基于他的内存语义，在获取锁和释放锁时，对CPU缓存做一个同步到主内存的操作。 Lock锁是基于volatile实现的。Lock锁内部再进行加锁和释放锁时，会对一个由volatile修饰的state属性进行加减操作。 如果对volatile修饰的属性进行写操作，CPU会执行带有lock前缀的指令，CPU会将修改的数据，从CPU缓存立即同步到主内存，同时也会将其他的属性也立即同步到主内存中。还会将其他CPU缓存行中的这个数据设置为无效，必须重新从主内存中拉取。 private static boolean flag = true; private static Lock lock = new ReentrantLock(); public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { while (flag) { lock.lock(); try{ //... }finally { lock.unlock(); } } System.out.println(\"t1线程结束\"); }); t1.start(); Thread.sleep(10); flag = false; System.out.println(\"主线程将flag改为false\"); } 2.2.4 finalfinal修饰的属性，在运行期间是不允许修改的，这样一来，就间接的保证了可见性，所有多线程读取final属性，值肯定是一样。 final并不是说每次取数据从主内存读取，他没有这个必要，而且final和volatile是不允许同时修饰一个属性的 final修饰的内容已经不允许再次被写了，而volatile是保证每次读写数据去主内存读取，并且volatile会影响一定的性能，就不需要同时修饰。 三、有序性3.1 什么是有序性在Java中，.java文件中的内容会被编译，在执行前需要再次转为CPU可以识别的指令，CPU在执行这些指令时，为了提升执行效率，在不影响最终结果的前提下（满足一些要求），会对指令进行重排。 指令乱序执行的原因，是为了尽可能的发挥CPU的性能。 Java中的程序是乱序执行的。 Java程序验证乱序执行效果： static int a,b,x,y; public static void main(String[] args) throws InterruptedException { for (int i = 0; i &lt; Integer.MAX_VALUE; i++) { a = 0; b = 0; x = 0; y = 0; Thread t1 = new Thread(() -&gt; { a = 1; x = b; }); Thread t2 = new Thread(() -&gt; { b = 1; y = a; }); t1.start(); t2.start(); t1.join(); t2.join(); if(x == 0 &amp;&amp; y == 0){ System.out.println(\"第\" + i + \"次，x = \"+ x + \",y = \" + y); } } } 单例模式由于指令重排序可能会出现问题： 线程可能会拿到没有初始化的对象，导致在使用时，可能由于内部属性为默认值，导致出现一些不必要的问题 private static volatile MiTest test; private MiTest(){} public static MiTest getInstance(){ // B if(test == null){ synchronized (MiTest.class){ if(test == null){ // A , 开辟空间，test指向地址，初始化 test = new MiTest(); } } } return test; } 3.2 as-if-serialas-if-serial语义： 不论指定如何重排序，需要保证单线程的程序执行结果是不变的。 而且如果存在依赖的关系，那么也不可以做指令重排。 // 这种情况肯定不能做指令重排序 int i = 0; i++; // 这种情况肯定不能做指令重排序 int j = 200; j * 100; j + 100; // 这里即便出现了指令重排，也不可以影响最终的结果，20100 3.3 happens-before具体规则： 1. 单线程happen-before原则：在同一个线程中，书写在前面的操作happen-before后面的操作。 2. 锁的happen-before原则：同一个锁的unlock操作happen-before此锁的lock操作。 3. volatile的happen-before原则： 对一个volatile变量的写操作happen-before对此变量的任意操作。 4. happen-before的传递性原则： 如果A操作 happen-before B操作，B操作happen-before C操作，那么A操作happen-before C操作。 5. 线程启动的happen-before原则：同一个线程的start方法happen-before此线程的其它方法。 6. 线程中断的happen-before原则：对线程interrupt方法的调用happen-before被中断线程的检测到中断发送的代码。 7. 线程终结的happen-before原则：线程中的所有操作都happen-before线程的终止检测。 8. 对象创建的happen-before原则：一个对象的初始化完成先于他的finalize方法调用。 JMM只有在不出现上述8中情况时，才不会触发指令重排效果。 不需要过分的关注happens-before原则，只需要可以写出线程安全的代码就可以了。 3.4 volatile如果需要让程序对某一个属性的操作不出现指令重排，除了满足happens-before原则之外，还可以基于volatile修饰属性，从而对这个属性的操作，就不会出现指令重排的问题了。 volatile如何实现的禁止指令重排？ 内存屏障概念。将内存屏障看成一条指令。 会在两个操作之间，添加上一道指令，这个指令就可以避免上下执行的其他指令进行重排序。 三、锁一、锁的分类1.1 可重入锁、不可重入锁Java中提供的synchronized，ReentrantLock，ReentrantReadWriteLock都是可重入锁。 重入：当前线程获取到A锁，在获取之后尝试再次获取A锁是可以直接拿到的。 不可重入：当前线程获取到A锁，在获取之后尝试再次获取A锁，无法获取到的，因为A锁被当前线程占用着，需要等待自己释放锁再获取锁。 1.2 乐观锁、悲观锁Java中提供的synchronized，ReentrantLock，ReentrantReadWriteLock都是悲观锁。 Java中提供的CAS操作，就是乐观锁的一种实现。 悲观锁：获取不到锁资源时，会将当前线程挂起（进入BLOCKED、WAITING），线程挂起会涉及到用户态和内核的太的切换，而这种切换是比较消耗资源的。 用户态：JVM可以自行执行的指令，不需要借助操作系统执行。 内核态：JVM不可以自行执行，需要操作系统才可以执行。 乐观锁：获取不到锁资源，可以再次让CPU调度，重新尝试获取锁资源。 Atomic原子性类中，就是基于CAS乐观锁实现的。 1.3 公平锁、非公平锁Java中提供的synchronized只能是非公平锁。 Java中提供的ReentrantLock，ReentrantReadWriteLock可以实现公平锁和非公平锁 公平锁：线程A获取到了锁资源，线程B没有拿到，线程B去排队，线程C来了，锁被A持有，同时线程B在排队。直接排到B的后面，等待B拿到锁资源或者是B取消后，才可以尝试去竞争锁资源。 非公平锁：线程A获取到了锁资源，线程B没有拿到，线程B去排队，线程C来了，先尝试竞争一波 拿到锁资源：开心，插队成功。 没有拿到锁资源：依然要排到B的后面，等待B拿到锁资源或者是B取消后，才可以尝试去竞争锁资源。 1.4 互斥锁、共享锁Java中提供的synchronized、ReentrantLock是互斥锁。 Java中提供的ReentrantReadWriteLock，有互斥锁也有共享锁。 互斥锁：同一时间点，只会有一个线程持有者当前互斥锁。 共享锁：同一时间点，当前共享锁可以被多个线程同时持有。 二、深入synchronized2.1 类锁、对象锁synchronized的使用一般就是同步方法和同步代码块。 synchronized的锁是基于对象实现的。 如果使用同步方法 static：此时使用的是当前类.class作为锁（类锁） 非static：此时使用的是当前对象做为锁（对象锁） public class MiTest { public static void main(String[] args) { // 锁的是，当前Test.class Test.a(); Test test = new Test(); // 锁的是new出来的test对象 test.b(); } } class Test{ public static synchronized void a(){ System.out.println(\"1111\"); } public synchronized void b(){ System.out.println(\"2222\"); } } 2.2 synchronized的优化在JDK1.5的时候，Doug Lee推出了ReentrantLock，lock的性能远高于synchronized，所以JDK团队就在JDK1.6中，对synchronized做了大量的优化。 锁消除：在synchronized修饰的代码中，如果不存在操作临界资源的情况，会触发锁消除，你即便写了synchronized，他也不会触发。 public synchronized void method(){ // 没有操作临界资源 // 此时这个方法的synchronized你可以认为木有~~ } 锁膨胀：如果在一个循环中，频繁的获取和释放做资源，这样带来的消耗很大，锁膨胀就是将锁的范围扩大，避免频繁的竞争和获取锁资源带来不必要的消耗。 public void method(){ for(int i = 0;i &lt; 999999;i++){ synchronized(对象){ } } // 这是上面的代码会触发锁膨胀 synchronized(对象){ for(int i = 0;i &lt; 999999;i++){ } } } 锁升级：ReentrantLock的实现，是先基于乐观锁的CAS尝试获取锁资源，如果拿不到锁资源，才会挂起线程。synchronized在JDK1.6之前，完全就是获取不到锁，立即挂起当前线程，所以synchronized性能比较差。 synchronized就在JDK1.6做了锁升级的优化 无锁、匿名偏向：当前对象没有作为锁存在。 偏向锁：如果当前锁资源，只有一个线程在频繁的获取和释放，那么这个线程过来，只需要判断，当前指向的线程是否是当前线程 。 如果是，直接拿着锁资源走。 如果当前线程不是我，基于CAS的方式，尝试将偏向锁指向当前线程。如果获取不到，触发锁升级，升级为轻量级锁。（偏向锁状态出现了锁竞争的情况） 轻量级锁：会采用自旋锁的方式去频繁的以CAS的形式获取锁资源（采用的是自适应自旋锁） 如果成功获取到，拿着锁资源走 如果自旋了一定次数，没拿到锁资源，锁升级。 重量级锁：就是最传统的synchronized方式，拿不到锁资源，就挂起当前线程。（用户态&amp;内核态） 2.3 synchronized实现原理synchronized是基于对象实现的。 先要对Java中对象在堆内存的存储有一个了解。 展开MarkWord MarkWord中标记着四种锁的信息：无锁、偏向锁、轻量级锁、 2.4 synchronized的锁升级为了可以在Java中看到对象头的MarkWord信息，需要导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt; &lt;/dependency&gt; 锁默认情况下，开启了偏向锁延迟。 偏向锁在升级为轻量级锁时，会涉及到偏向锁撤销，需要等到一个安全点（STW），才可以做偏向锁撤销，在明知道有并发情况，就可以选择不开启偏向锁，或者是设置偏向锁延迟开启 因为JVM在启动时，需要加载大量的.class文件到内存中，这个操作会涉及到synchronized的使用，为了避免出现偏向锁撤销操作，JVM启动初期，有一个延迟4s开启偏向锁的操作 如果正常开启偏向锁了，那么不会出现无锁状态，对象会直接变为匿名偏向 public static void main(String[] args) throws InterruptedException { Thread.sleep(5000); Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); new Thread(() -&gt; { synchronized (o){ //t1 - 偏向锁 System.out.println(\"t1:\" + ClassLayout.parseInstance(o).toPrintable()); } }).start(); //main - 偏向锁 - 轻量级锁CAS - 重量级锁 synchronized (o){ System.out.println(\"main:\" + ClassLayout.parseInstance(o).toPrintable()); } } 整个锁升级状态的转变： Lock Record以及ObjectMonitor存储的内容 2.5 重量锁底层ObjectMonitor需要去找到openjdk，在百度中直接搜索openjdk，第一个链接就是 找到ObjectMonitor的两个文件，hpp，cpp 先查看核心属性：http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/69087d08d473/src/share/vm/runtime/objectMonitor.hpp ObjectMonitor() { _header = NULL; // header存储着MarkWord _count = 0; // 竞争锁的线程个数 _waiters = 0, // wait的线程个数 _recursions = 0; // 标识当前synchronized锁重入的次数 _object = NULL; _owner = NULL; // 持有锁的线程 _WaitSet = NULL; // 保存wait的线程信息，双向链表 _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; // 获取锁资源失败后，线程要放到当前的单向链表中 FreeNext = NULL ; _EntryList = NULL ; // _cxq以及被唤醒的WaitSet中的线程，在一定机制下，会放到EntryList中 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; _previous_owner_tid = 0; } 适当的查看几个C++中实现的加锁流程 http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/69087d08d473/src/share/vm/runtime/objectMonitor.cpp TryLock int ObjectMonitor::TryLock (Thread * Self) { for (;;) { // 拿到持有锁的线程 void * own = _owner ; // 如果有线程持有锁，告辞 if (own != NULL) return 0 ; // 说明没有线程持有锁，own是null，cmpxchg指令就是底层的CAS实现。 if (Atomic::cmpxchg_ptr (Self, &amp;_owner, NULL) == NULL) { // 成功获取锁资源 return 1 ; } // 这里其实重试操作没什么意义，直接返回-1 if (true) return -1 ; } } try_entry bool ObjectMonitor::try_enter(Thread* THREAD) { // 在判断_owner是不是当前线程 if (THREAD != _owner) { // 判断当前持有锁的线程是否是当前线程，说明轻量级锁刚刚升级过来的情况 if (THREAD-&gt;is_lock_owned ((address)_owner)) { _owner = THREAD ; _recursions = 1 ; OwnerIsThread = 1 ; return true; } // CAS操作，尝试获取锁资源 if (Atomic::cmpxchg_ptr (THREAD, &amp;_owner, NULL) != NULL) { // 没拿到锁资源，告辞 return false; } // 拿到锁资源 return true; } else { // 将_recursions + 1，代表锁重入操作。 _recursions++; return true; } } enter（想方设法拿到锁资源，如果没拿到，挂起扔到_cxq单向链表中） void ATTR ObjectMonitor::enter(TRAPS) { // 拿到当前线程 Thread * const Self = THREAD ; void * cur ; // CAS走你， cur = Atomic::cmpxchg_ptr (Self, &amp;_owner, NULL) ; if (cur == NULL) { // 拿锁成功 return ; } // 锁重入操作 if (cur == Self) { // TODO-FIXME: check for integer overflow! BUGID 6557169. _recursions ++ ; return ; } //轻量级锁过来的。 if (Self-&gt;is_lock_owned ((address)cur)) { _recursions = 1 ; _owner = Self ; OwnerIsThread = 1 ; return ; } // 走到这了，没拿到锁资源，count++ Atomic::inc_ptr(&amp;_count); for (;;) { jt-&gt;set_suspend_equivalent(); // 入队操作，进到cxq中 EnterI (THREAD) ; if (!ExitSuspendEquivalent(jt)) break ; _recursions = 0 ; _succ = NULL ; exit (false, Self) ; jt-&gt;java_suspend_self(); } } // count-- Atomic::dec_ptr(&amp;_count); } EnterI for (;;) { // 入队 node._next = nxt = _cxq ; // CAS的方式入队。 if (Atomic::cmpxchg_ptr (&amp;node, &amp;_cxq, nxt) == nxt) break ; // 重新尝试获取锁资源 if (TryLock (Self) &gt; 0) { assert (_succ != Self , \"invariant\") ; assert (_owner == Self , \"invariant\") ; assert (_Responsible != Self , \"invariant\") ; return ; } } 三、深入ReentrantLock3.1 ReentrantLock和synchronized的区别废话区别：单词不一样。。。 核心区别： ReentrantLock是个类，synchronized是关键字，当然都是在JVM层面实现互斥锁的方式 效率区别： 如果竞争比较激烈，推荐ReentrantLock去实现，不存在锁升级概念。而synchronized是存在锁升级概念的，如果升级到重量级锁，是不存在锁降级的。 底层实现区别： 实现原理是不一样，ReentrantLock基于AQS实现的，synchronized是基于ObjectMonitor 功能向的区别： ReentrantLock的功能比synchronized更全面。 ReentrantLock支持公平锁和非公平锁 ReentrantLock可以指定等待锁资源的时间。 选择哪个：如果你对并发编程特别熟练，推荐使用ReentrantLock，功能更丰富。如果掌握的一般般，使用synchronized会更好 3.2 AQS概述AQS就是AbstractQueuedSynchronizer抽象类，AQS其实就是JUC包下的一个基类，JUC下的很多内容都是基于AQS实现了部分功能，比如ReentrantLock，ThreadPoolExecutor，阻塞队列，CountDownLatch，Semaphore，CyclicBarrier等等都是基于AQS实现。 首先AQS中提供了一个由volatile修饰，并且采用CAS方式修改的int类型的state变量。 其次AQS中维护了一个双向链表，有head，有tail，并且每个节点都是Node对象 static final class Node { static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; volatile Thread thread; } AQS内部结构和属性 3.3 加锁流程源码剖析3.3.1 加锁流程概述这个是非公平锁的流程 3.3.2 三种加锁源码分析3.3.2.1 lock方法 执行lock方法后，公平锁和非公平锁的执行套路不一样 // 非公平锁 final void lock() { // 上来就先基于CAS的方式，尝试将state从0改为1 if (compareAndSetState(0, 1)) // 获取锁资源成功，会将当前线程设置到exclusiveOwnerThread属性，代表是当前线程持有着锁资源 setExclusiveOwnerThread(Thread.currentThread()); else // 执行acquire，尝试获取锁资源 acquire(1); } // 公平锁 final void lock() { // 执行acquire，尝试获取锁资源 acquire(1); } acquire方法，是公平锁和非公平锁的逻辑一样 public final void acquire(int arg) { // tryAcquire：再次查看，当前线程是否可以尝试获取锁资源 if (!tryAcquire(arg) &amp;&amp; // 没有拿到锁资源 // addWaiter(Node.EXCLUSIVE)：将当前线程封装为Node节点，插入到AQS的双向链表的结尾 // acquireQueued：查看我是否是第一个排队的节点，如果是可以再次尝试获取锁资源，如果长时间拿不到，挂起线程 // 如果不是第一个排队的额节点，就尝试挂起线程即可 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 中断线程的操作 selfInterrupt(); } tryAcquire方法竞争锁最资源的逻辑，分为公平锁和非公平锁 // 非公平锁实现 final boolean nonfairTryAcquire(int acquires) { // 获取当前线程 final Thread current = Thread.currentThread(); // 获取了state熟属性 int c = getState(); // 判断state当前是否为0,之前持有锁的线程释放了锁资源 if (c == 0) { // 再次抢一波锁资源 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); // 拿锁成功返回true return true; } } // 不是0，有线程持有着锁资源，如果是，证明是锁重入操作 else if (current == getExclusiveOwnerThread()) { // 将state + 1 int nextc = c + acquires; if (nextc &lt; 0) // 说明对重入次数+1后，超过了int正数的取值范围 // 01111111 11111111 11111111 11111111 // 10000000 00000000 00000000 00000000 // 说明重入的次数超过界限了。 throw new Error(\"Maximum lock count exceeded\"); // 正常的将计算结果，复制给state setState(nextc); // 锁重入成功 return true; } // 返回false return false; } // 公平锁实现 protected final boolean tryAcquire(int acquires) { // 获取当前线程 final Thread current = Thread.currentThread(); // …. int c = getState(); if (c == 0) { // 查看AQS中是否有排队的Node // 没人排队抢一手 。有人排队，如果我是第一个，也抢一手 if (!hasQueuedPredecessors() &amp;&amp; // 抢一手~ compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } // 锁重入~~~ else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(“Maximum lock count exceeded”); setState(nextc); return true; } return false; } // 查看是否有线程在AQS的双向队列中排队 // 返回false，代表没人排队 public final boolean hasQueuedPredecessors() { // 头尾节点 Node t = tail; Node h = head; // s为头结点的next节点 Node s; // 如果头尾节点相等，证明没有线程排队，直接去抢占锁资源 return h != t &amp;&amp; // s节点不为null，并且s节点的线程为当前线程（排在第一名的是不是我） (s == null || s.thread != Thread.currentThread()); } 4. addWaite方法，将没有拿到锁资源的线程扔到AQS队列中去排队 ```java // 没有拿到锁资源，过来排队， mode：代表互斥锁 private Node addWaiter(Node mode) { // 将当前线程封装为Node， Node node = new Node(Thread.currentThread(), mode); // 拿到尾结点 Node pred = tail; // 如果尾结点不为null if (pred != null) { // 当前节点的prev指向尾结点 node.prev = pred; // 以CAS的方式，将当前线程设置为tail节点 if (compareAndSetTail(pred, node)) { // 将之前的尾结点的next指向当前节点 pred.next = node; return node; } } // 如果CAS失败，以死循环的方式，保证当前线程的Node一定可以放到AQS队列的末尾 enq(node); return node; } private Node enq(final Node node) { for (;;) { // 拿到尾结点 Node t = tail; // 如果尾结点为空，AQS中一个节点都没有，构建一个伪节点，作为head和tail if (t == null) { if (compareAndSetHead(new Node())) tail = head; } else { // 比较熟悉了，以CAS的方式，在AQS中有节点后，插入到AQS队列的末尾 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } acquireQueued方法，判断当前线程是否还能再次尝试获取锁资源，如果不能再次获取锁资源，或者又没获取到，尝试将当前线程挂起 // 当前没有拿到锁资源后，并且到AQS排队了之后触发的方法。 中断操作这里不用考虑 final boolean acquireQueued(final Node node, int arg) { // 不考虑中断 // failed：获取锁资源是否失败（这里简单掌握落地，真正触发的，还是tryLock和lockInterruptibly） boolean failed = true; try { boolean interrupted = false; // 死循环………… for (;;) { // 拿到当前节点的前继节点 final Node p = node.predecessor(); // 前继节点是否是head，如果是head，再次执行tryAcquire尝试获取锁资源。 if (p == head &amp;&amp; tryAcquire(arg)) { // 获取锁资源成功 // 设置头结点为当前获取锁资源成功Node，并且取消thread信息 setHead(node); // help GC p.next = null; // 获取锁失败标识为false failed = false; return interrupted; } // 没拿到锁资源…… // shouldParkAfterFailedAcquire：基于上一个节点转改来判断当前节点是否能够挂起线程，如果可以返回true， // 如果不能，就返回false，继续下次循环 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 这里基于Unsafe类的park方法，将当前线程挂起 parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) // 在lock方法中，基本不会执行。 cancelAcquire(node); } } // 获取锁资源成功后，先执行setHead private void setHead(Node node) { // 当前节点作为头结点 伪 head = node; // 头结点不需要线程信息 node.thread = null; node.prev = null; } // 当前Node没有拿到锁资源，或者没有资格竞争锁资源，看一下能否挂起当前线程 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { // -1，SIGNAL状态：代表当前节点的后继节点，可以挂起线程，后续我会唤醒我的后继节点 // 1，CANCELLED状态：代表当前节点以及取消了 int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 上一个节点为-1之后，当前节点才可以安心的挂起线程 return true; if (ws &gt; 0) { // 如果当前节点的上一个节点是取消状态，我需要往前找到一个状态不为1的Node，作为他的next节点 // 找到状态不为1的节点后，设置一下next和prev do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { // 上一个节点的状态不是1或者-1，那就代表节点状态正常，将上一个节点的状态改为-1 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 3.3.2.2 tryLock方法 tryLock(); // tryLock方法，无论公平锁还有非公平锁。都会走非公平锁抢占锁资源的操作 // 就是拿到state的值， 如果是0，直接CAS浅尝一下 // state 不是0，那就看下是不是锁重入操作 // 如果没抢到，或者不是锁重入操作，告辞，返回false public boolean tryLock() { // 非公平锁的竞争锁操作 return sync.nonfairTryAcquire(1); } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false; } tryLock(time,unit); 第一波分析，类似的代码： // tryLock(time,unit)执行的方法 public final boolean tryAcquireNanos(int arg, long nanosTimeout)throws InterruptedException { // 线程的中断标记位，是不是从false，别改为了true，如果是，直接抛异常 if (Thread.interrupted()) throw new InterruptedException(); // tryAcquire分为公平和非公平锁两种执行方式，如果拿锁成功， 直接告辞， return tryAcquire(arg) || // 如果拿锁失败，在这要等待指定时间 doAcquireNanos(arg, nanosTimeout); } private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { // 如果等待时间是0秒，直接告辞，拿锁失败 if (nanosTimeout &lt;= 0L) return false; // 设置结束时间。 final long deadline = System.nanoTime() + nanosTimeout; // 先扔到AQS队列 final Node node = addWaiter(Node.EXCLUSIVE); // 拿锁失败，默认true boolean failed = true; try { for (;;) { // 如果在AQS中，当前node是head的next，直接抢锁 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return true; } // 结算剩余的可用时间 nanosTimeout = deadline - System.nanoTime(); // 判断是否是否用尽的位置 if (nanosTimeout &lt;= 0L) return false; // shouldParkAfterFailedAcquire：根据上一个节点来确定现在是否可以挂起线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 避免剩余时间太少，如果剩余时间少就不用挂起线程 nanosTimeout &gt; spinForTimeoutThreshold) // 如果剩余时间足够，将线程挂起剩余时间 LockSupport.parkNanos(this, nanosTimeout); // 如果线程醒了，查看是中断唤醒的，还是时间到了唤醒的。 if (Thread.interrupted()) // 是中断唤醒的！ throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } 取消节点分析： // 取消在AQS中排队的Node private void cancelAcquire(Node node) { // 如果当前节点为null，直接忽略。 if (node == null) return; //1. 线程设置为null node.thread = null; //2. 往前跳过被取消的节点，找到一个有效节点 Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; //3. 拿到了上一个节点之前的next Node predNext = pred.next; //4. 当前节点状态设置为1，代表节点取消 node.waitStatus = Node.CANCELLED; // 脱离AQS队列的操作 // 当前Node是尾结点，将tail从当前节点替换为上一个节点 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) { compareAndSetNext(pred, predNext, null); } else { // 到这，上面的操作CAS操作失败 int ws = pred.waitStatus; // 不是head的后继节点 if (pred != head &amp;&amp; // 拿到上一个节点的状态，只要上一个节点的状态不是取消状态，就改为-1 (ws == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) { // 上面的判断都是为了避免后面节点无法被唤醒。 // 前继节点是有效节点，可以唤醒后面的节点 Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); } else { // 当前节点是head的后继节点 unparkSuccessor(node); } node.next = node; // help GC } } 3.3.2.3 lockInterruptibly方法// 这个是lockInterruptibly和tryLock(time,unit)唯一的区别 // lockInterruptibly，拿不到锁资源，就死等，等到锁资源释放后，被唤醒，或者是被中断唤醒 private void doAcquireInterruptibly(int arg) throws InterruptedException { final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 中断唤醒抛异常！ throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } private final boolean parkAndCheckInterrupt() { LockSupport.park(this); // 这个方法可以确认，当前挂起的线程，是被中断唤醒的，还是被正常唤醒的。 // 中断唤醒，返回true，如果是正常唤醒，返回false return Thread.interrupted(); } 3.4 释放锁流程源码剖析3.4.1 释放锁流程概述 3.4.2 释放锁源码分析public void unlock() { // 释放锁资源不分为公平锁和非公平锁，都是一个sync对象 sync.release(1); } // 释放锁的核心流程 public final boolean release(int arg) { // 核心释放锁资源的操作之一 if (tryRelease(arg)) { // 如果锁已经释放掉了，走这个逻辑 Node h = head; // h不为null，说明有排队的（录课时估计脑袋蒙圈圈。） // 如果h的状态不为0（为-1），说明后面有排队的Node，并且线程已经挂起了。 if (h != null &amp;&amp; h.waitStatus != 0) // 唤醒排队的线程 unparkSuccessor(h); return true; } return false; } // ReentrantLock释放锁资源操作 protected final boolean tryRelease(int releases) { // 拿到state - 1（并没有赋值给state） int c = getState() - releases; // 判断当前持有锁的线程是否是当前线程，如果不是，直接抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); // free，代表当前锁资源是否释放干净了。 boolean free = false; if (c == 0) { // 如果state - 1后的值为0，代表释放干净了。 free = true; // 将持有锁的线程置位null setExclusiveOwnerThread(null); } // 将c设置给state setState(c); // 锁资源释放干净返回true，否则返回false return free; } // 唤醒后面排队的Node private void unparkSuccessor(Node node) { // 拿到头节点状态 int ws = node.waitStatus; if (ws &lt; 0) // 先基于CAS，将节点状态从-1，改为0 compareAndSetWaitStatus(node, ws, 0); // 拿到头节点的后续节点。 Node s = node.next; // 如果后续节点为null或者，后续节点的状态为1，代表节点取消了。 if (s == null || s.waitStatus &gt; 0) { s = null; // 如果后续节点为null，或者后续节点状态为取消状态，从后往前找到一个有效节点环境 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) // 从后往前找到状态小于等于0的节点 // 找到离head最新的有效节点，并赋值给s if (t.waitStatus &lt;= 0) s = t; } // 只要找到了这个需要被唤醒的节点，执行unpark唤醒 if (s != null) LockSupport.unpark(s.thread); } 3.5 AQS常见的问题3.5.1 AQS中为什么要有一个虚拟的head节点因为AQS提供了ReentrantLock的基本实现，而在ReentrantLock释放锁资源时，需要去考虑是否需要执行unparkSuccessor方法，去唤醒后继节点。 因为Node中存在waitStatus的状态，默认情况下状态为0，如果当前节点的后继节点线程挂起了，那么就将当前节点的状态设置为-1。这个-1状态的出现是为了避免重复唤醒或者释放资源的问题。 因为AQS中排队的Node中的线程如果挂起了，是无法自动唤醒的。需要释放锁或者释放资源后，再被释放的线程去唤醒挂起的线程。 因为唤醒节点需要从整个AQS双向链表中找到离head最近的有效节点去唤醒。而这个找离head最近的Node可能需要遍历整个双向链表。如果AQS中，没有挂起的线程，代表不需要去遍历AQS双向链表去找离head最近的有效节点。 为了避免出现不必要的循环链表操作，提供了一个-1的状态。如果只有一个Node进入到AQS中排队，所以发现如果是第一个Node进来，他必须先初始化一个虚拟的head节点作为头，来监控后继节点中是否有挂起的线程。 3.5.2 AQS中为什么选择使用双向链表，而不是单向链表首先AQS中一般是存放没有获取到资源的Node，而在竞争锁资源时，ReentrantLock提供了一个方法，lockInterruptibly方法，也就是线程在竞争锁资源的排队途中，允许中断。中断后会执行cancelAcquire方法，从而将当前节点状态置位1，并且从AQS队列中移除掉。如果采用单向链表，当前节点只能按到后继或者前继节点，这样是无法将前继节点指向后继节点的，需要遍历整个AQS从头或者从尾去找。单向链表在移除AQS中排队的Node时，成本很高。 当前在唤醒后继节点时，如果是单向链表也会出问题，因为节点插入方式的问题，导致只能单向的去找有效节点去唤醒，从而造成很多次无效的遍历操作，如果是双向链表就可以解决这个问题。 3.6 ConditionObject3.6.1 ConditionObject的介绍&amp;应用像synchronized提供了wait和notify的方法实现线程在持有锁时，可以实现挂起，已经唤醒的操作。 ReentrantLock也拥有这个功能。 ReentrantLock提供了await和signal方法去实现类似wait和notify的功能。 想执行await或者是signal就必须先持有lock锁的资源。 先look一下Condition的应用 public static void main(String[] args) throws InterruptedException, IOException { ReentrantLock lock = new ReentrantLock(); Condition condition = lock.newCondition(); new Thread(() -&gt; { lock.lock(); System.out.println(\"子线程获取锁资源并await挂起线程\"); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } try { condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"子线程挂起后被唤醒！持有锁资源\"); }).start(); Thread.sleep(100); // =================main====================== lock.lock(); System.out.println(\"主线程等待5s拿到锁资源，子线程执行了await方法\"); condition.signal(); System.out.println(\"主线程唤醒了await挂起的子线程\"); lock.unlock(); } 3.6.2 Condition的构建方式&amp;核心属性发现在通过lock锁对象执行newCondition方法时，本质就是直接new的AQS提供的ConditionObject对象 final ConditionObject newCondition() { return new ConditionObject(); } 其实lock锁中可以有多个Condition对象。 在对Condition1进行操作时，不会影响到Condition2的单向链表。 其次可以发现ConditionObject中，只有两个核心属性： /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; 虽然Node对象有prev和next，但是在ConditionObject中是不会使用这两个属性的，只要在Condition队列中，这两个属性都是null。在ConditionObject中只会使用nextWaiter的属性实现单向链表的效果。 3.6.3 Condition的await方法分析（前置分析）持有锁的线程在执行await方法后会做几个操作： 判断线程是否中断，如果中断了，什么都不做。 没有中断，就讲当前线程封装为Node添加到Condition的单向链表中 一次性释放掉锁资源。 如果当前线程没有在AQS队列，就正常执行LockSupport.park(this)挂起线程。 // await方法的前置分析，只分析到线程挂起 public final void await() throws InterruptedException { // 先判断线程的中断标记位是否是true if (Thread.interrupted()) // 如果是true，就没必要执行后续操作挂起了。 throw new InterruptedException(); // 在线程挂起之前，先将当前线程封装为Node，并且添加到Condition队列中 Node node = addConditionWaiter(); // fullyRelease在释放锁资源，一次性将锁资源全部释放，并且保留重入的次数 int savedState = fullyRelease(node); // 省略一行代码…… // 当前Node是否在AQS队列中？ // 执行fullyRelease方法后，线程就释放锁资源了，如果线程刚刚释放锁资源，其他线程就立即执行了signal方法， // 此时当前线程就被放到了AQS的队列中，这样一来线程就不需要执行LockSupport.park(this);去挂起线程了 while (!isOnSyncQueue(node)) { // 如果没有在AQS队列中，正常在Condition单向链表里，正常挂起线程。 LockSupport.park(this); // 省略部分代码…… } // 省略部分代码…… } // 线程挂起先，添加到Condition单向链表的业务~~ private Node addConditionWaiter() { // 拿到尾节点。 Node t = lastWaiter; // 如果尾节点有值，并且尾节点的状态不正常，不是-2，尾节点可能要拜拜了~ if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) { // 如果尾节点已经取消了，需要干掉取消的尾节点~ unlinkCancelledWaiters(); // 重新获取lastWaiter t = lastWaiter; } // 构建当前线程的Node，并且状态设置为-2 Node node = new Node(Thread.currentThread(), Node.CONDITION); // 如果last节点为null。直接将当前节点设置为firstWaiter if (t == null) firstWaiter = node; else // 如果last节点不为null，说明有值，就排在lastWaiter的后面 t.nextWaiter = node; // 把当前节点设置为最后一个节点 lastWaiter = node; // 返回当前节点 return node; } // 干掉取消的尾节点。 private void unlinkCancelledWaiters() { // 拿到头节点 Node t = firstWaiter; // 声明一个节点，爱啥啥~~~ Node trail = null; // 如果t不为null，就正常执行~~ while (t != null) { // 拿到t的next节点 Node next = t.nextWaiter; // 如果t的状态不为-2，说明有问题 if (t.waitStatus != Node.CONDITION) { // t节点的next为null t.nextWaiter = null; // 如果trail为null，代表头结点状态就是1， if (trail == null) // 将头结点指向next节点 firstWaiter = next; else // 如果trail有值，说明不是头结点位置 trail.nextWaiter = next; // 如果next为null，说明单向链表遍历到最后了，直接结束 if (next == null) lastWaiter = trail; } // 如果t的状态是-2，一切正常 else { // 临时存储t trail = t; } // t指向之前的next t = next; } } // 一次性释放锁资源 final int fullyRelease(Node node) { // 标记位，释放锁资源默认失败！ boolean failed = true; try { // 拿到现在state的值 int savedState = getState(); // 一次性释放干净全部锁资源 if (release(savedState)) { // 释放锁资源失败了么？ 没有！ failed = false; // 返回对应的锁资源信息 return savedState; } else { throw new IllegalMonitorStateException(); } } finally { if (failed) // 如果释放锁资源失败，将节点状态设置为取消 node.waitStatus = Node.CANCELLED; } } 3.6.4 Condition的signal方法分析分为了几个部分： 确保执行signal方法的是持有锁的线程 脱离Condition的队列 将Node状态从-2改为0 将Node添加到AQS队列 为了避免当前Node无法在AQS队列正常唤醒做了一些判断和操作 // 线程挂起后，可以基于signal唤醒~ public final void signal() { // 在ReentrantLock中，如果执行signal的线程没有持有锁资源，直接扔异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 拿到排在Condition首位的Node Node first = firstWaiter; // 有Node在排队，才需要唤醒，如果没有，直接告辞~~ if (first != null) doSignal(first); } // 开始唤醒Condition中的Node中的线程 private void doSignal(Node first) { // 先一波do-while走你~~~ do { // 获取到第二个节点，并且将第二个节点设置为firstWaiter if ( (firstWaiter = first.nextWaiter) == null) // 说明就一个节点在Condition队列中，那么直接将firstWaiter和lastWaiter置位null lastWaiter = null; // 如果还有nextWaiter节点，因为当前节点要被唤醒了，脱离整个Condition队列。将nextWaiter置位null first.nextWaiter = null; // 如果transferForSignal返回true，一切正常，退出while循环 } while (!transferForSignal(first) &amp;&amp; // 如果后续节点还有，往后面继续唤醒，如果没有，退出while循环 (first = firstWaiter) != null); } // 准备开始唤醒在Condition中排队的Node final boolean transferForSignal(Node node) { // 将在Condition队列中的Node的状态从-2，改为0，代表要扔到AQS队列了。 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) // 如果失败了，说明在signal之前应当是线程被中断了，从而被唤醒了。 return false; // 如果正常的将Node的状态从-2改为0，这是就要将Condition中的这个Node扔到AQS的队列。 // 将当前Node扔到AQS队列，返回的p是当前Node的prev Node p = enq(node); // 获取上一个Node的状态 int ws = p.waitStatus; // 如果ws &gt; 0 ，说明这个Node已经被取消了。 // 如果ws状态不是取消，将prev节点的状态改为-1,。 if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 如果prev节点已经取消了，可能会导致当前节点永远无法被唤醒。立即唤醒当前节点，基于acquireQueued方法， // 让当前节点找到一个正常的prev节点，并挂起线程 // 如果prev节点正常，但是CAS修改prev节点失败了。证明prev节点因为并发原因导致状态改变。还是为了避免当前 // 节点无法被正常唤醒，提前唤醒当前线程，基于acquireQueued方法，让当前节点找到一个正常的prev节点，并挂起线程 LockSupport.unpark(node.thread); // 返回true return true; } 3.6.5 Conditiond的await方法分析（后置分析）分为了几个部分： 唤醒之后，要先确认是中断唤醒还是signal唤醒，还是signal唤醒后被中断 确保当前线程的Node已经在AQS队列中 执行acquireQueued方法，等待锁资源。 在获取锁资源后，要确认是否在获取锁资源的阶段被中断过，如果被中断过，并且不是THROW_IE，那就确保interruptMode是REINTERRUPT。 确认当前Node已经不在Condition队列中了 最终根据interruptMode来决定具体做的事情 0：嘛也不做。 THROW_IE：抛出异常 REINTERRUPT：执行线程的interrupt方法 // 现在分析await方法的后半部分 public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); // 中断模式~ int interruptMode = 0; while (!isOnSyncQueue(node)) { LockSupport.park(this); // 如果线程执行到这，说明现在被唤醒了。 // 线程可以被signal唤醒。（如果是signal唤醒，可以确认线程已经在AQS队列中） // 线程可以被interrupt唤醒，线程被唤醒后，没有在AQS队列中。 // 如果线程先被signal唤醒，然后线程中断了。。。。（做一些额外处理） // checkInterruptWhileWaiting可以确认当前中如何唤醒的。 // 返回的值，有三种 // 0：正常signal唤醒，没别的事（不知道Node是否在AQS队列） // THROW_IE（-1）：中断唤醒，并且可以确保在AQS队列 // REINTERRUPT（1）：signal唤醒，但是线程被中断了，并且可以确保在AQS队列 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } // Node一定在AQS队列 // 执行acquireQueued，尝试在ReentrantLock中获取锁资源。 // acquireQueued方法返回true：代表线程在AQS队列中挂起时，被中断过 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) // 如果线程在AQS队列排队时，被中断了，并且不是THROW_IE状态，确保线程的interruptMode是REINTERRUPT // REINTERRUPT：await不是中断唤醒，但是后续被中断过！！！ interruptMode = REINTERRUPT; // 如果当前Node还在condition的单向链表中，脱离Condition的单向链表 if (node.nextWaiter != null) unlinkCancelledWaiters(); // 如果interruptMode是0，说明线程在signal后以及持有锁的过程中，没被中断过，什么事都不做！ if (interruptMode != 0) // 如果不是0~ reportInterruptAfterWait(interruptMode); } // 判断当前线程被唤醒的模式，确认interruptMode的值。 private int checkInterruptWhileWaiting(Node node) { // 判断线程是否中断了。 return Thread.interrupted() ? // THROW_IE：代表线程是被interrupt唤醒的，需要向上排除异常 // REINTERRUPT：代表线程是signal唤醒的，但是在唤醒之后，被中断了。 (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : // 线程是正常的被signal唤醒，并且线程没有中断过。 0; } // 判断线程到底是中断唤醒的，还是signal唤醒的！ final boolean transferAfterCancelledWait(Node node) { // 基于CAS将Node的状态从-2改为0 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) { // 说明是中断唤醒的线程。因为CAS成功了。 // 将Node添加到AQS队列中~（如果是中断唤醒的，当前线程同时存在Condition的单向链表以及AQS的队列中） enq(node); // 返回true return true; } // 判断当前的Node是否在AQS队列（signal唤醒的，但是可能线程还没放到AQS队列） // 等到signal方法将线程的Node扔到AQS队列后，再做后续操作 while (!isOnSyncQueue(node)) // 如果没在AQS队列上，那就线程让步，稍等一会，Node放到AQS队列再处理（看CPU） Thread.yield(); // signal唤醒的，返回false return false; } // 确认Node是否在AQS队列上 final boolean isOnSyncQueue(Node node) { // 如果线程状态为-2，肯定没在AQS队列 // 如果prev节点的值为null，肯定没在AQS队列 if (node.waitStatus == Node.CONDITION || node.prev == null) // 返回false return false; // 如果节点的next不为null。说明已经在AQS队列上。、 if (node.next != null) // 确定AQS队列上有！ return true; // 如果上述判断都没有确认节点在AQS队列上，在AQS队列中寻找一波 return findNodeFromTail(node); } // 在AQS队列中找当前节点 private boolean findNodeFromTail(Node node) { // 拿到尾节点 Node t = tail; for (;;) { // tail是否是当前节点，如果是，说明在AQS队列 if (t == node) // 可以跳出while循环 return true; // 如果节点为null，AQS队列中没有当前节点 if (t == null) // 进入while，让步一手 return false; // t向前引用 t = t.prev; } } private void reportInterruptAfterWait(int interruptMode) throws InterruptedException { // 如果是中断唤醒的await，直接抛出异常！ if (interruptMode == THROW_IE) throw new InterruptedException(); // 如果是REINTERRUPT，signal后被中断过 else if (interruptMode == REINTERRUPT) // 确认线程的中断标记位是true // Thread.currentThread().interrupt(); selfInterrupt(); } 3.6.6 Condition的awaitNanos&amp;signalAll方法分析awaitNanos：仅仅是在await方法的基础上，做了一内内的改变，整体的逻辑思想都是一样的。 挂起线程时，传入要阻塞的时间，时间到了，自动唤醒，走添加到AQS队列的逻辑 // await指定时间，多了个时间到了自动醒。 public final long awaitNanos(long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); // deadline：当前线程最多挂起到什么时间点 final long deadline = System.nanoTime() + nanosTimeout; int interruptMode = 0; while (!isOnSyncQueue(node)) { // nanosTimeout的时间小于等于0，直接告辞！！ if (nanosTimeout &lt;= 0L) { // 正常扔到AQS队列 transferAfterCancelledWait(node); break; } // nanosTimeout的时间大于1000纳秒时，才可以挂起线程 if (nanosTimeout &gt;= spinForTimeoutThreshold) // 如果大于，正常挂起 LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; // 计算剩余的挂起时间，可能需要重新的走while循环，再次挂起线程 nanosTimeout = deadline - System.nanoTime(); } if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); // 剩余的挂起时间 return deadline - System.nanoTime(); } signalAll方法。这个方法一看就懂，之前signal是唤醒1个，这个是全部唤醒 // 以do-while的形式，将Condition单向链表中的所有Node，全部唤醒并扔到AQS队列 private void doSignalAll(Node first) { // 将头尾都置位null~ lastWaiter = firstWaiter = null; do { // 拿到next节点的引用 Node next = first.nextWaiter; // 断开当前Node的nextWaiter first.nextWaiter = null; // 修改Node状态，扔AQS队列，是否唤醒！ transferForSignal(first); // 指向下一个节点 first = next; } while (first != null); } 四、深入ReentrantReadWriteLock一、为什么要出现读写锁synchronized和ReentrantLock都是互斥锁。 如果说有一个操作是读多写少的，还要保证线程安全的话。如果采用上述的两种互斥锁，效率方面很定是很低的。 在这种情况下，咱们就可以使用ReentrantReadWriteLock读写锁去实现。 读读之间是不互斥的，可以读和读操作并发执行。 但是如果涉及到了写操作，那么还得是互斥的操作。 static ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); static ReentrantReadWriteLock.WriteLock writeLock = lock.writeLock(); static ReentrantReadWriteLock.ReadLock readLock = lock.readLock(); public static void main(String[] args) throws InterruptedException { new Thread(() -&gt; { readLock.lock(); try { System.out.println(\"子线程！\"); try { Thread.sleep(500000); } catch (InterruptedException e) { e.printStackTrace(); } } finally { readLock.unlock(); } }).start(); Thread.sleep(1000); writeLock.lock(); try { System.out.println(\"主线程！\"); } finally { writeLock.unlock(); } } 二、读写锁的实现原理ReentrantReadWriteLock还是基于AQS实现的，还是对state进行操作，拿到锁资源就去干活，如果没有拿到，依然去AQS队列中排队。 读锁操作：基于state的高16位进行操作。 写锁操作：基于state的低16为进行操作。 ReentrantReadWriteLock依然是可重入锁。 写锁重入：读写锁中的写锁的重入方式，基本和ReentrantLock一致，没有什么区别，依然是对state进行+1操作即可，只要确认持有锁资源的线程，是当前写锁线程即可。只不过之前ReentrantLock的重入次数是state的正数取值范围，但是读写锁中写锁范围就变小了。 读锁重入：因为读锁是共享锁。读锁在获取锁资源操作时，是要对state的高16位进行 + 1操作。因为读锁是共享锁，所以同一时间会有多个读线程持有读锁资源。这样一来，多个读操作在持有读锁时，无法确认每个线程读锁重入的次数。为了去记录读锁重入的次数，每个读操作的线程，都会有一个ThreadLocal记录锁重入的次数。 写锁的饥饿问题：读锁是共享锁，当有线程持有读锁资源时，再来一个线程想要获取读锁，直接对state修改即可。在读锁资源先被占用后，来了一个写锁资源，此时，大量的需要获取读锁的线程来请求锁资源，如果可以绕过写锁，直接拿资源，会造成写锁长时间无法获取到写锁资源。 读锁在拿到锁资源后，如果再有读线程需要获取读锁资源，需要去AQS队列排队。如果队列的前面需要写锁资源的线程，那么后续读线程是无法拿到锁资源的。持有读锁的线程，只会让写锁线程之前的读线程拿到锁资源 三、写锁分析3.1 写锁加锁流程概述 3.2 写锁加锁源码分析写锁加锁流程 // 写锁加锁的入口 public void lock() { sync.acquire(1); } // 阿巴阿巴！！ public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } // 读写锁的写锁实现tryAcquire protected final boolean tryAcquire(int acquires) { // 拿到当前线程 Thread current = Thread.currentThread(); // 拿到state的值 int c = getState(); // 得到state低16位的值 int w = exclusiveCount(c); // 判断是否有线程持有着锁资源 if (c != 0) { // 当前没有线程持有写锁，读写互斥，告辞。 // 有线程持有写锁，持有写锁的线程不是当前线程，不是锁重入，告辞。 if (w == 0 || current != getExclusiveOwnerThread()) return false; // 当前线程持有写锁。 锁重入。 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // 没有超过锁重入的次数，正常 + 1 setState(c + acquires); return true; } // 尝试获取锁资源 if (writerShouldBlock() || // CAS拿锁 !compareAndSetState(c, c + acquires)) return false; // 拿锁成功，设置占有互斥锁的线程 setExclusiveOwnerThread(current); // 返回true return true; } // ================================================================ // 这个方法是将state的低16位的值拿到 int w = exclusiveCount(c); state &amp; ((1 &lt;&lt; 16) - 1) 00000000 00000000 00000000 00000001 == 1 00000000 00000001 00000000 00000000 == 1 &lt;&lt; 16 00000000 00000000 11111111 11111111 == (1 &lt;&lt; 16) - 1 &amp;运算，一个为0，必然为0，都为1，才为1 // ================================================================ // writerShouldBlock方法查看公平锁和非公平锁的效果 // 非公平锁直接返回false执行CAS尝试获取锁资源 // 公平锁需要查看是否有排队的，如果有排队的，我是否是head的next 3.3 写锁释放锁流程概述&amp;释放锁源码释放的流程和ReentrantLock一致，只是在判断释放是否干净时，判断低16位的值 // 写锁释放锁的tryRelease方法 protected final boolean tryRelease(int releases) { // 判断当前持有写锁的线程是否是当前线程 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 获取state - 1 int nextc = getState() - releases; // 判断低16位结果是否为0，如果为0，free设置为true boolean free = exclusiveCount(nextc) == 0; if (free) // 将持有锁的线程设置为null setExclusiveOwnerThread(null); // 设置给state setState(nextc); // 释放干净，返回true。 写锁有冲入，这里需要返回false，不去释放排队的Node return free; } 四、读锁分析4.1 读锁加锁流程概述1、分析读锁加速的基本流程 2、分析读锁的可重入锁实现以及优化 3、解决ThreadLocal内存泄漏问题 4、读锁获取锁自后，如果唤醒AQS中排队的读线程 4.1.1 基础读锁流程 针对上述简单逻辑的源码分析 // 读锁加锁的方法入口 public final void acquireShared(int arg) { // 竞争锁资源滴干活 if (tryAcquireShared(arg) &lt; 0) // 没拿到锁资源，去排队 doAcquireShared(arg); } // 读锁竞争锁资源的操作 protected final int tryAcquireShared(int unused) { // 拿到当前线程 Thread current = Thread.currentThread(); // 拿到state int c = getState(); // 拿到state的低16位，判断 != 0，有写锁占用着锁资源 // 并且，当前占用锁资源的线程不是当前线程 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) // 写锁被其他线程占用，无法获取读锁，直接返回 -1，去排队 return -1; // 没有线程持有写锁、当前线程持有写锁 // 获取读锁的信息，state的高16位。 int r = sharedCount(c); // 公平锁：就查看队列是由有排队的，有排队的，直接告辞，进不去if，后面也不用判断（没人排队继续走） // 非公平锁：没有排队的，直接抢。 有排队的，但是读锁其实不需要排队，如果出现这个情况，大部分是写锁资源刚刚释放， // 后续Node还没有来记得拿到读锁资源，当前竞争的读线程，可以直接获取 if (!readerShouldBlock() &amp;&amp; // 判断持有读锁的临界值是否达到 r &lt; MAX_COUNT &amp;&amp; // CAS修改state，对高16位进行 + 1 compareAndSetState(c, c + SHARED_UNIT)) { // 省略部分代码！！！！ return 1; } return fullTryAcquireShared(current); } // 非公平锁的判断 final boolean apparentlyFirstQueuedIsExclusive() { Node h, s; return (h = head) != null &amp;&amp; // head为null，可以直接抢占锁资源 (s = h.next) != null &amp;&amp; // head的next为null，可以直接抢占锁资源 !s.isShared() &amp;&amp; // 如果排在head后面的Node，是共享锁，可以直接抢占锁资源。 s.thread != null; // 后面排队的thread为null，可以直接抢占锁资源 } 4.1.2 读锁重入流程 =============================重入操作================ 前面阐述过，读锁为了记录锁重入的次数，需要让每个读线程用ThreadLocal存储重入次数 ReentrantReadWriteLock对读锁重入做了一些优化操作 ============================记录重入次数的核心================ ReentrantReadWriteLock在内部对ThreadLocal做了封装，基于HoldCount的对象存储重入次数，在内部有个count属性记录，而且每个线程都是自己的ThreadLocalHoldCounter，所以可以直接对内部的count进行++操作。 =============================第一个获取读锁资源的重入次数记录方式================ 第一个拿到读锁资源的线程，不需要通过ThreadLocal存储，内部提供了两个属性来记录第一个拿到读锁资源线程的信息 内部提供了firstReader记录第一个拿到读锁资源的线程，firstReaderHoldCount记录firstReader的锁重入次数 ==============================最后一个获取读锁资源的重入次数记录方式================ 最后一个拿到读锁资源的线程，也会缓存他的重入次数，这样++起来更方便 基于cachedHoldCounter缓存最后一个拿到锁资源现成的重入次数 ==============================最后一个获取读锁资源的重入次数记录方式================ 重入次数的流程执行方式： 1、判断当前线程是否是第一个拿到读锁资源的：如果是，直接将firstReader以及firstReaderHoldCount设置为当前线程的信息 2、判断当前线程是否是firstReader：如果是，直接对firstReaderHoldCount++即可。 3、跟firstReader没关系了，先获取cachedHoldCounter，判断是否是当前线程。 3.1、如果不是，获取当前线程的重入次数，将cachedHoldCounter设置为当前线程。 3.2、如果是，判断当前重入次数是否为0，重新设置当前线程的锁从入信息到readHolds（ThreadLocal）中，算是初始化操作，重入次数是0 3.3、前面两者最后都做count++ 上述逻辑源码分析 protected final int tryAcquireShared(int unused) { Thread current = Thread.currentThread(); int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) { // =============================================================== // 判断r == 0，当前是第一个拿到读锁资源的线程 if (r == 0) { // 将firstReader设置为当前线程 firstReader = current; // 将count设置为1 firstReaderHoldCount = 1; } // 判断当前线程是否是第一个获取读锁资源的线程 else if (firstReader == current) { // 直接++。 firstReaderHoldCount++; } // 到这，就说明不是第一个获取读锁资源的线程 else { // 那获取最后一个拿到读锁资源的线程 HoldCounter rh = cachedHoldCounter; // 判断当前线程是否是最后一个拿到读锁资源的线程 if (rh == null || rh.tid != getThreadId(current)) // 如果不是，设置当前线程为cachedHoldCounter cachedHoldCounter = rh = readHolds.get(); // 当前线程是之前的cacheHoldCounter else if (rh.count == 0) // 将当前的重入信息设置到ThreadLocal中 readHolds.set(rh); // 重入的++ rh.count++; } // =============================================================== return 1; } return fullTryAcquireShared(current); } 4.1.3 读锁加锁的后续逻辑fullTryAcquireShared// tryAcquireShard方法中，如果没有拿到锁资源，走这个方法，尝试再次获取，逻辑跟上面基本一致。 final int fullTryAcquireShared(Thread current) { // 声明当前线程的锁重入次数 HoldCounter rh = null; // 死循环 for (;;) { // 再次拿到state int c = getState(); // 当前如果有写锁在占用锁资源，并且不是当前线程，返回-1，走排队策略 if (exclusiveCount(c) != 0) { if (getExclusiveOwnerThread() != current) return -1; } // 查看当前是否可以尝试竞争锁资源（公平锁和非公平锁的逻辑） else if (readerShouldBlock()) { // 无论公平还是非公平，只要进来，就代表要放到AQS队列中了，先做一波准备 // 在处理ThreadLocal的内存泄漏问题 if (firstReader == current) { // 如果当前当前线程是之前的firstReader，什么都不用做 } else { // 第一次进来是null。 if (rh == null) { // 拿到最后一个获取读锁的线程 rh = cachedHoldCounter; // 当前线程并不是cachedHoldCounter，没到拿到 if (rh == null || rh.tid != getThreadId(current)) { // 从自己的ThreadLocal中拿到重入计数器 rh = readHolds.get(); // 如果计数器为0，说明之前没拿到过读锁资源 if (rh.count == 0) // remove，避免内存泄漏 readHolds.remove(); } } // 前面处理完之后，直接返回-1 if (rh.count == 0) return -1; } } // 判断重入次数，是否超出阈值 if (sharedCount(c) == MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // CAS尝试获取锁资源 if (compareAndSetState(c, c + SHARED_UNIT)) { if (sharedCount(c) == 0) { firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) { firstReaderHoldCount++; } else { if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release } return 1; } } } 4.1.4 读线程在AQS队列获取锁资源的后续操作 1、正常如果都是读线程来获取读锁资源，不需要使用到AQS队列的，直接CAS操作即可 2、如果写线程持有着写锁，这是读线程就需要进入到AQS队列排队，可能会有多个读线程在AQS中。 当写锁释放资源后，会唤醒head后面的读线程，当head后面的读线程拿到锁资源后，还需要查看next节点是否也是读线程在阻塞，如果是，直接唤醒 源码分析 // 读锁需要排队的操作 private void doAcquireShared(int arg) { // 声明Node，类型是共享锁，并且扔到AQS中排队 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { // 拿到上一个节点 final Node p = node.predecessor(); // 如果prev节点是head，直接可以执行tryAcquireShared if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { // 拿到读锁资源后，需要做的后续处理 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } // 找到prev有效节点，将状态设置为-1，挂起当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } private void setHeadAndPropagate(Node node, int propagate) { // 拿到head节点 Node h = head; // 将当前节点设置为head节点 setHead(node); // 第一个判断更多的是在信号量有处理JDK1.5 BUG的操作。 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { // 拿到当前Node的next节点 Node s = node.next; // 如果next节点是共享锁，直接唤醒next节点 if (s == null || s.isShared()) doReleaseShared(); } } 4.2 读锁的释放锁流程1、处理重入以及state的值 2、唤醒后续排队的Node 源码分析 // 读锁释放锁流程 public final boolean releaseShared(int arg) { // tryReleaseShared：处理state的值，以及可重入的内容 if (tryReleaseShared(arg)) { // AQS队列的事！ doReleaseShared(); return true; } return false; } // 1、 处理重入问题 2、 处理state protected final boolean tryReleaseShared(int unused) { // 拿到当前线程 Thread current = Thread.currentThread(); // 如果是firstReader，直接干活，不需要ThreadLocal if (firstReader == current) { // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; } // 不是firstReader，从cachedHoldCounter以及ThreadLocal处理 else { // 如果是cachedHoldCounter，正常-- HoldCounter rh = cachedHoldCounter; // 如果不是cachedHoldCounter，从自己的ThreadLocal中拿 if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; // 如果为1或者更小，当前线程就释放干净了，直接remove，避免value内存泄漏 if (count &lt;= 1) { readHolds.remove(); // 如果已经是0，没必要再unlock，扔个异常 if (count &lt;= 0) throw unmatchedUnlockException(); } // -- 走你。 --rh.count; } for (;;) { // 拿到state，高16位，-1，成功后，返回state是否为0 int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) return nextc == 0; } } // 唤醒AQS中排队的线程 private void doReleaseShared() { // 死循环 for (;;) { // 拿到头 Node h = head; // 说明有排队的 if (h != null &amp;&amp; h != tail) { // 拿到head的状态 int ws = h.waitStatus; // 判断是否为 -1 if (ws == Node.SIGNAL) { // 到这，说明后面有挂起的线程，先基于CAS将head的状态从-1，改为0 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒后续节点 unparkSuccessor(h); } // 这里不是给读写锁准备的，在信号量里说。。。 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; } // 这里是出口 if (h == head) break; } } 五、死锁问题在咱们的操作系统2022版本有，已经有最新的死锁课程了，这里就不做过多讲解 查看这个课程： https://www.mashibing.com/course/1368 四、阻塞队列一、基础概念1.1 生产者消费者概念生产者消费者是设计模式的一种。让生产者和消费者基于一个容器来解决强耦合问题。 生产者 消费者彼此之间不会直接通讯的，而是通过一个容器（队列）进行通讯。 所以生产者生产完数据后扔到容器中，不通用等待消费者来处理。 消费者不需要去找生产者要数据，直接从容器中获取即可。 而这种容器最常用的结构就是队列。 1.2 JUC阻塞队列的存取方法常用的存取方法都是来自于JUC包下的BlockingQueue 生产者存储方法 add(E) // 添加数据到队列，如果队列满了，无法存储，抛出异常 offer(E) // 添加数据到队列，如果队列满了，返回false offer(E,timeout,unit) // 添加数据到队列，如果队列满了，阻塞timeout时间，如果阻塞一段时间，依然没添加进入，返回false put(E) // 添加数据到队列，如果队列满了，挂起线程，等到队列中有位置，再扔数据进去，死等！ 消费者取数据方法 remove() // 从队列中移除数据，如果队列为空，抛出异常 poll() // 从队列中移除数据，如果队列为空，返回null，么的数据 poll(timeout,unit) // 从队列中移除数据，如果队列为空，挂起线程timeout时间，等生产者扔数据，再获取 take() // 从队列中移除数据，如果队列为空，线程挂起，一直等到生产者扔数据，再获取 二、ArrayBlockingQueue2.1 ArrayBlockingQueue的基本使用ArrayBlockingQueue在初始化的时候，必须指定当前队列的长度。 因为ArrayBlockingQueue是基于数组实现的队列结构，数组长度不可变，必须提前设置数组长度信息。 public static void main(String[] args) throws ExecutionException, InterruptedException, IOException { // 必须设置队列的长度 ArrayBlockingQueue queue = new ArrayBlockingQueue(4); // 生产者扔数据 queue.add(\"1\"); queue.offer(\"2\"); queue.offer(\"3\",2,TimeUnit.SECONDS); queue.put(\"2\"); // 消费者取数据 System.out.println(queue.remove()); System.out.println(queue.poll()); System.out.println(queue.poll(2,TimeUnit.SECONDS)); System.out.println(queue.take()); } 2.2 生产者方法实现原理生产者添加数据到队列的方法比较多，需要一个一个查看 2.2.1 ArrayBlockingQueue的常见属性ArrayBlockingQueue中的成员变量 lock = 就是一个ReentrantLock count = 就是当前数组中元素的个数 iterms = 就是数组本身 # 基于putIndex和takeIndex将数组结构实现为了队列结构 putIndex = 存储数据时的下标 takeIndex = 去数据时的下标 notEmpty = 消费者挂起线程和唤醒线程用到的Condition（看成sync的wait和notify） notFull = 生产者挂起线程和唤醒线程用到的Condition（看成sync的wait和notify） 2.2.2 add方法实现add方法本身就是调用了offer方法，如果offer方法返回false，直接抛出异常 public boolean add(E e) { if (offer(e)) return true; else // 抛出的异常 throw new IllegalStateException(\"Queue full\"); } 2.2.3 offer方法实现public boolean offer(E e) { // 要求存储的数据不允许为null，为null就抛出空指针 checkNotNull(e); // 当前阻塞队列的lock锁 final ReentrantLock lock = this.lock; // 为了保证线程安全，加锁 lock.lock(); try { // 如果队列中的元素已经存满了， if (count == items.length) // 返回false return false; else { // 队列没满，执行enqueue将元素添加到队列中 enqueue(e); // 返回true return true; } } finally { // 操作完释放锁 lock.unlock(); } } //========================================================== private void enqueue(E x) { // 拿到数组的引用 final Object[] items = this.items; // 将元素放到指定位置 items[putIndex] = x; // 对inputIndex进行++操作，并且判断是否已经等于数组长度，需要归位 if (++putIndex == items.length) // 将索引设置为0 putIndex = 0; // 元素添加成功，进行++操作。 count++; // 将一个Condition中阻塞的线程唤醒。 notEmpty.signal(); } 2.2.4 offer(time,unit)方法生产者在添加数据时，如果队列已经满了，阻塞一会。 阻塞到消费者消费了消息，然后唤醒当前阻塞线程 阻塞到了time时间，再次判断是否可以添加，不能，直接告辞。 // 如果线程在挂起的时候，如果对当前阻塞线程的中断标记位进行设置，此时会抛出异常直接结束 public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException { // 非空检验 checkNotNull(e); // 将时间单位转换为纳秒 long nanos = unit.toNanos(timeout); // 加锁 final ReentrantLock lock = this.lock; // 允许线程中断并排除异常的加锁方式 lock.lockInterruptibly(); try { // 为什么是while（虚假唤醒） // 如果元素个数和数组长度一致，队列慢了 while (count == items.length) { // 判断等待的时间是否还充裕 if (nanos &lt;= 0) // 不充裕，直接添加失败 return false; // 挂起等待，会同时释放锁资源（对标sync的wait方法） // awaitNanos会挂起线程，并且返回剩余的阻塞时间 // 恢复执行时，需要重新获取锁资源 nanos = notFull.awaitNanos(nanos); } // 说明队列有空间了，enqueue将数据扔到阻塞队列中 enqueue(e); return true; } finally { // 释放锁资源 lock.unlock(); } } 2.2.5 put方法如果队列是满的， 就一直挂起，直到被唤醒，或者被中断 public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) // await方法一直阻塞，直到被唤醒或者中断标记位 notFull.await(); enqueue(e); } finally { lock.unlock(); } } 2.3 消费者方法实现原理2.3.1 remove方法// remove方法就是调用了poll public E remove() { E x = poll(); // 如果有数据，直接返回 if (x != null) return x; // 没数据抛出异常 else throw new NoSuchElementException(); } 2.4.2 poll方法// 拉取数据 public E poll() { // 加锁操作 final ReentrantLock lock = this.lock; lock.lock(); try { // 如果没有数据，直接返回null，如果有数据，执行dequeue，取出数据并返回 return (count == 0) ? null : dequeue(); } finally { lock.unlock(); } } //========================================================== // 取出数据 private E dequeue() { // 将成员变量引用到局部变量 final Object[] items = this.items; // 直接获取指定索引位置的数据 E x = (E) items[takeIndex]; // 将数组上指定索引位置设置为null items[takeIndex] = null; // 设置下次取数据时的索引位置 if (++takeIndex == items.length) takeIndex = 0; // 对count进行--操作 count--; // 迭代器内容，先跳过 if (itrs != null) itrs.elementDequeued(); // signal方法，会唤醒当前Condition中排队的一个Node。 // signalAll方法，会将Condition中所有的Node，全都唤醒 notFull.signal(); // 返回数据。 return x; } 2.4.3 poll(time,unit)方法public E poll(long timeout, TimeUnit unit) throws InterruptedException { // 转换时间单位 long nanos = unit.toNanos(timeout); // 竞争锁 final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { // 如果没有数据 while (count == 0) { if (nanos &lt;= 0) // 没数据，也无法阻塞了，返回null return null; // 没数据，挂起消费者线程 nanos = notEmpty.awaitNanos(nanos); } // 取数据 return dequeue(); } finally { lock.unlock(); } } 2.4.4 take方法public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { // 虚假唤醒 while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } 2.4.5 虚假唤醒阻塞队列中，如果需要线程挂起操作，判断有无数据的位置采用的是while循环 ，为什么不能换成if 肯定是不能换成if逻辑判断 线程A，线程B，线程E，线程C。 其中ABE生产者，C属于消费者 假如线程的队列是满的 // E，拿到锁资源，还没有走while判断 while (count == items.length) // A醒了 // B挂起 notFull.await(); enqueue(e)； C此时消费一条数据，执行notFull.signal()唤醒一个线程，A线程被唤醒 E走判断，发现有空余位置，可以添加数据到队列，E添加数据，走enqueue 如果判断是if，A在E释放锁资源后，拿到锁资源，直接走enqueue方法。 此时A线程就是在putIndex的位置，覆盖掉之前的数据，造成数据安全问题 三、LinkedBlockingQueue3.1 LinkedBlockingQueue的底层实现查看LinkedBlockingQueue是如何存储数据，并且实现链表结构的。 // Node对象就是存储数据的单位 static class Node&lt;E&gt; { // 存储的数据 E item; // 指向下一个数据的指针 Node&lt;E&gt; next; // 有参构造 Node(E x) { item = x; } } 查看LinkedBlockingQueue的有参构造 // 可以手动指定LinkedBlockingQueue的长度，如果没有指定，默认为Integer.MAX_VALUE public LinkedBlockingQueue(int capacity) { if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; // 在初始化时，构建一个item为null的节点，作为head和last // 这种node可以成为哨兵Node， // 如果没有哨兵节点，那么在获取数据时，需要判断head是否为null，才能找next // 如果没有哨兵节点，那么在添加数据时，需要判断last是否为null，才能找next last = head = new Node&lt;E&gt;(null); } 查看LinkedBlockingQueue的其他属性 // 因为是链表，没有想数组的length属性，基于AtomicInteger来记录长度 private final AtomicInteger count = new AtomicInteger(); // 链表的头，取 transient Node&lt;E&gt; head; // 链表的尾，存 private transient Node&lt;E&gt; last; // 消费者的锁 private final ReentrantLock takeLock = new ReentrantLock(); // 消费者的挂起操作，以及唤醒用的condition private final Condition notEmpty = takeLock.newCondition(); // 生产者的锁 private final ReentrantLock putLock = new ReentrantLock(); // 生产者的挂起操作，以及唤醒用的condition private final Condition notFull = putLock.newCondition(); 3.2 生产者方法实现原理3.2.1 add方法你懂得，还是走offer方法 public boolean add(E e) { if (offer(e)) return true; else throw new IllegalStateException(\"Queue full\"); } 3.2.2 offer方法public boolean offer(E e) { // 非空校验 if (e == null) throw new NullPointerException(); // 拿到存储数据条数的count final AtomicInteger count = this.count; // 查看当前数据条数，是否等于队列限制长度，达到了这个长度，直接返回false if (count.get() == capacity) return false; // 声明c，作为标记存在 int c = -1; // 将存储的数据封装为Node对象 Node&lt;E&gt; node = new Node&lt;E&gt;(e); // 获取生产者的锁。 final ReentrantLock putLock = this.putLock; // 竞争锁资源 putLock.lock(); try { // 再次做一个判断，查看是否还有空间 if (count.get() &lt; capacity) { // enqueue，扔数据 enqueue(node); // 将数据个数 + 1 c = count.getAndIncrement(); // 拿到count的值 小于 长度限制 // 有生产者在基于await挂起，这里添加完数据后，发现还有空间可以存储数据， // 唤醒前面可能已经挂起的生产者 // 因为这里生产者和消费者不是互斥的，写操作进行的同时，可能也有消费者在消费数据。 if (c + 1 &lt; capacity) // 唤醒生产者 notFull.signal(); } } finally { // 释放锁资源 putLock.unlock(); } // 如果c == 0，代表添加数据之前，队列元素个数是0个。 // 如果有消费者在队列没有数据的时候，来消费，此时消费者一定会挂起线程 if (c == 0) // 唤醒消费者 signalNotEmpty(); // 添加成功返回true，失败返回-1 return c &gt;= 0; } //================================================ private void enqueue(Node&lt;E&gt; node) { // 将当前Node设置为last的next，并且再将当前Node作为last last = last.next = node; } //================================================ private void signalNotEmpty() { // 获取读锁 final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try { // 唤醒。 notEmpty.signal(); } finally { takeLock.unlock(); } } sync -&gt; wait / notify 3.2.3 offer(time,unit)方法public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException { // 非空检验 if (e == null) throw new NullPointerException(); // 将时间转换为纳秒 long nanos = unit.toNanos(timeout); // 标记 int c = -1; // 写锁，数据条数 final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 允许中断的加锁方式 putLock.lockInterruptibly(); try { // 如果元素个数和限制个数一致，直接准备挂起 while (count.get() == capacity) { // 挂起的时间是不是已经没了 if (nanos &lt;= 0) // 添加失败，返回false return false; // 挂起线程 nanos = notFull.awaitNanos(nanos); } // 有空余位置，enqueue添加数据 enqueue(new Node&lt;E&gt;(e)); // 元素个数 + 1 c = count.getAndIncrement(); // 当前添加完数据，还有位置可以添加数据，唤醒可能阻塞的生产者 if (c + 1 &lt; capacity) notFull.signal(); } finally { // 释放锁 putLock.unlock(); } // 如果之前元素个数是0，唤醒可能等待的消费者 if (c == 0) signalNotEmpty(); return true; } 3.2.4 put方法public void put(E e) throws InterruptedException { if (e == null) throw new NullPointerException(); int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try { while (count.get() == capacity) { // 一直挂起线程，等待被唤醒 notFull.await(); } enqueue(node); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); } finally { putLock.unlock(); } if (c == 0) signalNotEmpty(); } 3.3 消费者方法实现原理从remove方法开始，查看消费者获取数据的方式 3.3.1 remove方法public E remove() { E x = poll(); if (x != null) return x; else throw new NoSuchElementException(); } 3.3.2 poll方法public E poll() { // 拿到队列数据个数的计数器 final AtomicInteger count = this.count; // 当前队列中数据是否0 if (count.get() == 0) // 说明队列没数据，直接返回null即可 return null; // 声明返回结果 E x = null; // 标记 int c = -1; // 获取消费者的takeLock final ReentrantLock takeLock = this.takeLock; // 加锁 takeLock.lock(); try { // 基于DCL，确保当前队列中依然有元素 if (count.get() &gt; 0) { // 从队列中移除数据 x = dequeue(); // 将之前的元素个数获取，并-- c = count.getAndDecrement(); if (c &gt; 1) // 如果依然有数据，继续唤醒await的消费者。 notEmpty.signal(); } } finally { // 释放锁资源 takeLock.unlock(); } // 如果之前的元素个数为当前队列的限制长度， // 现在消费者消费了一个数据，多了一个空位可以添加 if (c == capacity) // 唤醒阻塞的生产者 signalNotFull(); return x; } //================================================ private E dequeue() { // 拿到队列的head位置数据 Node&lt;E&gt; h = head; // 拿到了head的next，因为这个是哨兵Node，需要拿到的head.next的数据 Node&lt;E&gt; first = h.next; // 将之前的哨兵Node.next置位null。help GC。 h.next = h; // 将first置位新的head head = first; // 拿到返回结果first节点的item数据，也就是之前head.next.item E x = first.item; // 将first数据置位null，作为新的head first.item = null; // 返回数据 return x; } //================================================ private void signalNotFull() { final ReentrantLock putLock = this.putLock; putLock.lock(); try { // 唤醒生产者。 notFull.signal(); } finally { putLock.unlock(); } } 3.3.3 poll(time,unit)方法public E poll(long timeout, TimeUnit unit) throws InterruptedException { // 返回结果 E x = null; // 标识 int c = -1; // 将挂起实现设置为纳秒级别 long nanos = unit.toNanos(timeout); // 拿到计数器 final AtomicInteger count = this.count; // take锁加锁 final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try { // 如果没数据，进到while while (count.get() == 0) { if (nanos &lt;= 0) return null; // 挂起当前线程 nanos = notEmpty.awaitNanos(nanos); } // 剩下内容，和之前一样。 x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); } finally { takeLock.unlock(); } if (c == capacity) signalNotFull(); return x; } 3.3.4 take方法public E take() throws InterruptedException { E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try { // 相比poll(time,unit)方法，这里的出口只有一个，就是中断标记位，抛出异常，否则一直等待 while (count.get() == 0) { notEmpty.await(); } x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); } finally { takeLock.unlock(); } if (c == capacity) signalNotFull(); return x; } 四、PriorityBlockingQueue概念4.1 PriorityBlockingQueue介绍首先PriorityBlockingQueue是一个优先级队列，他不满足先进先出的概念。 会将查询的数据进行排序，排序的方式就是基于插入数据值的本身。 如果是自定义对象必须要实现Comparable接口才可以添加到优先级队列 排序的方式是基于二叉堆实现的。底层是采用数据结构实现的二叉堆。 4.2 二叉堆结构介绍优先级队列PriorityBlockingQueue基于二叉堆实现的。 private transient Object[] queue; PriorityBlockingQueue是基于数组实现的二叉堆。 二叉堆是什么？ 二叉堆就是一个完整的二叉树。 任意一个节点大于父节点或者小于父节点 基于同步的方式，可以定义出小顶堆和大顶堆 小顶堆以及小顶堆基于数据实现的方式。 4.3 PriorityBlockingQueue核心属性// 数组的初始长度 private static final int DEFAULT_INITIAL_CAPACITY = 11; // 数组的最大长度 // -8的目的是为了适配各个版本的虚拟机 // 默认当前使用的hotspot虚拟机最大支持Integer.MAX_VALUE - 2，但是其他版本的虚拟机不一定。 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; // 存储数据的数组，也是基于这个数组实现的二叉堆。 private transient Object[] queue; // size记录当前阻塞队列中元素的个数 private transient int size; // 要求使用的对象要实现Comparable比较器。基于comparator做对象之间的比较 private transient Comparator&lt;? super E&gt; comparator; // 实现阻塞队列的lock锁 private final ReentrantLock lock; // 挂起线程操作。 private final Condition notEmpty; // 因为PriorityBlockingQueue的底层是基于二叉堆的，而二叉堆又是基于数组实现的，数组长度是固定的，如果需要扩容，需要构建一个新数组。PriorityBlockingQueue在做扩容操作时，不会lock住的，释放lock锁，基于allocationSpinLock属性做标记，来避免出现并发扩容的问题。 private transient volatile int allocationSpinLock; // 阻塞队列中用到的原理，其实就是普通的优先级队列。 private PriorityQueue&lt;E&gt; q; 4.4 PriorityBlockingQueue的写入操作毕竟是阻塞队列，添加数据的操作，咱们是很了解，无法还是add，offer，offer(time,unit)，put。但是因为优先级队列中，数组是可以扩容的，虽然有长度限制，但是依然属于无界队列的概念，所以生产者不会阻塞，所以只有offer方法可以查看。 这次核心的内容并不是添加数据的区别。主要关注的是如何保证二叉堆中小顶堆的结构的，并且还要查看数组扩容的一个过程是怎样的。 4.4.1 offer基本流程因为add方法依然调用的是offer方法，直接查看offer方法即可 public boolean offer(E e) { // 非空判断。 if (e == null) throw new NullPointerException(); // 拿到锁，直接上锁 final ReentrantLock lock = this.lock; lock.lock(); // n：size，元素的个数 // cap：当前数组的长度 // array：就是存储数据的数组 int n, cap; Object[] array; while ((n = size) &gt;= (cap = (array = queue).length)) // 如果元素个数大于等于数组的长度，需要尝试扩容。 tryGrow(array, cap); try { // 拿到了比较器 Comparator&lt;? super E&gt; cmp = comparator; // 比较数据大小，存储数据，是否需要做上移操作，保证平衡的 if (cmp == null) siftUpComparable(n, e, array); else siftUpUsingComparator(n, e, array, cmp); // 元素个数 + 1 size = n + 1; // 如果有挂起的线程，需要去唤醒挂起的消费者。 notEmpty.signal(); } finally { // 释放锁 lock.unlock(); } // 返回true return true; } 4.4.2 offer扩容操作在添加数据之前，会采用while循环的方式，来判断当前元素个数是否大于等于数组长度。如果满足，需要执行tryGrow方法，对数组进行扩容 如果两个线程同时执行tryGrow，只会有一个线程在扩容，另一个线程可能多次走while循环，多次走tryGrow方法，但是依然需要等待前面的线程扩容完毕。 private void tryGrow(Object[] array, int oldCap) { // 释放锁资源。 lock.unlock(); // 声明新数组。 Object[] newArray = null; // 如果allocationSpinLock属性值为0，说明当前没有线程正在扩容的。 if (allocationSpinLock == 0 &amp;&amp; // 基于CAS的方式，将allocationSpinLock从0修改为1，代表当前线程可以开始扩容 UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset,0, 1)) { try { // 计算新数组长度 int newCap = oldCap + ((oldCap &lt; 64) ? // 如果数组长度比较小，这里加快扩容长度速度。 (oldCap + 2) : // 如果长度大于等于64了，每次扩容到1.5倍即可。 (oldCap &gt;&gt; 1)); // 如果新数组长度大于MAX_ARRAY_SIZE，需要做点事了。 if (newCap - MAX_ARRAY_SIZE &gt; 0) { // 声明minCap，长度为老数组 + 1 int minCap = oldCap + 1; // 老数组+1变为负数，或者老数组长度已经大于MAX_ARRAY_SIZE了，无法扩容了。 if (minCap &lt; 0 || minCap &gt; MAX_ARRAY_SIZE) // 告辞，凉凉~~~~ throw new OutOfMemoryError(); // 如果没有超过限制，直接设置为最大长度即可 newCap = MAX_ARRAY_SIZE; } // 新数组长度，得大于老数组长度， // 第二个判断确保没有并发扩容的出现。 if (newCap &gt; oldCap &amp;&amp; queue == array) // 构建出新数组 newArray = new Object[newCap]; } finally { // 新数组有了，标记位归0~~ allocationSpinLock = 0; } } // 如果到了这，newArray依然为null，说明这个线程没有进到if方法中，去构建新数组 if (newArray == null) // 稍微等一手。 Thread.yield(); // 拿锁资源， lock.lock(); // 拿到锁资源后，确认是构建了新数组的线程，这里就需要将新数组复制给queue，并且导入数据 if (newArray != null &amp;&amp; queue == array) { // 将新数组赋值给queue queue = newArray; // 将老数组的数据全部导入到新数组中。 System.arraycopy(array, 0, newArray, 0, oldCap); } } 4.4.3 offer添加数据这里是数据如何放到数组上，并且如何保证的二叉堆结构 // k：当前元素的个数（其实就是要放的索引位置） // x：需要添加的数据 // array：数组。。 private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] array) { // 将插入的元素直接强转为Comparable（com.mashibing.User cannot be cast to java.lang.Comparable） // 这行强转，会导致添加没有实现Comparable的元素，直接报错。 Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; // k大于0，走while逻辑。（原来有数据） while (k &gt; 0) { // 获取父节点的索引位置。 int parent = (k - 1) &gt;&gt;&gt; 1; // 拿到父节点的元素。 Object e = array[parent]; // 用子节点compareTo父节点，如果 &gt;= 0，说明当前son节点比parent要大。 if (key.compareTo((T) e) &gt;= 0) // 直接break，完事， break; // 将son节点的位置设置上之前的parent节点 array[k] = e; // 重新设置x节点需要放置的位置。 k = parent; } // k == 0，当前元素是第一个元素，直接插入进去。 array[k] = key; } 4.5 PriorityBlockingQueue的读取操作读取操作是存储现在挂起的情况的，因为如果数组中元素个数为0，当前线程如果执行了take方法，必然需要挂起。 其次获取数据，因为是优先级队列，所以需要从二叉堆栈顶拿数据，直接拿索引为0的数据即可，但是拿完之后，需要保持二叉堆结构，所以会有下移操作。 4.5.1 查看获取方法流程poll： public E poll() { final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try { // 拿到返回数据，没拿到，返回null return dequeue(); } finally { lock.unlock(); } } poll(time,unit)： public E poll(long timeout, TimeUnit unit) throws InterruptedException { // 将挂起的时间转换为纳秒 long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; // 允许线程中断抛异常的加锁 lock.lockInterruptibly(); // 声明结果 E result; try { // dequeue是去拿数据的，可能会出现拿到的数据为null，如果为null，同时挂起时间还有剩余，这边就直接通过notEmpty挂起线程 while ( (result = dequeue()) == null &amp;&amp; nanos &gt; 0) nanos = notEmpty.awaitNanos(nanos); } finally { lock.unlock(); } // 有数据正常返回，没数据，告辞~ return result; } take： public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); E result; try { while ( (result = dequeue()) == null) // 无线等，要么有数据，要么中断线程 notEmpty.await(); } finally { lock.unlock(); } return result; } 4.5.2 查看dequeue获取数据获取数据主要就是从数组中拿到0索引位置数据，然后保持二叉堆结构 private E dequeue() { // 将元素个数-1，拿到了索引位置。 int n = size - 1; // 判断是不是木有数据了，没数据直接返回null即可 if (n &lt; 0) return null; // 说明有数据 else { // 拿到数组，array Object[] array = queue; // 拿到0索引位置的数据 E result = (E) array[0]; // 拿到最后一个数据 E x = (E) array[n]; // 将最后一个位置置位null array[n] = null; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) siftDownComparable(0, x, array, n); else siftDownUsingComparator(0, x, array, n, cmp); // 元素个数-1，赋值size size = n; // 返回result return result; } } 4.6.3 下移做平衡操作一定要以局部的方式去查看树结构的变化，他是从跟节点往下找较小的一个子节点，将较小的子节点挪动到父节点位置，再将循环往下走，如果一来，整个二叉堆的结构就可以保证了。 // k：默认进来是0 // x：代表二叉堆的最后一个数据 // array：数组 // n：最后一个索引 private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] array,int n) { // 健壮性校验，取完第一个数据，已经没数据了，那就不需要做平衡操作 if (n &gt; 0) { // 拿到最后一个数据的比较器 Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; // 因为二叉堆是一个二叉满树，所以在保证二叉堆结构时，只需要做一半就可以 int half = n &gt;&gt;&gt; 1; // 做了超过一半，就不需要再往下找了。 while (k &lt; half) { // 找左子节点索引，一个公式，可以找到当前节点的左子节点 int child = (k &lt;&lt; 1) + 1; // 拿到左子节点的数据 Object c = array[child]; // 拿到右子节点索引 int right = child + 1; // 确认有右子节点 // 判断左节点是否大于右节点 if (right &lt; n &amp;&amp; c.compareTo(array[right]) &gt; 0) // 如果左大于右，那么c就执行右 c = array[child = right]; // 比较最后一个节点是否小于当前的较小的子节点 if (key.compareTo((T) c) &lt;= 0) break; // 将左右子节点较小的放到之前的父节点位置 array[k] = c; // k重置到之前的子节点位置 k = child; } // 上面while循环搞定后，可以确认整个二叉堆中，数据已经移动ok了，只差当前k的位置数据是null // 将最后一个索引的数据放到k的位置 array[k] = key; } } 五、DelayQueue5.1 DelayQueue介绍&amp;应用DelayQueue就是一个延迟队列，生产者写入一个消息，这个消息还有直接被消费的延迟时间。 需要让消息具有延迟的特性。 DelayQueue也是基于二叉堆结构实现的，甚至本事就是基于PriorityQueue实现的功能。二叉堆结构每次获取的是栈顶的数据，需要让DelayQueue中的数据，在比较时，跟根据延迟时间做比较，剩余时间最短的要放在栈顶。 查看DelayQueue类信息： public class DelayQueue&lt;E extends Delayed&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt; { // 发现DelayQueue中的元素，需要继承Delayed接口。 } // ========================================== // 接口继承了Comparable，这样就具备了比较的能力。 public interface Delayed extends Comparable&lt;Delayed&gt; { // 抽象方法，就是咱们需要设置的延迟时间 long getDelay(TimeUnit unit); // Comparable接口提供的：public int compareTo(T o); } 基于上述特点，声明一个可以写入DelayQueue的元素类 public class Task implements Delayed { /** 任务的名称 */ private String name; /** 什么时间点执行 */ private Long time; /** * * @param name * @param delay 单位毫秒。 */ public Task(String name, Long delay) { // 任务名称 this.name = name; this.time = System.currentTimeMillis() + delay; } /** * 设置任务什么时候可以出延迟队列 * @param unit * @return */ @Override public long getDelay(TimeUnit unit) { // 单位是毫秒，视频里写错了，写成了纳秒， return unit.convert(time - System.currentTimeMillis(),TimeUnit.MILLISECONDS); } /** * 两个任务在插入到延迟队列时的比较方式 * @param o * @return */ @Override public int compareTo(Delayed o) { return (int) (this.time - ((Task)o).getTime()); } } 在使用时，查看到DelayQueue底层用了PriorityQueue，在一定程度上，DelayQueue也是无界队列。 测试效果 public static void main(String[] args) throws InterruptedException { // 声明元素 Task task1 = new Task(\"A\",1000L); Task task2 = new Task(\"B\",5000L); Task task3 = new Task(\"C\",3000L); Task task4 = new Task(\"D\",2000L); // 声明阻塞队列 DelayQueue&lt;Task&gt; queue = new DelayQueue&lt;&gt;(); // 将元素添加到延迟队列中 queue.put(task1); queue.put(task2); queue.put(task3); queue.put(task4); // 获取元素 System.out.println(queue.take()); System.out.println(queue.take()); System.out.println(queue.take()); System.out.println(queue.take()); // A,D,C,B } 在应用时，外卖，15分钟商家需要节点，如果不节点，这个订单自动取消。 可以每下一个订单，就放到延迟队列中，如果规定时间内，商家没有节点，直接通过消费者获取元素，然后取消订单。 只要是有需要延迟一定时间后，再执行的任务，就可以通过延迟队列去实现。 5.2、DelayQueue核心属性可以查看到DelayQueue就四个核心属性 // 因为DelayQueue依然属于阻塞队列，需要保证线程安全。看到只有一把锁，生产者和消费者使用的是一个lock private final transient ReentrantLock lock = new ReentrantLock(); // 因为DelayQueue还是基于二叉堆结构实现的，没有必要重新搞一个二叉堆，直接使用的PriorityQueue private final PriorityQueue&lt;E&gt; q = new PriorityQueue&lt;E&gt;(); // leader一般会存储等待栈顶数据的消费者，在整体写入和消费的过程中，会设置的leader的一些判断。 private Thread leader = null; // 生产者在插入数据时，不会阻塞的。当前的Condition就是给消费者用的 // 比如消费者在获取数据时，发现栈顶的数据还又没到延迟时间。 // 这个时候，咱们就需要将消费者线程挂起，阻塞一会，阻塞到元素到了延迟时间，或者是，生产者插入的元素到了栈顶，此时生产者会唤醒消费者。 private final Condition available = lock.newCondition(); 5.3、DelayQueue写入流程分析Delay是无界的，数组可以动态的扩容，不需要关注生产者的阻塞问题，他就没有阻塞问题。 这里只需要查看offer方法即可。 public boolean offer(E e) { // 直接获取lock，加锁。 final ReentrantLock lock = this.lock; lock.lock(); try { // 直接调用PriorityQueue的插入方法，这里会根据之前重写Delayed接口中的compareTo方法做排序，然后调整上移和下移操作。 q.offer(e); // 调用优先级队列的peek方法，拿到堆顶的数据 // 拿到堆顶数据后，判断是否是刚刚插入的元素 if (q.peek() == e) { // leader赋值为null。在消费者的位置再提一嘴 leader = null; // 唤醒消费者，避免刚刚插入的数据的延迟时间出现问题。 available.signal(); } // 插入成功， return true; } finally { // 释放锁 lock.unlock(); } } 5.4、DelayQueue读取流程分析消费者依然还是存在阻塞的情况，因为有两个情况 消费者要拿到栈顶数据，但是延迟时间还没到，此时消费者需要等待一会。 消费者要来拿数据，但是发现已经有消费者在等待栈顶数据了，这个后来的消费者也需要等待一会。 依然需要查看四个方法的实现 5.4.1 remove方法// 依然是AbstractQueue提供的方法，有结果就返回，没结果扔异常 public E remove() { E x = poll(); if (x != null) return x; else throw new NoSuchElementException(); } 5.4.2 poll方法// poll是浅尝一下，不会阻塞消费者，能拿就拿，拿不到就拉倒 public E poll() { // 消费者和生产者是一把锁，先拿锁，加锁。 final ReentrantLock lock = this.lock; lock.lock(); try { // 拿到栈顶数据。 E first = q.peek(); // 如果元素为null，直接返回null // 如果getDelay方法返回的结果是大于0的，那说明当前元素还每到延迟时间，元素无法返回，返回null if (first == null || first.getDelay(NANOSECONDS) &gt; 0) return null; else // 到这说明元素不为null，并且已经达到了延迟时间，直接调用优先级队列的poll方法 return q.poll(); } finally { // 释放锁。 lock.unlock(); } } 5.4.3 poll(time,unit)方法这个是允许阻塞的，并且指定一定的时间 public E poll(long timeout, TimeUnit unit) throws InterruptedException { // 先将时间转为纳秒 long nanos = unit.toNanos(timeout); // 拿锁，加锁。 final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { // 死循环。 for (;;) { // 拿到堆顶数据 E first = q.peek(); // 如果元素为null if (first == null) { // 并且等待的时间小于等于0。不能等了，直接返回null if (nanos &lt;= 0) return null; // 说明当前线程还有可以阻塞的时间，阻塞指定时间即可。 else // 这里挂起线程后，说明队列没有元素，在生产者添加数据之后，会唤醒 nanos = available.awaitNanos(nanos); // 到这说明，有数据 } else { // 有数据的话，先获取数据现在是否可以执行，延迟时间是否已经到了指定时间 long delay = first.getDelay(NANOSECONDS); // 延迟时间是否已经到了， if (delay &lt;= 0) // 时间到了，直接执行优先级队列的poll方法，返回元素 return q.poll(); // ==================延迟时间没到，消费者需要等一会=================== // 这个是查看消费者可以等待的时间， if (nanos &lt;= 0) // 直接返回nulll return null; // ==================延迟时间没到，消费者可以等一会=================== // 把first赋值为null first = null; // 如果等待的时间，小于元素剩余的延迟时间，消费者直接挂起。反正暂时拿不到，但是不能保证后续是否有生产者添加一个新的数据，我是可以拿到的。 // 如果已经有一个消费者在等待堆顶数据了，我这边不做额外操作，直接挂起即可。 if (nanos &lt; delay || leader != null) nanos = available.awaitNanos(nanos); // 当前消费者的阻塞时间可以拿到数据，并且没有其他消费者在等待堆顶数据 else { // 拿到当前消费者的线程对象 Thread thisThread = Thread.currentThread(); // 将leader设置为当前线程 leader = thisThread; try { // 会让当前消费者，阻塞这个元素的延迟时间 long timeLeft = available.awaitNanos(delay); // 重新计算当前消费者剩余的可阻塞时间，。 nanos -= delay - timeLeft; } finally { // 到了时间，将leader设置为null if (leader == thisThread) leader = null; } } } } } finally { // 没有消费者在等待元素，队列中的元素不为null if (leader == null &amp;&amp; q.peek() != null) // 只要当前没有leader在等，并且队列有元素，就需要再次唤醒消费者。、 // 避免队列有元素，但是没有消费者处理的问题 available.signal(); // 释放锁 lock.unlock(); } } 5.4.4 take方法这个是允许阻塞的，但是可以一直等，要么等到元素，要么等到被中断。 public E take() throws InterruptedException { // 正常加锁，并且允许中断 final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { for (;;) { // 拿到元素 E first = q.peek(); if (first == null) // 没有元素挂起。 available.await(); else { // 有元素，获取延迟时间。 long delay = first.getDelay(NANOSECONDS); // 判断延迟时间是不是已经到了 if (delay &lt;= 0) // 基于优先级队列的poll方法返回 return q.poll(); first = null; // 如果有消费者在等，就正常await挂起 if (leader != null) available.await(); // 如果没有消费者在等的堆顶数据，我来等 else { // 获取当前线程 Thread thisThread = Thread.currentThread(); // 设置为leader，代表等待堆顶的数据 leader = thisThread; try { // 等待指定（堆顶元素的延迟时间）时长， available.awaitNanos(delay); } finally { if (leader == thisThread) // leader赋值null leader = null; } } } } } finally { // 避免消费者无线等，来一个唤醒消费者的方法，一般是其他消费者拿到元素走了之后，并且延迟队列还有元素，就执行if内部唤醒方法 if (leader == null &amp;&amp; q.peek() != null) available.signal(); // 释放锁 lock.unlock(); } } 六、SynchronousQueue6.1 SynchronousQueue介绍SynchronousQueue这个阻塞队列和其他的阻塞队列有很大的区别 在咱们的概念中，队列肯定是要存储数据的，但是SynchronousQueue不会存储数据的 SynchronousQueue队列中，他不存储数据，存储生产者或者是消费者 当存储一个生产者到SynchronousQueue队列中之后，生产者会阻塞（看你调用的方法） 生产者最终会有几种结果： 如果在阻塞期间有消费者来匹配，生产者就会将绑定的消息交给消费者 生产者得等阻塞结果，或者不允许阻塞，那么就直接失败 生产者在阻塞期间，如果线程中断，直接告辞。 同理，消费者和生产者的效果是一样。 生产者和消费者的数据是直接传递的，不会经过SynchronousQueue。 SynchronousQueue是不会存储数据的。 经过阻塞队列的学习： 生产者： offer()：生产者在放到SynchronousQueue的同时，如果有消费者在等待消息，直接配对。如果没有消费者在等待消息，这里直接返回，告辞。 offer(time,unit)：生产者在放到SynchronousQueue的同时，如果有消费者在等待消息，直接配对。如果没有消费者在等待消息，阻塞time时间，如果还没有，告辞。 put()：生产者在放到SynchronousQueue的同时，如果有消费者在等待消息，直接配对。如果没有，死等。 消费者：poll()，poll(time,unit)，take()。道理和上面的生产者一致。 测试效果： public static void main(String[] args) throws InterruptedException { // 因为当前队列不存在数据，没有长度的概念。 SynchronousQueue queue = new SynchronousQueue(); String msg = \"消息！\"; /*new Thread(() -&gt; { // b = false：代表没有消费者来拿 boolean b = false; try { b = queue.offer(msg,1, TimeUnit.SECONDS); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(b); }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(queue.poll()); }).start();*/ new Thread(() -&gt; { try { System.out.println(queue.poll(1, TimeUnit.SECONDS)); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); Thread.sleep(100); new Thread(() -&gt; { queue.offer(msg); }).start(); } 6.2 SynchronousQueue核心属性进到SynchronousQueue类的内部后，发现了一个内部类，Transferer，内部提供了一个transfer的方法 abstract static class Transferer&lt;E&gt; { abstract E transfer(E e, boolean timed, long nanos); } 当前这个类中提供的transfer方法，就是生产者和消费者在调用读写数据时要用到的核心方法。 生产者在调用上述的transfer方法时，第一个参数e会正常传递数据 消费者在调用上述的transfer方法时，第一个参数e会传递null SynchronousQueue针对抽象类Transferer做了几种实现。 一共看到了两种实现方式： TransferStack TransferQueue 这两种类继承了Transferer抽象类，在构建SynchronousQueue时，会指定使用哪种子类 // 到底采用哪种实现，需要把对应的对象存放到这个属性中 private transient volatile Transferer&lt;E&gt; transferer; // 采用无参时，会调用下述方法，再次调用有参构造传入false public SynchronousQueue() { this(false); } // 调用的是当前的有参构造，fair代表公平还是不公平 public SynchronousQueue(boolean fair) { // 如果是公平，采用Queue，如果是不公平，采用Stack transferer = fair ? new TransferQueue&lt;E&gt;() : new TransferStack&lt;E&gt;(); } TransferQueue的特点 代码查看效果 public static void main(String[] args) throws InterruptedException { // 因为当前队列不存在数据，没有长度的概念。 SynchronousQueue queue = new SynchronousQueue(true); SynchronousQueue queue = new SynchronousQueue(false); new Thread(() -&gt; { try { queue.put(\"生1\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); new Thread(() -&gt; { try { queue.put(\"生2\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); new Thread(() -&gt; { try { queue.put(\"生3\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(\"消1：\" + queue.poll()); }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(\"消2：\" + queue.poll()); }).start(); Thread.sleep(100); new Thread(() -&gt; { System.out.println(\"消3：\" + queue.poll()); }).start(); } 6.3 SynchronousQueue的TransferQueue源码为了查看清除SynchronousQueue的TransferQueue源码，需要从两点开始查看源码信息 6.3.1 QNode源码信息static final class QNode { // 当前节点可以获取到next节点 volatile QNode next; // item在不同情况下效果不同 // 生产者：有数据 // 消费者：为null volatile Object item; // 当前线程 volatile Thread waiter; // 当前属性是永磊区分消费者和生产者的属性 final boolean isData; // 最终生产者需要将item交给消费者 // 最终消费者需要获取生产者的item // 省略了大量提供的CAS操作 .... } 6.3.2 transfer方法实现// 当前方法是TransferQueue的核心内容 // e：传递的数据 // timed：false，代表无限阻塞，true，代表阻塞nacos时间 E transfer(E e, boolean timed, long nanos) { // 当前QNode是要封装当前生产者或者消费者的信息 QNode s = null; // isData == true：代表是生产者 // isData == false：代表是消费者 boolean isData = (e != null); // 死循环 for (;;) { // 获取尾节点和头结点 QNode t = tail; QNode h = head; // 为了避免TransferQueue还没有初始化，这边做一个健壮性判断 if (t == null || h == null) continue; // 如果满足h == t 条件，说明当前队列没有生产者或者消费者，为空 // 如果有节点，同时当前节点和队列节点属于同一种角色。 // if中的逻辑是进到队列 if (h == t || t.isData == isData) { // ===================在判断并发问题========================== // 拿到尾节点的next QNode tn = t.next; // 如果t不为尾节点，进来说明有其他线程并发修改了tail if (t != tail) // 重新走for循环 continue; // tn如果为不null，说明前面有线程并发，添加了一个节点 if (tn != null) { // 直接帮助那个并发线程修改tail的指向 advanceTail(t, tn); // 重新走for循环 continue; } // 获取当前线程是否可以阻塞 // 如果timed为true，并且阻塞的时间小于等于0 // 不需要匹配，直接告辞！！！ if (timed &amp;&amp; nanos &lt;= 0) return null; // 如果可以阻塞，将当前需要插入到队列的QNode构建出来 if (s == null) s = new QNode(e, isData); // 基于CAS操作，将tail节点的next设置为当前线程 if (!t.casNext(null, s)) // 如果进到if，说明修改失败，重新执行for循环修改 continue; // CAS操作成功，直接替换tail的指向 advanceTail(t, s); // 如果进到队列中了，挂起线程，要么等生产者，要么等消费者。 // x是返回替换后的数据 Object x = awaitFulfill(s, e, timed, nanos); // 如果元素和节点相等，说明节点取消了 if (x == s) { // 清空当前节点，将上一个节点的next指向当前节点的next，直接告辞 clean(t, s); return null; } // 判断当前节点是否还在队列中 if (!s.isOffList()) { // 将当前节点设置为head advanceHead(t, s); // 如果 x != null， 如果拿到了数据，说明我是消费者 if (x != null) // 将当前节点的item设置为自己 s.item = s; // 线程置位null s.waiter = null; } // 返回数据 return (x != null) ? (E)x : e; } // 匹配队列中的橘色 else { // 拿到head的next，作为要匹配的节点 QNode m = h.next; // 做并发判断，如果头节点，尾节点，或者head.next发生了变化，这边要重新走for循环 if (t != tail || m == null || h != head) continue; // 没并发问题，可以拿数据 // 拿到m节点的item作为x。 Object x = m.item; // 如果isData == (x != null)满足，说明当前出现了并发问题，消费者去匹配队列的消费者不合理 if (isData == (x != null) || // 如果排队的节点取消，就会讲当前QNode中的item指向QNode x == m || // 如果前面两个都没满足，可以交换数据了。 // 如果交换失败，说明有并发问题， !m.casItem(x, e)) { // 重新设置head节点，并且再走一次循环 advanceHead(h, m); continue; } // 替换head advanceHead(h, m); // 唤醒head.next中的线程 LockSupport.unpark(m.waiter); // 这边匹配好了，数据也交换了，直接返回 // 如果 x != null，说明队列中是生产者，当前是消费者，这边直接返回x具体数据 // 反之，队列中是消费者，当前是生产者，直接返回自己的数据 return (x != null) ? (E)x : e; } } }","categories":[{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"java 并发","slug":"java-并发","permalink":"http://example.com/tags/java-%E5%B9%B6%E5%8F%91/"}]},{"title":"算法基础(图)","slug":"算法基础-图","date":"2023-08-29T07:56:19.000Z","updated":"2023-09-11T01:48:08.735Z","comments":true,"path":"posts/19.html","link":"","permalink":"http://example.com/posts/19.html","excerpt":"","text":"图1.何为图 由点的集合和边的集合构成 有向图和无向图都可用有向图来表示 边上可能带有权值 2.图结构的表达 邻接表 邻接矩阵 其它 邻接表: 邻接表是一种链表的集合，用于表示图中每个节点的邻居节点。对于一个有n个节点的图，邻接表是一个包含n个链表的数组，数组的每个元素对应一个节点。对于节点i，邻接表中第i个链表存储了与节点i直接相连的节点。邻接表的实现可以使用链表、数组或其他数据结构。 result： ​ A: (B 3) ​ B: (C 2) ​ C: (A 4),(E 5) ​ D: (C 3) ​ E: (D 1),(A 2) 邻接表的代码实现：# Define a function to create an adjacency list def create_adjacency_list(graph): adjacency_list = {} for i in range(len(graph)): neighbors = [] for j in range(len(graph[i])): if graph[i][j] == 1: neighbors.append(j) adjacency_list[i] = neighbors return adjacency_list 邻接矩阵: 邻接矩阵是一个二维数组，用于表示图中节点之间的连接关系。对于一个有n个节点的图，邻接矩阵是一个大小为n×n的矩阵，其中的元素表示节点之间的连接情况。如果节点i和节点j之间存在连接，则邻接矩阵中的第i行第j列（或第j行第i列）的元素为权值；否则，元素为 ∞ 对于无向图而言，邻接矩阵是对称的，即第i行第j列的元素与第j行第i列的元素相等 result: A B C D E A ∞ 3 ∞ ∞ ∞ B ∞ ∞ 2 ∞ ∞ C 4 ∞ ∞ ∞ 5 D ∞ ∞ 3 ∞ ∞ E 2 ∞ ∞ 1 ∞ 邻接矩阵的代码实现：# Define a function to create an adjacency matrix def create_adjacency_matrix(graph): num_vertices = len(graph) adjacency_matrix = [[0] * num_vertices for _ in range(num_vertices)] for i in range(num_vertices): for j in range(num_vertices): if graph[i][j] == 1: adjacency_matrix[i][j] = 1 return adjacency_matrix 3.图的面试题如何搞定图的算法都不算难，只不过coding的代价比较高1)先用自己最熟练的方式，实现图结构的表达2)在自己熟悉的结构上，实现所有常用的图算法作为模板3)把面试题提供的图结构转化为自己熟悉的图结构，再调用模板或改写即可 4.图的宽度优先&amp;深度优先遍历1. 宽度优先遍历1,利用队列queue实现，hashset2,从源节点开始依次按照宽度进队列，然后弹出3,每弹出一个点，把该节点所有没有进过队列的邻接点放入队列4,直到队列变空 其中：将当前没进过set的邻接点全部加入队列中，依次弹出。 宽度优先遍历的实现：// 宽度优先遍历 public static void bfs(Node node){ if (node == null){ return; } PriorityQueue&lt;Node&gt; queue = new PriorityQueue&lt;&gt;(); HashSet&lt;Node&gt; set = new HashSet&lt;&gt;(); queue.add(node); set.add(node); while (!queue.isEmpty()){ Node cur = queue.poll(); for (Node next : cur.nexts) { if (!set.contains(next)){ queue.add(next); set.add(next); } } } } 2.深度优先遍历不撞南墙不回头算法 1利用栈stack实现，hashset2,从源节点开始把节点按照深度放入栈，然后弹出3,每弹出一个点，把该节点下一个没有进过栈的邻接点放入栈4.直到栈变空 其中：有邻接点不在set中 则只放入一个进入stack，stack从底到上表示走过的路径。 深度优先遍历的实现// 深度优先遍历 public static void dfs(Node node){ if (node == null){ return; } Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); HashSet&lt;Node&gt; set = new HashSet&lt;&gt;(); stack.add(node); set.add(node); System.out.println(node.value); while (!stack.isEmpty()){ Node cur = stack.pop(); for (Node next : cur.nexts) { if (!set.contains(next)){ stack.push(cur); stack.push(next); set.add(next); break; } } } } 5.图的拓扑排序算法1)在图中找到所有入度为0的点输出2)把所有入度为0的点在图中删掉，继续找入度为0的点输出，周而复始3)图的所有点都被删除后，依次输出的顺序就是拓扑排序 要求：有向图且其中没有环应用：事件安排、编译顺序 拓扑排序算法的实现// 拓扑排序 public static List&lt;Node&gt; top(Graph graph){ if (graph == null){ return null; } List&lt;Node&gt; res = new ArrayList&lt;&gt;(); HashMap&lt;Node,Integer&gt; inMap = new HashMap&lt;&gt;(); PriorityQueue&lt;Node&gt; zeroQueue = new PriorityQueue&lt;&gt;(); for (Node node : graph.nodes.values()) { inMap.put(node, node.in); if (node.in == 0) { zeroQueue.add(node); } } while (!zeroQueue.isEmpty()){ Node n = zeroQueue.poll(); res.add(n); for (Node next : n.nexts) { inMap.put(next,inMap.get(next)-1); if (inMap.get(next) == 0){ zeroQueue.add(next); } } } return res; } 6.最小生成树算法之Kruska1)总是从权值最小的边开始考虑，依次考察权值依次变大的边2)当前的边要么进入最小生成树的集合，要么丢弃3)如果当前的边进入最小生成树的集合中不会形成环，就要当前边4)如果当前的边进入最小生成树的集合中会形成环，就不要当前边5)考察完所有边之后，最小生成树的集合也得到了","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"算法基础(并查集)","slug":"算法基础-并查集","date":"2023-08-27T07:29:31.000Z","updated":"2023-08-27T07:56:11.752Z","comments":true,"path":"posts/18.html","link":"","permalink":"http://example.com/posts/18.html","excerpt":"","text":"并查集1.什么是并查集1)有若干个样本a、b、c、d…类型假设是V2)在并查集中一开始认为每个样本都在单独的集合里3)用户可以在任何时候调用如下两个方法：boolean isSameSet(Vx,Vy): 查询样本x和样本y是否属于一个集合void union(Vx,Vy): 把x和y各自所在集合的所有样本合并成一个集合4)isSameSet: 和union方法的代价越低越好 O(1)","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"vue小组件","slug":"vue小组件","date":"2023-08-21T07:52:57.000Z","updated":"2023-09-11T01:48:51.012Z","comments":true,"path":"posts/55a57103.html","link":"","permalink":"http://example.com/posts/55a57103.html","excerpt":"","text":"Vue表单及其它组件1.搜索框template中: &lt;!-- 搜索框 v-model: 绑定data中的值 slot=\"append\" 搜索按钮与输入框连在一起 --&gt; &lt;el-input v-model=\"queryInfo.query\" placeholder=\"请输入客户端IP\" :maxlength=\"15\" style=\"width: 200px; margin-right: 20px;\" &gt; &lt;el-button slot=\"append\" icon=\"el-icon-search\" @click=\"handleSearch\"&gt;&lt;/el-button&gt; &lt;/el-input&gt; script中： script中: // 根据客户端IP进行模糊查询 if (query !== '') { filteredList = filteredList.filter( item =&gt; item.ipClient.includes(query) ) } 运行效果图： 2.时间范围选择框template中: &lt;!-- 时间选择框 --&gt; &lt;el-date-picker v-model=\"queryInfo.dateRange\" type=\"daterange\" range-separator=\"至\" start-placeholder=\"开始日期\" end-placeholder=\"结束日期\" style=\"width: 400px;margin-right: 20px;\" /&gt; script中： // 根据更新时间范围进行筛选 if (dateRange &amp;&amp; dateRange.length === 2) { const startDate = dateRange[0] const endDate = dateRange[1] filteredList = filteredList.filter(item =&gt; { const updateTime = new Date(item.updateTime) return updateTime &gt;= startDate &amp;&amp; updateTime &lt;= endDate }) } 运行效果图： 3.下拉框选择template中: &lt;!-- 更新状态下拉框 --&gt; &lt;el-select v-model=\"queryInfo.updateState\" placeholder=\"请选择更新状态\" style=\"width: 150px;margin-right: 20px;\"&gt; &lt;el-option label=\"未更新\" value=\"0\"&gt;&lt;/el-option&gt; &lt;el-option label=\"已更新\" value=\"1\"&gt;&lt;/el-option&gt; &lt;/el-select&gt; script中： // 根据更新状态进行筛选 if (updateState !== null) { filteredList = filteredList.filter( item =&gt; item.updateState === Number(updateState) ) } 运行效果图： 4.表格&lt;el-table :data=\"updateDataList.slice((queryInfo.pageNum-1)*queryInfo.pageSize,queryInfo.pageNum*queryInfo.pageSize)\" border stripe v-loading=\"loading\" highlight-current-row style=\"width: 100%\" &gt; &lt;el-table-column prop=\"appId\" width=\"100\"/&gt; &lt;el-table-column prop=\"equipmentName\" label=\"设备名称\" width=\"230\"/&gt; &lt;el-table-column prop=\"ipClient\" label=\"客户端IP\" width=\"230\"/&gt; &lt;el-table-column prop=\"updateState\" label=\"更新状态\" width=\"150\"/&gt; &lt;el-table-column prop=\"updateTime\" label=\"更新时间\" width=\"329\"&gt; &lt;template slot-scope=\"scope\"&gt; &lt;span&gt;{{ changeDateFormat(scope.row.updateTime) }}&lt;/span&gt; &lt;/template&gt; &lt;/el-table-column&gt; &lt;/el-table&gt;","categories":[],"tags":[]},{"title":"贪心算法","slug":"贪心算法","date":"2023-08-07T13:00:13.000Z","updated":"2023-09-11T06:47:05.821Z","comments":true,"path":"posts/17.html","link":"","permalink":"http://example.com/posts/17.html","excerpt":"","text":"贪心算法贪心算法求解的标准过程1,分析业务2,根据业务逻辑找到不同的贪心策略3,对于能举出反例的策略直接跳过，不能举出反例的策略要证明有效性这往往是特别困难的，要求数学能力很高且不具有统一的技巧性 从头到尾讲一道利用贪心算法求解的题目给定一个由字符串组成的数组strs,必须把所有的字符串拼接起来，返回所有可能的拼接结果中，字典序最小的结果 贪心算法的解题套路1,实现一个不依靠贪心策略的解法X,可以用最暴力的尝试2,脑补出贪心策略A、贪心策略B、贪心策略C3,用解法X和对数器，用实验的方式得知哪个贪心策略正确4,不要去纠结贪心策略的证明","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"flutter(三) -组件","slug":"flutter-组件","date":"2023-07-19T07:02:48.000Z","updated":"2023-07-19T08:49:08.173Z","comments":true,"path":"posts/16.html","link":"","permalink":"http://example.com/posts/16.html","excerpt":"","text":"Flutter – 组件无状态组件和有状态组件分类无状态组件（Stateless Widget）无状态组件是指在创建后不会发生变化的小部件。它们通常用于显示静态内容或根据传入的参数进行简单的渲染。 无状态组件的特点： 不会随时间变化而重新渲染。 只能接收外部传入的参数作为输入。 不会保存状态。 // 无状态组件(Stateless Widget) import 'package:flutter/material.dart'; import 'package:hospitalbalance/main.dart'; class MyText extends StatelessWidget { // 组件的参数 final String text; // 组件的构造函数 MyText({required this.text}); Widget build(BuildContext context) { return Container( child: Text( text, style: new TextStyle(fontSize: 40), ), ); } } // MyText 是一个无状态组件，它接收一个 title 参数，并在一个容器中显示该标题 有状态组件（Stateful Widget）有状态组件是指在创建后可以发生变化的小部件。它们通常用于包含交互性或需要根据用户操作更新的内容。 有状态组件的特点： 可以根据需要重新渲染。 可以保存和管理状态。 可以响应用户交互。 // 有状态组件(Stateful Widget) class MyHomePageWidget extends StatefulWidget { // 组件参数 final String title; // 构造 const MyHomePageWidget({Key key,this.title}): super(key: key); //没有build方法，但是createState()方法 _MyHomePageWidgetState createState() =&gt; new _MyHomePageWidgetState(); } class _MyHomePageWidgetState extends State&lt;MyHomePageWidget&gt; { // 状态 int counter = 0; void increaseCount(){ setState(() { this.counter++; }); } // build 方法 Widget build(context) { /*return new RaisedButton( onPressed: increaseCount, child: new Text('点击＋1'), ); */ // RaisedButton已过时 可用ElevatedButton替代 return Column( children:[ Text('Counter: $counter') ElevatedButton( onPressed: increaseCount, child: new Text('点击+1'), ), ], ); } } /* MyHomePageWidget 是一个有状态组件，它包含一个计数器变量 counter 和一个增加计数器的按钮。通过调用 setState 方法，可以更新状态并触发重新渲染。 */ 项目中的该如何选择？在实际项目中，无状态组件常用于显示静态内容，例如显示文本、图像或简单的布局。而有状态组件常用于包含交互性的部分，例如表单输入、动画效果或需要根据用户操作更新的内容。 例如，一个购物应用中的商品列表可以使用无状态组件展示每个商品的信息。而购物车部分可以使用有状态组件来管理商品的添加和删除操作，并根据购物车状态更新展示 无状态组件适合静态内容展示，而有状态组件适合交互性和动态内容的管理。根据项目需求，可以选择合适的组件类型来构建应用程序。 功能分类布局和容器类小部件： MaterialApp: 核心小部件 MaterialApp 是 Flutter 中的一个核心小部件，它用于创建一个遵循 Material Design 规范的应用程序。MaterialApp 提供了许多功能和属性，用于配置应用程序的外观、主题、路由和其他行为。 MaterialApp 的一些常用属性和功能： title：设置应用程序的标题，通常显示在设备的任务管理器中。 home：设置应用程序的主页小部件，将作为默认显示的页面。 routes：定义应用程序的命名路由，可以通过名称导航到不同的页面。 theme：设置应用程序的主题，包括颜色、字体和其他视觉属性。 darkTheme：设置应用程序的暗黑模式主题。 themeMode：设置应用程序的主题模式，可以是 ThemeMode.light（亮色模式）、ThemeMode.dark（暗黑模式）或 ThemeMode.system（跟随系统模式）。 initialRoute：设置应用程序的初始路由。 onGenerateRoute：定义一个回调函数，用于生成动态路由。 navigatorKey：设置导航键，用于在应用程序中执行导航操作。 debugShowCheckedModeBanner：设置是否显示调试模式的横幅。 例如： import 'package:flutter/material.dart'; void main() { runApp(MyApp()); } class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( title: 'My App', home: MyHomePage(), theme: ThemeData( primaryColor: Colors.blue, accentColor: Colors.orange, ), routes: { '/page1': (context) =&gt; Page1(), '/page2': (context) =&gt; Page2(), }, ); } } /* MyApp 是一个 MaterialApp，它设置了应用程序的标题、主题、主页和路由*/ class MyHomePage extends StatelessWidget { @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text('Home'), ), body: Center( child: Text('Welcome to my app!'), ), floatingActionButton: FloatingActionButton( onPressed: () { Navigator.pushNamed(context, '/page1'); }, child: Icon(Icons.arrow_forward), ), ); } } /* MyHomePage 是应用程序的主页小部件，其中包含一个浮动操作按钮，点击该按钮会导航到 /page1 路由对应的页面。*/ class Page1 extends StatelessWidget { @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text('Page 1'), ), body: Center( child: Text('This is Page 1'), ), ); } } class Page2 extends StatelessWidget { @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text('Page 2'), ), body: Center( child: Text('This is Page 2'), ), ); } } /* Page1 和 Page2 是两个简单的页面小部件，分别显示不同的文本内容 */ Scaffold ：home常用小部件之一 Scaffold 是 Flutter 中常用的小部件之一，它提供了一个基本的应用程序框架，用于构建具有标准布局结构的应用程序界面。 Scaffold 常用属性的详细用法及其功能： appBar: 用于设置应用程序的顶部栏，通常用于显示标题、操作按钮等。可以通过传递一个 AppBar 小部件来定义应用程序的顶部栏。 body: 用于设置应用程序的主要内容区域。可以将任何小部件作为 body 的子部件，例如 Container、ListView、Column 等。 floatingActionButton: 用于设置悬浮操作按钮，通常用于触发常用的操作。可以通过传递一个 FloatingActionButton 小部件来定义悬浮操作按钮。 drawer: 用于设置应用程序的侧边抽屉菜单。可以通过传递一个 Drawer 小部件来定义侧边抽屉菜单。 bottomNavigationBar: 用于设置应用程序的底部导航栏，通常用于切换不同的页面或功能。可以通过传递一个 BottomNavigationBar 小部件来定义底部导航栏。 backgroundColor: 用于设置应用程序的背景颜色。 resizeToAvoidBottomInset: 用于控制当键盘弹出时是否调整 body 部件的大小以避免被键盘覆盖，默认为 true。 extendBody: 用于控制 body 是否延伸到底部导航栏的区域，默认为 false。 extendBodyBehindAppBar: 用于控制 body 是否延伸到顶部栏的区域，默认为 false。 drawerScrimColor: 用于设置侧边抽屉菜单背后的蒙版颜色。 drawerEdgeDragWidth: 用于设置从屏幕边缘拖动以打开侧边抽屉菜单的宽度。 drawerEnableOpenDragGesture: 用于控制是否启用从屏幕边缘拖动以打开侧边抽屉菜单的手势，默认为 true。 Container：用于创建一个矩形容器。可以设置背景颜色、边框、填充等属性。 Container 常用属性： alignment（AlignmentGeometry 类型）：设置容器内部子部件的对齐方式。默认值是 Alignment.center，表示子部件居中对齐。 padding（EdgeInsetsGeometry 类型）：设置容器的内边距（填充）。可以使用 EdgeInsets 类来创建填充。填充会在容器的边界和子部件之间创建空白区域。 color（Color 类型）：设置容器的背景颜色。可以使用 Colors 类中的预定义颜色，也可以使用自定义颜色。 decoration（Decoration 类型）：设置容器的装饰，可以是背景图片、边框、阴影等。通常与 color 属性互斥，只能同时使用其中一个。 foregroundDecoration（Decoration 类型）：设置容器的前景装饰，可以在容器的子部件之前显示，类似于覆盖在容器上的装饰。 width（double 类型）：设置容器的宽度。 height（double 类型）：设置容器的高度。 constraints（BoxConstraints 类型）：设置容器的约束条件，用于控制容器的最小和最大宽度/高度。 margin（EdgeInsetsGeometry 类型）：设置容器的外边距。与 padding 不同，外边距是容器与其父部件之间的空白区域。 transform（Matrix4 类型）：设置容器的变换矩阵，可以实现平移、旋转、缩放等效果。 child（Widget 类型）：设置容器的子部件。可以是任何有效的小部件，用于填充容器。 例如： import 'package:flutter/material.dart'; class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( home: Scaffold( appBar: AppBar( title: Text('Container Demo'), ), body: Center( child: Container( // 创建了一个 Container width: 200, height: 200, //设置了宽度和高度为 200 像素 alignment: Alignment.center, // 将子部件居中对齐 padding: EdgeInsets.all(16), margin: EdgeInsets.all(16), // 内边距和外边距为 16 像素 color: Colors.blue, // 背景颜色为蓝色 decoration: BoxDecoration( // 设置了圆角和阴影效果 borderRadius: BorderRadius.circular(10), boxShadow: [ BoxShadow( color: Colors.black.withOpacity(0.3), spreadRadius: 2, blurRadius: 5, offset: Offset(0, 3), ), ], ), child: Text( // 放置了一个文本部件，并设置了文本的样式 'Hello, Container!', style: TextStyle( color: Colors.white, fontSize: 20, ), ), ), ), ), ); } } Row：用于在水平方向上排列子部件。例如： Row( children: &lt;Widget&gt;[ Text('Hello'), Text('World'), ], ) Column：用于在垂直方向上排列子部件。例如： Column( children: &lt;Widget&gt;[ Text('Hello'), Text('World'), ], ) Stack：用于将子部件堆叠在一起，可以通过定位子部件的方式来控制它们的位置。例如： Stack( children: &lt;Widget&gt;[ Positioned( top: 50, left: 50, child: Text('Hello'), ), Text('World'), ], ) ListView：用于在滚动视图中显示一个可滚动的列表，可以垂直或水平滚动。例如： ListView( children: &lt;Widget&gt;[ ListTile(title: Text('Item 1')), ListTile(title: Text('Item 2')), ListTile(title: Text('Item 3')), ], ) 点这里👉 查看ListTile详细用法 GridView：用于以网格形式显示子部件，可以根据交叉轴的数量自动调整布局。例如： GridView.count( crossAxisCount: 2, children: &lt;Widget&gt;[ Text('Item 1'), Text('Item 2'), Text('Item 3'), Text('Item 4'), ], ) Wrap：用于自动换行的布局，当子部件超出容器宽度时会自动换行。例如： Wrap( children: &lt;Widget&gt;[ Text('Item 1'), Text('Item 2'), Text('Item 3'), Text('Item 4'), ], ) Expanded：用于占据可用空间的部件，可以在 Row、Column 或 Flex 中使用。例如： Row( children: &lt;Widget&gt;[ Expanded(child: Text('Left')), Expanded(child: Text('Right')), ], ) AspectRatio：用于设置部件的宽高比。例如： AspectRatio( aspectRatio: 16 / 9, child: Image.network('https://example.com/image.jpg'), ) SafeArea: 自适应显示系统UI元素 SafeArea 的主要功能是自动适应不同设备的屏幕边缘，并根据设备的状态栏、导航栏等系统UI元素来调整应用程序的布局。它会在应用程序的内容周围创建一个安全区域，确保内容在可见区域内显示，避免被系统UI元素遮挡。 SafeArea 还提供了一些可选的参数，以进一步控制安全区域的行为，例如： left、top、right、bottom：可以指定在安全区域周围添加的边距，以便在内容和边缘之间创建间距。 minimum：可以指定安全区域的最小值，以确保内容不会被缩小到太小的尺寸。 maintainBottomViewPadding：默认情况下，SafeArea 会考虑底部的导航栏高度。如果将此参数设置为 false，则 SafeArea 将忽略底部的导航栏高度。 例如： SafeArea( left: true, top: true, right: true, bottom: true, minimum: EdgeInsets.all(16.0), maintainBottomViewPadding: false, child: Container( // 应用程序的内容 ), ) /* SafeArea 在内容周围添加了左、上、右、下的边距，并设置了最小值为 16.0，同时忽略了底部的导航栏高度。 */ Padding：用于在子部件周围添加填充。例如： Padding( padding: EdgeInsets.all(16.0), child: Text('Hello'), ) EdgeInsets.all(double value)：创建一个具有相同填充值的对称填充。所有四个边缘都将具有相同的填充值。 例如： Padding( padding: EdgeInsets.all(16.0), child: Text('Hello, World!'), ) // 在文本周围创建一个对称的 16.0 像素填充。 EdgeInsets.only({double left, double top, double right, double bottom})：创建具有指定边缘填充值的非对称填充。可以通过指定不同的边缘参数来设置不同的填充值。 例如： Padding( padding: EdgeInsets.only(left: 16.0, top: 8.0), child: Text('Hello, World!'), ) // 在文本的左侧应用 16.0 像素的填充，在文本的顶部应用 8.0 像素的填充 EdgeInsets.symmetric({double vertical, double horizontal})：创建具有对称填充值的垂直和水平填充。可以通过指定 vertical 和 horizontal 参数来设置垂直和水平方向上的填充值。 EdgeInsets.fromLTRB(double left, double top, double right, double bottom)：创建具有指定边缘填充值的非对称填充。可以通过指定左、上、右、下边缘的填充值来创建填充。 Align：用于根据指定的对齐方式对子部件进行对齐。例如： Align( alignment: Alignment.centerRight, child: Text('Hello'), ) Center：用于将子部件居中显示。例如： Center( child: Text('Hello'), ) SizedBox：用于指定固定大小的盒子。例如： SizedBox( width: 200, height: 200, child: Text('Hello'), ) 文本和样式类小部件： Text：用于显示文本内容。例如： Text('Hello World') RichText：用于显示富文本，可以对不同的文本片段应用不同的样式。例如： RichText( text: TextSpan( text: 'Hello', style: TextStyle(color: Colors.black), children: &lt;TextSpan&gt;[ TextSpan(text: ' World', style: TextStyle(fontWeight: FontWeight.bold)), ], ), ) SelectableText：用于显示可选中的文本。例如： SelectableText('Hello World') TextField：用于接收用户输入的文本框。例如： TextField( decoration: InputDecoration( labelText: 'Enter your name', ), ) TextFormField：用于包装 TextField，提供表单验证和处理功能。例如： TextFormField( decoration: InputDecoration( labelText: 'Enter your name', ), validator: (value) { if (value.isEmpty) { return 'Please enter your name'; } return null; }, onSaved: (value) { // 处理保存逻辑 }, ) DefaultTextStyle：用于设置默认文本样式。例如： DefaultTextStyle( style: TextStyle(fontSize: 20), child: Text('Hello'), ) TextStyle：用于定义文本的样式，如字体、颜色、大小等。例如： Text( 'Hello', style: TextStyle( fontSize: 20, fontWeight: FontWeight.bold, color: Colors.blue, ), ) TextSpan：用于在 RichText 中定义不同样式的文本片段。例如： RichText( text: TextSpan( text: 'Hello', style: TextStyle(color: Colors.black), children: &lt;TextSpan&gt;[ TextSpan(text: ' World', style: TextStyle(fontWeight: FontWeight.bold)), ], ), ) 按钮和交互类小部件： ElevatedButton：用于创建凸起的按钮。例如： ElevatedButton( onPressed: () { // 处理按钮点击事件 }, child: Text('Submit'), ) TextButton：用于创建一个文本按钮。例如： TextButton( onPressed: () { // 处理按钮点击事件 }, child: Text('Submit'), ) OutlinedButton：用于创建一个带有边框的按钮。例如： new Padding( padding: const EdgeInsets.fromLTRB(18.0, 0, 0, 25.0), child: new Row( children: &lt;Widget&gt;[ new Expanded( child: new OutlineButton( borderSide:new BorderSide(width: 2,color: Colors.black), child: new Text('登录',style: new TextStyle(color: Theme.of(context).primaryColor), ), onPressed: (){}, ) ), ], ), ), IconButton：用于创建带有图标的按钮。例如： IconButton( onPressed: () { // 处理按钮点击事件 }, icon: Icon(Icons.add), ) /* icon：指定按钮的图标，可以使用 Icon 小部件或自定义图标。 onPressed：指定按钮被点击时的回调函数，通常在此处执行按钮点击后的操作。 color：设置按钮的颜色。 tooltip：设置按钮的提示文本，当用户长按按钮时会显示该文本。 */ PopupMenuButton：用于显示一个弹出菜单。例如： PopupMenuButton&lt;String&gt;( itemBuilder: (context) =&gt; [ PopupMenuItem&lt;String&gt;( value: 'option1', child: Text('Option 1'), ), PopupMenuItem&lt;String&gt;( value: 'option2', child: Text('Option 2'), ), ], onSelected: (value) { // 处理菜单选项选择事件 }, ) DropdownButton：用于显示一个下拉菜单。例如： DropdownButton&lt;String&gt;( value: selectedValue, items: [ DropdownMenuItem&lt;String&gt;( value: 'option1', child: Text('Option 1'), ), DropdownMenuItem&lt;String&gt;( value: 'option2', child: Text('Option 2'), ), ], onChanged: (value) { setState(() { selectedValue = value; }); }, ) Checkbox：用于显示复选框。例如： Checkbox( value: isChecked, onChanged: (value) { setState(() { isChecked = value; }); }, ) Radio：用于显示单选按钮。例如： Radio( value: selectedValue, groupValue: groupValue, onChanged: (value) { setState(() { groupValue = value; }); }, ) Switch：用于显示开关按钮。例如： bool _switchValue = false; Switch( value: _switchValue, onChanged: (value) { setState(() { _switchValue = value; }); }, ) /* 创建了一个 Switch，初始状态为关闭（false）。通过设置 value 属性来指定当前开关的状态，并通过 onChanged 回调函数来处理开关状态的变化。 当用户点击开关时，onChanged 回调函数将被触发，并将新的开关状态传递给它。在回调函数中，您可以使用 setState 方法来更新状态变量 _switchValue，从而触发小部件的重建，以反映新的开关状态。 */ Switch 还提供了其他一些属性，例如： activeColor：指定开启状态下的颜色。 inactiveColor：指定关闭状态下的颜色。 activeTrackColor：指定开启状态下的轨道颜色。 inactiveTrackColor：指定关闭状态下的轨道颜色。 Slider （滑块）:用于创建一个可滑动的滑块，允许用户在一个范围内选择一个值。例如： double _sliderValue = 0.0; Slider( value: _sliderValue, min: 0.0, max: 100.0, onChanged: (value) { setState(() { _sliderValue = value; }); }, ) /* 创建了一个滑块，其值范围从 0.0 到 100.0。通过设置 `value` 属性来指定当前滑块的值，并通过 `onChanged` 回调函数来处理滑块值的变化。*/ GestureDetector（手势检测器）: 用于检测各种手势的小部件，例如点击、拖动、缩放等。例如： GestureDetector( onTap: () { // 处理点击事件 }, onDoubleTap: () { // 处理双击事件 }, onLongPress: () { // 处理长按事件 }, onPanUpdate: (details) { // 处理拖动事件 }, child: Container( width: 200, height: 200, color: Colors.blue, ), ) /* 创建了一个 `GestureDetector`，它包裹了一个蓝色的容器。通过设置不同的回调函数来处理不同的手势事件，例如 `onTap` 处理点击事件，`onDoubleTap` 处理双击事件，`onLongPress` 处理长按事件，`onPanUpdate` 处理拖动事件 */ InkWell（水波纹效果）: 用于创建带有水波纹效果的可点击部件。例如： InkWell( onTap: () { // 处理点击事件 }, child: Container( width: 200, height: 200, color: Colors.blue, child: Center( child: Text('Click Me'), ), ), ) /* 创建了一个带有水波纹效果的可点击容器。通过设置 onTap 回调函数来处理点击事件。在 InkWell 的子部件中，可以放置任何您想要的内容，例如文本、图像等。 InkWell 还提供了其他一些属性，例如 `onDoubleTap` 处理双击事件，`onLongPress` 处理长按事件，`onHighlightChanged` 处理高亮变化事件等。 */ 图像和图标类小部件： Image：用于显示图像。可以从网络加载图像或使用本地资源。例如： Image.network('https://example.com/image.jpg') Icon：用于显示图标。可以使用内置的图标或自定义图标。例如： Icon(Icons.star) CircleAvatar：用于显示圆形头像。例如： CircleAvatar( backgroundImage: NetworkImage('https://example.com/avatar.jpg'), ) ClipRRect：用于剪切子部件的圆角。例如： ClipRRect( borderRadius: BorderRadius.circular(10), child: Image.network('https://example.com/image.jpg'), ) Placeholder：用于占位的矩形框，通常用于正在加载图像时。例如： Placeholder( fallbackHeight: 200, fallbackWidth: 200, ) 对话框和提示类小部件： AlertDialog：用于显示一个对话框。例如： AlertDialog( title: Text('Alert'), content: Text('This is an alert dialog.'), actions: &lt;Widget&gt;[ TextButton( onPressed: () { // 处理按钮点击事件 }, child: Text('OK'), ), ], ) SnackBar：用于显示一个底部提示。例如： ScaffoldMessenger.of(context).showSnackBar( SnackBar( content: Text('This is a snackbar.'), action: SnackBarAction( label: 'OK', onPressed: () { // 处理按钮点击事件 }, ), ), ) 动画和过渡类小部件： AnimatedContainer：用于创建一个带有动画效果的容器。例如： AnimatedContainer( duration: Duration(seconds: 1), width: _width, height: _height, color: _color, ) AnimatedOpacity：用于创建一个带有透明度动画的部件。例如： AnimatedOpacity( opacity: _opacity, duration: Duration(seconds: 1), child: Text('Hello'), ) AnimatedBuilder：用于创建自定义动画。例如： AnimatedBuilder( animation: _controller, builder: (context, child) { return Transform.rotate( angle: _controller.value * 2 * pi, child: child, ); }, child: Icon(Icons.refresh), ) 滚动和可滚动类小部件： SingleChildScrollView：用于创建一个可以滚动的单个子部件。例如： SingleChildScrollView( child: Column( children: &lt;Widget&gt;[ Text('Item 1'), Text('Item 2'), Text('Item 3'), Text('Item 4'), ], ), ) ListView：用于在滚动视图中显示一个可滚动的列表。例如： ListView( children: &lt;Widget&gt;[ ListTile(title: Text('Item 1')), ListTile(title: Text('Item 2')), ListTile(title: Text('Item 3')), ], ) GridView：用于以网格形式显示子部件。例如： GridView.count( crossAxisCount: 2, children: &lt;Widget&gt;[ Text('Item 1'), Text('Item 2'), Text('Item 3'), Text('Item 4'), ], ) 项目案例：点这里👉 查看代码参考文档 class Product { const Product({required this.name}); final String name; } typedef CartChangedCallback = Function(Product product, bool inCart); // 函数类型CartChangedCallback表示购物车状态改变时的回调函数 class ShoppingListItem extends StatelessWidget { ShoppingListItem({ required this.product, required this.inCart, required this.onCartChanged, }) : super(key: ObjectKey(product)); // 购物列表项 final Product product; final bool inCart; final CartChangedCallback onCartChanged; Color _getColor(BuildContext context) { return inCart ? Colors.black54 : Theme.of(context).primaryColor; // 若商品在购物车内返回黑色半透明色，否则返回当前主题的主要颜色 } TextStyle? _getTextStyle(BuildContext context) { // 根据购物车状态获取文本样式 if (!inCart) return null; return const TextStyle( color: Colors.black54, decoration: TextDecoration.lineThrough, ); // 返回带有黑色删除线的黑色文本样式 } @override // 重写了 build 方法，用于构建购物列表项的界面。它返回一个 ListTile 小部件，包含商品的头像、名称和点击事件。 Widget build(BuildContext context) { return ListTile( onTap: () { onCartChanged(product, inCart); }, leading: CircleAvatar( backgroundColor: _getColor(context), child: Text(product.name[0]), ), title: Text(product.name, style: _getTextStyle(context)), ); } } void main() { runApp( MaterialApp( home: Scaffold( body: Center( child: ShoppingListItem( product: const Product(name: 'Chips'), inCart: true, onCartChanged: (product, inCart) {}, ), ), ), ), ); } 写到最后： 打开自己，接受事物原本的模样","categories":[],"tags":[{"name":"flutter","slug":"flutter","permalink":"http://example.com/tags/flutter/"}]},{"title":"算法基础(单调栈和滑动窗口)","slug":"算法基础-单调栈和滑动窗口","date":"2023-07-16T13:54:13.000Z","updated":"2023-07-16T14:15:32.953Z","comments":true,"path":"posts/15.html","link":"","permalink":"http://example.com/posts/15.html","excerpt":"","text":"单调栈和滑动窗口滑动窗口是什么滑动窗口是一种想象出来的数据结构：滑动窗口有左边界L和有边界R在数组或者字符串或者一个序列上，记为S,窗口就是S[L…R]这一部分L往右滑意味着一个样本出了窗口，R往右滑意味着一个样本进了窗口注：L和R都只能往右滑 滑动窗口能做什么？滑动窗口、首尾指针等技巧，是一种求解问题的流程设计 滑动内最大值和最小值的更新结构窗口不管L还是R滑动之后，都会让窗口呈现新状况，如何能够更快的得到窗口当前状况下的最大值和最小值？最好平均下来复杂度能做到0(1)利用单调双端队列","categories":[],"tags":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"flutter(二)","slug":"flutter-二","date":"2023-07-13T01:29:33.000Z","updated":"2023-07-19T07:23:10.721Z","comments":true,"path":"posts/14.html","link":"","permalink":"http://example.com/posts/14.html","excerpt":"","text":"Flutter基本操作一.build一个Flutter应用程序的build方法，用于构建应用程序的UI界面。包含如下组件 Widget build(BuildContext context) { return MaterialApp( debugShowCheckedModeBanner: false, home: Scaffold( appBar: AppBar( toolbarHeight: 200, backgroundColor: backgroundTransparent, elevation: 0.0, flexibleSpace: ClipPath( child: Container( height: 220, width: MediaQuery.of(context).size.width, decoration: const BoxDecoration( borderRadius: BorderRadius.only( topLeft: Radius.zero, topRight: Radius.zero, bottomLeft: Radius.circular(25), bottomRight: Radius.circular(25)), image: DecorationImage( image: AssetImage( 'images/home/drawable-xxxhdpi/xxx.png'), fit: BoxFit.fill), ), alignment: Alignment.bottomLeft, padding: const EdgeInsets.fromLTRB(18.0, 0, 0, 25.0), child: Image.asset('images/home/drawable-xxx/xxx.png'), ), ), ), body: AtmBalanceMainContainer( balanceType: balanceType, idenNoTXT: idenNoEdit, patientNoTXT: patientNoEdit ), bottomNavigationBar: const SpeachWidgeter(), // 修正了 \"SpeechWidgeter\" 的拼写 ), ); } return MaterialApp(...)：返回一个MaterialApp组件，它是一个根级别的组件，用于设置应用程序的主题和导航结构。 return GestureDetector(...)：处理手势操作的小组件 debugShowCheckedModeBanner: false：设置调试模式横幅的显示状态为false，以隐藏调试模式下的横幅。 home：属性用于指定应用程序的主页组件 Scaffold：Scaffold是一个常用的主页组件，提供了应用程序的基本布局结构，包括应用栏、主体内容和底部导航栏。 Container：Container是一个通用的容器组件，可以用作主页的根组件。它提供了对子组件的布局、绘制和装饰的能力。 自定义组件：你可以创建自定义的组件作为主页组件，根据应用程序的需求进行设计和实现。这些自定义组件可以是继承自StatelessWidget或StatefulWidget的类，用于构建特定的UI界面。 ListView：如果你希望主页是一个可滚动的列表视图，你可以使用ListView组件作为主页组件。ListView提供了垂直或水平滚动的能力，并可以包含多个子组件。 二. Scaffold常用属性 appBar: 一个常用的顶部应用栏组件，用于在应用程序的顶部显示标题、操作按钮和其他相关内容。以下是几种常见的AppBar组件： AppBar：AppBar是最基本的AppBar组件，它提供了一个顶部应用栏，通常包含一个标题和一些操作按钮。你可以通过设置title属性来指定标题文本，actions属性来指定操作按钮。 SliverAppBar：SliverAppBar是一个可滚动的AppBar组件，通常与CustomScrollView或NestedScrollView一起使用。它可以在滚动过程中收缩或展开，并提供了更多的自定义选项，如设置背景图像、设置展开高度等。 body: AtmBalanceMainContainer(...)：设置Scaffold的主体内容为一个名为AtmBalanceMainContainer的自定义组件，传递了一些参数。 BottomAppBar：BottomAppBar是一个位于底部的应用栏组件，通常与FloatingActionButton一起使用。它可以包含操作按钮、导航按钮和其他相关内容，并提供了一些布局选项，如设置底部导航栏的形状、设置导航按钮的位置等。 bottomNavigationBar: const SpeechWidgeter()：设置Scaffold的底部导航栏为一个名为SpeechWidgeter的自定义组件，使用const关键字创建一个常量组件。 三.AppBar AppBar组件常用的属性 自定义其外观和行为： backgroundColor：用于设置AppBar的背景颜色。你可以使用Color对象来指定颜色，例如：backgroundColor: Colors.blue。 toolbarHeight : toolbarHeight: 100：设置应用栏的高度为100像素 elevation：用于设置AppBar的阴影效果。你可以指定一个浮点数来控制阴影的高度，例如：elevation: 4.0。 title：用于设置AppBar的标题文本。你可以使用Text widget来指定标题文本，例如：title: Text('My App')。 **flexibleSpace : flexibleSpace: ClipPath(...)：设置应用栏的可变空间，使用ClipPath组件来裁剪子组件的形状。 actions：用于设置AppBar的操作按钮。你可以通过传递一个包含IconButton或其他Widget的列表来指定操作按钮，例如： actions: [ IconButton( icon: Icon(Icons.search), onPressed: () { // 处理按钮点击事件 }, ), IconButton( icon: Icon(Icons.settings), onPressed: () { // 处理按钮点击事件 }, ), ] ​ 7. leading：用于设置AppBar的导航按钮。你可以通过传递一个IconButton或其他Widget来指定导航按钮，例如： leading: IconButton( icon: Icon(Icons.menu), onPressed: () { // 处理导航按钮点击事件 }, ) centerTitle：用于控制AppBar标题是否居中显示。默认情况下，标题是左对齐的。你可以将centerTitle属性设置为true，使标题居中显示。例如： AppBar( title: Text('My App'), centerTitle: true, ) automaticallyImplyLeading：用于控制是否自动显示导航按钮。默认情况下，如果leading属性为空，AppBar会自动添加一个返回按钮作为导航按钮。你可以将automaticallyImplyLeading属性设置为false，禁止自动显示导航按钮。例如： AppBar( title: Text('My App'), automaticallyImplyLeading: false, ) 在这个例子中，即使没有设置leading属性，AppBar也不会自动添加导航按钮。 这些属性可以根据你的需求来进行组合和使用。例如，你可以将centerTitle设置为true，使标题居中显示，并将automaticallyImplyLeading设置为false，禁止自动显示导航按钮。这样可以创建一个居中标题且没有导航按钮的AppBar。 四.flexibleSpace: ClipPath属性 flexibleSpace: ClipPath(...)：设置应用栏的可变空间，使用ClipPath组件来裁剪子组件的形状。 child: Container(...)：作为ClipPath的子组件，创建一个容器用于显示应用栏的背景图像和其他内容。 height: 120：设置容器的高度为120像素。 width: MediaQuery.of(context).size.width：设置容器的宽度为屏幕的宽度，使用MediaQuery获取当前上下文中的屏幕尺寸。 decoration: const BoxDecoration(...)：设置容器的装饰，包括圆角边框和背景图像。 alignment: Alignment.bottomLeft：设置容器内部内容的对齐方式为左下角。 padding: const EdgeInsets.fromLTRB(18.0, 0, 0, 25.0)：设置容器内部内容的内边距，左边18像素，底部25像素。 child: Image.asset('images/home/drawable-xxxhdpi/xxx.png')：作为容器的子组件，显示一个图像，图像路径为’images/home/drawable-xxxhdpi/xxx.png’。 这段代码的作用是创建一个具有自定义样式的应用程序界面，包括一个顶部导航栏、一个主体内容区域和一个底部导航栏。其中，顶部导航栏包含一个背景图像和一个图标，主体内容区域使用了一个名为AtmBalanceMainContainer的自定义组件，底部导航栏使用了一个名为SpeechWidgeter的自定义组件。 五. decorationBoxDecoration构造方法：const BoxDecoration({ this.color, // 底色 this.image, // 图片 this.border, 边色 this.borderRadius, // 圆角度 this.boxShadow, // 阴影 this.gradient, // 渐变 this.backgroundBlendMode, // 混合Mode this.shape = BoxShape.rectangle, // 形状 }) 边框+圆角: decoration: new BoxDecoration( border: new Border.all(color: Color(0xFFFF0000), width: 0.5), // 边色与边宽度 color: Color(0xFF9E9E9E), // 底色 // borderRadius: new BorderRadius.circular((20.0)), // 圆角度 borderRadius: new BorderRadius.vertical(top: Radius.elliptical(20, 50)), // 也可控件一边圆角大小 ) 阴影： decoration: new BoxDecoration( border: new Border.all(color: Color(0xFFFF0000), width: 0.5), // 边色与边宽度 // 生成俩层阴影，一层绿，一层黄， 阴影位置由offset决定,阴影模糊层度由blurRadius大小决定（大就更透明更扩散），阴影模糊大小由spreadRadius决定 boxShadow: [BoxShadow(color: Color(0x99FFFF00), offset: Offset(5.0, 5.0), blurRadius: 10.0, spreadRadius: 2.0), BoxShadow(color: Color(0x9900FF00), offset: Offset(1.0, 1.0)), BoxShadow(color: Color(0xFF0000FF))], ) 形状（圆形与矩形）： decoration: new BoxDecoration( border: new Border.all(color: Color(0xFFFFFF00), width: 0.5), // 边色与边宽度 color: Color(0xFF9E9E9E), // 底色 // shape: BoxShape.circle, // 圆形，使用圆形时不可以使用borderRadius shape: BoxShape.rectangle, // 默认值也是矩形 borderRadius: new BorderRadius.circular((20.0)), // 圆角度 ) 渐变（环形、扫描式、线性）： decoration: new BoxDecoration( border: new Border.all(color: Color(0xFFFFFF00), width: 0.5), // 边色与边宽度 // 环形渲染 gradient: RadialGradient(colors: [Color(0xFFFFFF00), Color(0xFF00FF00), Color(0xFF00FFFF)],radius: 1, tileMode: TileMode.mirror) //扫描式渐变 // gradient: SweepGradient(colors: [Color(0xFFFFFF00), Color(0xFF00FF00), Color(0xFF00FFFF)], startAngle: 0.0, endAngle: 1*3.14) // 线性渐变 // gradient: LinearGradient(colors: [Color(0xFFFFFF00), Color(0xFF00FF00), Color(0xFF00FFFF)], begin: FractionalOffset(1, 0), end: FractionalOffset(0, 1)) ), 背景图像： decoration: new BoxDecoration( border: new Border.all(color: Color(0xFFFFFF00), width: 0.5), // 边色与边宽度 image: new DecorationImage( image: new NetworkImage('https://avatar.csdn.net/8/9/A/3_chenlove1.jpg'), // 网络图片 // image: new AssetImage('graphics/background.png'), 本地图片 fit: BoxFit.fill // 填满 // centerSlice: new Rect.fromLTRB(270.0, 180.0, 1360.0, 730.0),// 固定大小 ), ) ShapeDecoration构造方法：const ShapeDecoration({ this.color, this.image, this.gradient, this.shadows, @required this.shape, }) 除了shape，其他与BoxDecoration一致，shape研究： decoration: new ShapeDecoration( color: Color(0xFFFF00FF), // 底色 // 统一四边颜色和宽度 shape: Border.all(color: Color(0xFF00FFFF),style: BorderStyle.solid,width: 2) // 四个边分别指定颜色和宽度， 当只给bottom时与UnderlineInputBorder一致效果 // shape: Border(top: b, bottom: b, right: b, left: b) // 底部线 // shape: UnderlineInputBorder( borderSide:BorderSide(color: Color(0xFFFFFFFF), style: BorderStyle.solid, width: 2)) // 矩形边色 // shape: RoundedRectangleBorder(borderRadius: BorderRadius.all(Radius.circular(10)), side: BorderSide(color: Color(0xFFFFFFFF), style: BorderStyle.solid, width: 2)) // 圆形边色 // shape: CircleBorder(side: BorderSide(color: Color(0xFFFFFF00), style: BorderStyle.solid, width: 2)) // 体育场（竖向椭圆） // shape: StadiumBorder(side: BorderSide(width: 2, style: BorderStyle.solid, color: Color(0xFF00FFFF)) // 角形（八边角）边色 // shape: BeveledRectangleBorder(borderRadius: BorderRadius.all(Radius.circular(10)), side: BorderSide(color: Color(0xFFFFFFFF), style: BorderStyle.solid, width: 2)) ), FlutterLogoDecoration构造方法：const FlutterLogoDecoration({ this.lightColor = const Color(0xFF42A5F5), // Colors.blue[400] this.darkColor = const Color(0xFF0D47A1), // Colors.blue[900] this.textColor = const Color(0xFF616161), this.style = FlutterLogoStyle.markOnly, this.margin = EdgeInsets.zero, }) 不用解析，Flutter的logo，开发没啥用！ UnderlineTabindicator构造方法：const UnderlineTabIndicator({ this.borderSide = const BorderSide(width: 2.0, color: Colors.white), this.insets = EdgeInsets.zero, }) 这个比较简单就是加下划线，可以设置Insets值（控制下划高底，左右边距） decoration: new UnderlineTabIndicator( borderSide: BorderSide(width: 2.0, color: Colors.white), insets: EdgeInsets.fromLTRB(0,0,0,10) ), 写給我最爱的米兰·昆德拉：布拉格的大树开出了茂盛的花叶，林荫着历史著作中的过客","categories":[],"tags":[{"name":"flutter","slug":"flutter","permalink":"http://example.com/tags/flutter/"}]},{"title":"flutter(一)","slug":"flutter-一","date":"2023-07-11T03:19:26.000Z","updated":"2023-07-12T09:33:54.000Z","comments":true,"path":"posts/13.html","link":"","permalink":"http://example.com/posts/13.html","excerpt":"","text":"Flutter基础概念1.TextEditingController在Flutter中，TextEditingController 是一个用于控制文本输入框（Text Field）的控制器类。它提供了与文本输入框相关的操作和属性，用于管理输入框中的文本内容。 通过使用 TextEditingController，你可以实现以下功能： 获取和设置文本内容：你可以使用 TextEditingController 的 text 属性来获取或设置文本输入框中的文本内容。 监听文本变化：你可以通过添加文本变化的监听器来监听文本输入框中文本内容的变化。当用户输入或修改文本时，你可以在监听器中执行相应的操作。 清空文本内容：你可以使用 TextEditingController 的 clear() 方法来清空文本输入框中的文本内容。 管理焦点：你可以使用 TextEditingController 的 focusNode 属性来管理文本输入框的焦点。通过设置焦点节点，你可以控制文本输入框的焦点状态，例如获取焦点、失去焦点等。 销毁资源：在不使用 TextEditingController 时，你可以调用其 dispose() 方法来释放资源，防止内存泄漏。 2.StatelessWidget使用StatelessWidget可以方便地创建简单的静态UI组件，它们通常用于显示静态内容或者根据传入的属性进行简单的UI渲染。如果需要在UI中保存和更新状态，就需要使用StatefulWidget。 3.Flutter基础语法 创建 .dart文件 类名WhateverDart class WhateverDart { //动态变量定义 var和dynamic var vStr = \"字符串1\"; var vInt = 1; dynamic dStr = \"字符串2\" dynamic dInt = 2; // 数值定义 num包含int和double num n = 3; int i = 4; double d = 5.1; // 计算 print(d * i); // 20.4 print(d / i); //1.275 print(d ~/ i); // 取整 1 print(d % i); // 1.0999999999999996 print(\"d + i = ${d + i}\"); //字符串定义和拼接 String str1 = \"one-string\"; String str2 = \"two-string\"; String str3 = str1 + str2; //布尔值定义 bool b = true; // 集合定义 默认长度为0 List&lt;T&gt; list01 = new List&lt;T&gt;(3); List&lt;T&gt; list02 = new List&lt;T&gt;()..length=3; List&lt;String&gt; list03 = [\"str1\",\"str2\",\"str3\",\"str4\"]; //取值 print(\"第一个str的值为${list03[0]}\"); //取长度 print(list03.length); // 输出：4 //遍历 list03.forEach((str) =&gt; print(str)); // map映射 键值对 Map&lt;int ,String&gt; map ={}; // 定义空map Map&lt;int ,String&gt; map = new Map(); String mValue = map[2001]; // 取2001对应的值 map[2002] = 'new'; // 修改或添加 //若map中不存在key为apple的数据,则添加;已存在,不修改数据 map.putIfAbsent('apple',()=&gt; 5); bool hasOneKey = map.containsKey(2001); bool hasOneValue = map.containsValue('new'); //迭代键值对 map.forEach((key,value){print('$key:$value')}); //删除键值对 map.remove(2001); //取得所有的键或值 List&lt;int&gt; keys = map.keys.toList(); List&lt;String&gt; values = map.values.toList(); //Lambda表达式 Function func1 = () =&gt; print('随便写点'); Function func2 = (String str) =&gt; str.trim().toLowerCase(); // 无入参检查 容易bug // 下面是优化 String Function(String) func22 = (String str){ String res = str.trim().toLowerCase(); return res; }; String out = func22('hello world'); //修饰词 final String finalStr = '不可修改字符串'; static String staticStr = '静态字符串'; static final String sFStr = '不可修改的静态字符串'; // ！注意 一定是 static final这个顺序 String _privateStr = '私有字符串' // 注意变量前的 _ // 构造函数 ！一个类只能有一个且与类同名 WhateverDart(String [String str]){ if(str != null){ this._privateStr = str; } // 其他定义。。。 } // 方法私有化 void _method01(){ print('随便写点东西打印'); } // 方法async异步 方法参数后定义 void method02() async{ await print(\"初始化\"); //await等待 必须在async下使用 } // 函数入参定义 {}内参数位置可变 []可选参数一定放最后 static void method03(double param1,{String param2 , String param3},[int param4,String param5]){} // 调入方法 method03(12.5,param2:'aaa',param3:'bbb',22); } 创建新dart文件 SecondDart.dart 引入上定义的dart文件 //同一目录下默认引入的 import 'package:WhateverDart.dart'; //deffered as 指定懒加载 import 'package:WhateverDart.dart' deffered as lazy; //show 只引入指定文件和文件下的某些方法 import 'package:WhateverDart.dart' show WhateverDart,method01; //hide 指定文件除了文件下的某些方法其他都引入 import 'package:WhateverDart.dart' hide WhateverDart,method01; void main(){ //main方法 作为程序的入口 // 如果是懒更新 每次使用需调用 loadLibrary() await lazy.loadLibrary(); // for循环 for(int i = 0;i &lt; 5; i++){ if(i&gt;3){ print(\"i=${i}\"); break; } } // while循环 while(条件){ // 执行语句 } // 断言 !非真条件终止程序 assert(b); //switch选择 int i = 0; switch(i){ case 0: print(\"i = 0\"); break; case 1: print(\"i = 1\"); break; default: print(\"i=${i}\"); } }","categories":[],"tags":[{"name":"flutter","slug":"flutter","permalink":"http://example.com/tags/flutter/"}]},{"title":"flutter环境搭建处理错误","slug":"flutter环境搭建处理错误","date":"2023-07-10T08:12:15.000Z","updated":"2023-07-12T09:29:36.000Z","comments":true,"path":"posts/12.html","link":"","permalink":"http://example.com/posts/12.html","excerpt":"","text":"搭建常见问题 问题：What went wrong:Execution failed for task ‘:app:packageDebug’.&gt; A failure occurred while executing com.android.build.gradle.internal.tasks.Workers$ActionFacade &gt; com.android.ide.common.signing.KeytoolException: Failed to read key AndroidDebugKey from store “C:\\Users\\26676.android\\debug.keystore”: Integrity check failed: java.security.NoSuchAlgorithmException: Algorithm HmacPBESHA256 not available* 解决办法：删除文件C:\\Users\\26676.android\\debug.keystore","categories":[],"tags":[{"name":"flutter","slug":"flutter","permalink":"http://example.com/tags/flutter/"}]},{"title":"数据结构与算法(二叉树)","slug":"算法基础-二叉树","date":"2023-05-29T10:24:34.000Z","updated":"2023-05-29T15:05:58.000Z","comments":true,"path":"posts/11.html","link":"","permalink":"http://example.com/posts/11.html","excerpt":"","text":"算法基础篇 –二叉树递归序: 递归一条树，如果完全递归结束，每个节点会来到三次 递归序实现结果: {1 , 2 , 4 , 4 , 4 , 2 , 5 , 5 , 5 , 2 , 1 , 3 , 3 , 3 , 1} 先序(头左右) 得到结果: 1 , 2 , 4 , 5 , 3 中序(左头右) 得到结果: 4 , 2 , 5 , 1 , 3 后序(左右头) 得到结果: 4 , 5 , 2 , 3 , 1 非递归实现二叉树遍历：先序 头节点先入栈，while(栈不为空){ ①弹出打印； ②有有先压右 ③有左再压左 } 后序 (用到两个栈) 头右左: 头节点先入栈，while(栈不为空){ ① 弹出打印 ②有左先压左 ③有右再压右 } 再用第二个栈反转--&gt; 左右头 中序 while(头不为空 || 栈不为空){ ①整条左边界依次入栈 ②上步无法继续，弹出打印栈顶，新栈顶的右树继续① } 二叉树的遍历二叉树的宽度优先遍历 用队列实现 设置flag变量 来发现某一层结束 头节点入队列，while(队列不为空){ ①弹出打印 ②有做先加左 ③有右再加右 } 二叉树的序列化和反序列化二叉树的序列化是指将二叉树转换成一个字符串，以便可以方便地存储和传输二叉树结构。同时，序列化也使得二叉树可以被持久化到磁盘或数据库中。反之，反序列化就是将序列化后的字符串转换为原来的二叉树结构。 具体来说，二叉树的序列化通常使用先序遍历，对于当前节点，先序遍历先输出当前节点的值，然后再依次输出左子树和右子树。序列化字符串可以使用逗号或其他字符分隔节点值，例如：1,2,#,#,3,4,#,#,5,#,#。 反序列化二叉树时，可以采用递归或迭代的方式，从序列化字符串中取出当前节点的值，并分别递归构建左子树和右子树，直到遇到叶子节点或结束标记。 可用先序或后序或按层遍历来实现二叉树的序列化 注: 中序不行~ 比如如下两棵树 2 / 1 和 1 \\ 2 补足空位置的中序遍历结果都是{ null, 1, null, 2, null} 用什么方式序列化 就用什么方式来反序列化 先序序列化: public static Queue&lt;String&gt; preSerial(Node head) { Queue&lt;String&gt; ans = new LinkedList&lt;&gt;(); pres(head, ans); return ans; } public static void pres(Node head, Queue&lt;String&gt; ans) { if (head == null) { ans.add(null); } else { ans.add(String.valueOf(head.value)); pres(head.left, ans); pres(head.right, ans); } } public static Node buildByPreQueue(Queue&lt;String&gt; prelist) { if (prelist == null || prelist.size() == 0) { return null; } return preb(prelist); } public static Node preb(Queue&lt;String&gt; prelist) { String value = prelist.poll(); if (value == null) { return null; } Node head = new Node(Integer.valueOf(value)); head.left = preb(prelist); head.right = preb(prelist); return head; } 后序序列化： public static Queue&lt;String&gt; posSerial(Node head) { Queue&lt;String&gt; ans = new LinkedList&lt;&gt;(); poss(head, ans); return ans; } public static void poss(Node head, Queue&lt;String&gt; ans) { if (head == null) { ans.add(null); } else { poss(head.left, ans); poss(head.right, ans); ans.add(String.valueOf(head.value)); } } public static Node buildByPosQueue(Queue&lt;String&gt; poslist) { if (poslist == null || poslist.size() == 0) { return null; } // 左右中 -&gt; stack(中右左) Stack&lt;String&gt; stack = new Stack&lt;&gt;(); while (!poslist.isEmpty()) { stack.push(poslist.poll()); } return posb(stack); } public static Node posb(Stack&lt;String&gt; posstack) { String value = posstack.pop(); if (value == null) { return null; } Node head = new Node(Integer.valueOf(value)); head.right = posb(posstack); head.left = posb(posstack); return head; } 按层遍历序列化和反序列化: public static Queue&lt;String&gt; levelSerial(Node head) { Queue&lt;String&gt; ans = new LinkedList&lt;&gt;(); if (head == null) { ans.add(null); } else { ans.add(String.valueOf(head.value)); Queue&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.add(head); while (!queue.isEmpty()) { head = queue.poll(); if (head.left != null) { ans.add(String.valueOf(head.left.value)); queue.add(head.left); } else { ans.add(null); } if (head.right != null) { ans.add(String.valueOf(head.right.value)); queue.add(head.right); } else { ans.add(null); } } } return ans; } public static Node buildByLevelQueue(Queue&lt;String&gt; levelList) { if (levelList == null || levelList.size() == 0) { return null; } Node head = generateNode(levelList.poll()); Queue&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); if (head != null) { queue.add(head); } Node node = null; while (!queue.isEmpty()) { node = queue.poll(); node.left = generateNode(levelList.poll()); node.right = generateNode(levelList.poll()); if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } return head; } 打印整颗二叉树打印顺序为 右头左 public static void printTree(Node head) { System.out.println(\"Binary Tree:\"); printInOrder(head, 0, \"H\", 17); System.out.println(); } /** * * @param head: 当前节点 * @param height: 当前节点输出所在的层数 * @param to: ^作为左孩子，v作为右孩子 * @param len: 调整节点间距离 美观用 */ public static void printInOrder(Node head, int height, String to, int len) { if (head == null) { return; } printInOrder(head.right, height + 1, \"v\", len); String val = to + head.value + to; int lenM = val.length(); int lenL = (len - lenM) / 2; int lenR = len - lenM - lenL; val = getSpace(lenL) + val + getSpace(lenR); System.out.println(getSpace(height * len) + val); printInOrder(head.left, height + 1, \"^\", len); } public static String getSpace(int num) { String space = \" \"; StringBuffer buf = new StringBuffer(\"\"); for (int i = 0; i &lt; num; i++) { buf.append(space); } return buf.toString(); } 前驱And后继 在二叉树中，节点的前驱和后继是相对于 中序遍历顺序而言的。 节点的前驱是指在中序遍历中，该节点的前一个节点。如果该节点有左子树，则其前驱为左子树中的最右节点。如果该节点没有左子树，则需要向上遍历其父节点，直到找到一个节点为父节点的右孩子节点，那么该节点的父节点就是该节点的前驱。 节点的后继是指在中序遍历中，该节点的后一个节点。如果该节点有右子树，则其后继为右子树中的最左节点。如果该节点没有右子树，则需要向上遍历其父节点，直到找到一个节点为父节点的左孩子节点，那么该节点的父节点就是该节点的后继。 下面是一个二叉树的例子，节点 4 的前驱是 2，后继是 3。节点 6 的前驱是 5，后继是 7。 5 / \\ 2 7 / \\ / \\ &gt;1 4 6 8 \\ 3 &gt;中序遍历结果: 1 , 2 , 4 , 3 , 5 , 6 , 7 , 8 打印后继节点①有右子树，右树最左下的节点为后续节点 ②无右子树，找到一个结点是其父亲节点的左孩子，该父亲节点即为后继节点 *二叉树递归套路能解决的问题· 可解决面试绝大部分二叉树问题 尤其是树形dp问题； · 其本质利用二叉树递归遍历的便利性 ① 假设以X为头节点，假设可向X的左树和右树要信息; ② 在上述假设下，讨论以X为头节点的树 得到答案的可能性 ③ 列出所有的可能性后，确定需要向左树和右树要什么信息 ④把左树和右树的信息收集全，就是任何一颗子树要返回的信息 ⑤递归函数都返回了，每一个子树都这么要求 ⑥在代码中考虑中怎么把左树和右树信息整合成整棵树的信息 经典问题补充: 平衡二叉树: |左树高 - 右树高| &lt;= 1 搜索二叉树: 左孩子值&lt; 父值 &lt; 右孩子值 完全二叉树: 要么这一层全满，不满的层 从左到右也能变慢 · 给定树的头节点，返回整棵树是否是平衡二叉树 答案可能性 ：① 左树平 ② 右树平 ③ |左树高 - 右树高| &lt;= 1 要什么信息：是否平衡，高度 · 给定树的头节点，任何两节点间存在距离，求二叉树的最大距离 答案可能性：①与头节点无关，max(左树最大距离，右树最大距离) ② 与X相关，左高＋右高 要什么信息：最大距离，高度 MaxDistance = Math.max( Math.max(leftI.m,rightI.m), leftI.h+rightI.h+1) · 给定树的头节点，返回最大二叉搜索子树的头节点 答案可能性：① 与X无关 左树最大二叉搜索子树的头，size，右树最大二叉树搜索子树的头 ②与x相关(leftInfo .maxHead==X.left &amp;&amp; rightInfo .maxHead==X.right) 左为搜，右为搜， 左树max&lt; 头 &lt; 右树min 要什么信息：搜索二叉树节点个数，是否搜，max，min · 给定树的头节点，返回最大二叉搜索子树的节点个数 答案可能性：① 与X无关 左树最大二叉搜索子树的个数，右树最大二叉搜索子树的个数 ② 与X相关 左为搜，右为搜，左树max&lt; 头 &lt; 右树min 要什么信息：搜索二叉树节点个数，是否搜，max，min · 给定树的头节点，是否为满二叉树 答案可能性： 是满二叉树，高度l，节点数N，需满足(2^l) - 1 = N 要什么信息： 高度，节点数 · 给定树的头节点，是否为满二叉树 答案可能性：① 有右无左 返回false 否则继续 ② 一旦遇到左右孩子补全（有缺口）后序遇到的节点都必须为叶节点 ​ 1&gt; 满二叉树 无缺口 （左满，右满，左高==右高） ​ 2&gt;左树不全 （左完，右满，左高==右高+1） ​ 3&gt;左树满 右树满 高度差一 （左满，右满，左高 = =右高+1） ​ 4&gt;左树满，右树不全 （左满，右完，左高==右高） 要什么信息： 是否为完全二叉树，是否为满二叉树，左右树高度 · 给定树的头节点，任何两节点 a b， 返回 a b的最低公共祖先 答案可能性： · 给定树的头节点，求派对最大值问题，上级来 直属下级就不能过来 答案可能性：①与X无关，max(下级来，下级不来) ② 与X相关 下级不来 要什么信息：来or不来的快乐值 写到最好：允许别人做自己，允许自己做自己","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"算法基础(三)","slug":"算法基础-三","date":"2023-05-25T13:16:54.000Z","updated":"2023-07-08T07:58:28.000Z","comments":true,"path":"posts/10.html","link":"","permalink":"http://example.com/posts/10.html","excerpt":"","text":"算法基础篇 –递归何为递归 将一件大事分解成若干件小事，小事的答案通过决策汇集成大事的决策。 利用了系统栈。 ​ 系统栈（System stack）是一个存储用于方法执行的信息的内存区域，包括方法的参数、局部变量、返回地址等信息。每当一个方法被调用时，系统栈都会分配一定的内存空间来存储当前方法的信息，当方法执行完毕之后，系统会自动释放该空间。系统栈是一个后进先出（LIFO）的数据结构，也就意味着最后调用的方法会最先执行完毕并退出栈。如果系统栈的空间不足以存储当前方法的信息，会抛出栈溢出异常 ​ 也可用用迭代实现（for ，where） Master公式T（N） = aT(N/b) +O(N^d) a: 调用几次递归 b:子问题调用规模 （若为二分，则b=2） O(N^d): 其余除子问题调用的时间复杂度 时间复杂度的计算： log以b为底的a &gt; d, T(N)=O( N^( log以b为底的a) ) log以b为底的a &lt; d, T(N)=O(N^d) log以b为底的a = d, T(N)=O(N^d * logN) 致夏天： 夏天的黄昏悠长而宁静，我们看到夕阳慢慢落下，看到天空由金黄变成粉红","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"算法基础（二）","slug":"算法基础（二）","date":"2023-05-24T13:19:03.000Z","updated":"2023-05-25T14:19:14.000Z","comments":true,"path":"posts/9.html","link":"","permalink":"http://example.com/posts/9.html","excerpt":"","text":"算法基础链表 数据结构常用技巧 ： 双指针 快慢指针 哈希表，数组 改原链表的方法要注意边界 ①单双链表的反转//单链表反转 public static Node reverseLinkedList(Node head) { Node pre = null; Node next = null; while (head != null) { next = head.next; head.next = pre; pre = head; head = next; } return pre; } //双链表反转 public static DoubleNode reverseDoubleLinkedList(DoubleNode head) { DoubleNode next = null; DoubleNode pre = null; while (head != null) { next = head.next; head.next = pre; head.last = next; pre = head; head = next; } return pre; } ②给定值删除//单链表删值 可能删除头节点 所以返回node类型 public static ListNode removeElements(ListNode head, int num) { while (head != null) { if (head.val != num) { break; } head = head.next; } //head 指向第一个不为num值的节点 ListNode pre = head; //存放上一个不为num的节点 ListNode cur = head; while (cur != null) { if (cur.val == num) { pre.next = cur.next; } else { pre = cur; } cur = cur.next; } return head; } ③将单链表按值划分成 大于 小于 等于区 1&gt; 将链表放入数组中 经行partition 2&gt; 分成大 中 小区，再将各种分串起来 ④判断链表是否是回文结构 1&gt; 栈 2&gt; 改原链表（快慢指针），再将各自串起来 ⑤复制链表含rand指针（无环）要求 时间复杂度 O（N） 额外空间复杂度O(1) 1&gt; hashMap 2&gt; 先拷贝出新节点，再断链 。eg: 1 - &gt;1’ - &gt; 2 -&gt; 2’ -&gt; 3 -&gt; 3’ -&gt;null ⑥两链表可能有环 可能无环，求两相交的第一个节点，若无返回null要求 时间复杂度O（N） 空间复杂度O(1) 注： 相交的节点 与值无关，与地址相关 hashset 先判断是否有环，若无环则返回null ，有环利用快慢指针，返回第一个入环节点 ​ 快慢指针: ​ ① 快指针走2，慢指针走1，到第一次相遇点； ​ ② 快指针回到头，慢不动，快指针走1，慢指针走 1，再次相遇为入环点 若都无环end1 = end2 则必相交，再从头节点开始比较 若一有环，一无环，则比不相交 若都有环 （相交必拥有公共环） ① 不相交 loop1 ！= loop2（入环节点不相等）return null； ② 入环节点相同 loop1 == loop2，计算cur1和cur2分别走到loop节点距离，长距离先走，保证到loop的距离相等，同步到第一次相遇点返回 ③ 入环节点不同 loop1 ！= loop2 ​ cur1==loop2时 返回loop1 ⑦不给点链表头节点 ，只給某个节点 就可再链表上删除它借尸还魂: 将下一节点的值放在当前位置，当前节点指向下下一个节点 可能会出现的问题: ①节点还在，只是内容被替代 ②若copy函数复杂或值复杂 可能无法覆盖 ③无法删除链表最后一节点 栈类似弹夹，所有数据操作均为O(1） Java里的创建: ↓ （慢）：Stack stack=new Stack&lt;&gt;(); stack.add(1);…..System.out.println(stack.pop()); ↓ （快）：LinkedList s=new LinkedLIst&lt;&gt;(); s.addlast(1);……System.out.println(stack.poplast()); ↓（更快）：int[] a=new int[]；int index=0; a[index++]=a;……System.out.println(a[–index]); ①数组实现栈public class ArrayStack { private int[] stack; private int top; private int size; public ArrayStack(int size) { this.size = size; stack = new int[size]; top = -1; } public void push(int value) { if (top == size - 1) { System.out.println(\"Stack is full\"); return; } top++; stack[top] = value; } public int pop() { if (top == -1) { System.out.println(\"Stack is empty\"); return -1; } int value = stack[top]; top--; return value; } public int peek() { if (top == -1) { System.out.println(\"Stack is empty\"); return -1; } return stack[top]; } public boolean isEmpty() { return top == -1; } public boolean isFull() { return top == size - 1; } } ②利用栈思维数组实现逆序打印//数组模拟栈 前提：容量确定 int[] arr = new int[10]; //填充数组，有数据则直接跳过 int index = 0; for (int i = 0; i &lt; arr.length; i++) { arr[i++] = ++index; System.out.print(index+\" \"); } System.out.println(); for (int i = arr.length-1; i &gt;= 0; i--){ System.out.print( arr[--i]+\" \"); } ③栈获得最小值public static class MyStack2 { private Stack&lt;Integer&gt; stackData; // 记录每个加入栈的数据 private Stack&lt;Integer&gt; stackMin; // 记录每一level下的最小值 public MyStack2() { this.stackData = new Stack&lt;Integer&gt;(); this.stackMin = new Stack&lt;Integer&gt;(); } public void push(int newNum) { if (this.stackMin.isEmpty()) { // 此时小栈空 加入当前值 this.stackMin.push(newNum); } else if (newNum &lt; this.getmin()) { // 小于此时小栈顶值 加入当前值 this.stackMin.push(newNum); } else { // 不小于此时小栈顶值 加入小栈顶值 int newMin = this.stackMin.peek(); this.stackMin.push(newMin); } this.stackData.push(newNum); // 别忘记更新栈顶值 } public int pop() { if (this.stackData.isEmpty()) { throw new RuntimeException(\"Your stack is empty.\"); } this.stackMin.pop(); return this.stackData.pop(); } public int getmin() { if (this.stackMin.isEmpty()) { throw new RuntimeException(\"Your stack is empty.\"); } return this.stackMin.peek(); } } ④ 栈结构实现队列原理：push管中的数据一次性倒空給pop管，pop再全部弹出 public static class TwoStacksQueue { public Stack&lt;Integer&gt; stackPush; public Stack&lt;Integer&gt; stackPop; public TwoStacksQueue() { stackPush = new Stack&lt;Integer&gt;(); stackPop = new Stack&lt;Integer&gt;(); } // push栈向pop栈倒入数据 private void pushToPop() { if (stackPop.empty()) { while (!stackPush.empty()) { stackPop.push(stackPush.pop()); } } } public void add(int pushInt) { stackPush.push(pushInt); pushToPop(); } public int poll() { if (stackPop.empty() &amp;&amp; stackPush.empty()) { throw new RuntimeException(\"Queue is empty!\"); } pushToPop(); return stackPop.pop(); } public int peek() { if (stackPop.empty() &amp;&amp; stackPush.empty()) { throw new RuntimeException(\"Queue is empty!\"); } pushToPop(); return stackPop.peek(); } } 队列同队列，所有操作都是O(1） Java里的使用： Queue queue = new LinkedList(); ①数组实现队列public class ArrayQueue { private int[] arr; private int pushi; private int polli; private int size; private final int limit; public MyQueue(int limit) { arr = new int[limit]; polli = 0; pushi = 0; size = 0; this.limit = limit; } //增加 public void push(int value) { if (size == limit) { throw new RuntimeException(\"队列满啦~ \"); } size++; arr[pushi] = value; pushi = getNextIndex(pushi); } //返回i的下一个节点位置 private int getNextIndex(int i) { return i &lt; limit - 1 ? i + 1 : 0; } //弹出 public int pop() { if (size == 0) { throw new RuntimeException(\"队列为空~\"); } size--; int ans = arr[polli]; polli = getNextIndex(polli); return ans; } } ②两队列实现栈原理: queue依次弹出 只留一个元素 給help队列 最后将queue队列中仅剩的一个元素弹出 queue和help交换 重复上面步骤直至queue队列为空 public static class TwoQueueStack&lt;T&gt; { public Queue&lt;T&gt; queue; public Queue&lt;T&gt; help; public TwoQueueStack() { queue = new LinkedList&lt;&gt;(); help = new LinkedList&lt;&gt;(); } public void push(T value) { queue.offer(value); } public T poll() { while (queue.size() &gt; 1) { help.offer(queue.poll()); } T ans = queue.poll(); Queue&lt;T&gt; tmp = queue; queue = help; help = tmp; return ans; } public T peek() { while (queue.size() &gt; 1) { help.offer(queue.poll()); } T ans = queue.poll(); help.offer(ans); Queue&lt;T&gt; tmp = queue; queue = help; help = tmp; return ans; } public boolean isEmpty() { return queue.isEmpty(); } } 拓展在图的章节中: 宽度优先遍历 –队列 深度优先遍历 –栈 写到最后: 当决定见面 玫瑰的也就降下来了","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"算法基础（一）","slug":"算法基础（一）","date":"2023-03-04T10:39:22.000Z","updated":"2023-03-04T15:09:16.000Z","comments":true,"path":"posts/8.html","link":"","permalink":"http://example.com/posts/8.html","excerpt":"","text":"算法基础计算机数： 原码：最高位为符号位，0正1负反码：正数与原码一致；负数最高位不变，其余对源码按位取反补码：正数与原码一致；负数为该数的反码加1 与（&amp;）：全1为1 或（ | ）：有1为1 异或（ ^ )–无进位相加：不同为1 ①打印32位二进制数 public static void print(int num) { for (int i = 31; i &gt;= 0; i--) { System.out.print((num &amp; (1 &lt;&lt; i)) == 0 ? \"0\" : \"1\"); } System.out.println(); } ②异或的玩法 N^0==N N^N==0 //nums 中只有一种数出现了奇数次； public static void printOddTimesNum1(int[] nums) { int orc = 0; //异或值。 for (int i = 0; i &lt; nums.length; i++) { orc ^= nums[i]; } System.out.println(orc); } //nums 中有两种数出现了奇数次； public static void printOddTimesNum2(int[] nums) { int eor = 0; int otherEor = 0; for (int i = 0; i &lt; nums.length; i++) { eor ^= nums[i]; } int moreRight = eor &amp; ((~eor) + 1); //提取除最右侧出现1 其余位置为0的数 for (int i = 0; i &lt; nums.length; i++) { if ((moreRight &amp; nums[i]) != 0) otherEor ^= nums[i]; } eor ^= otherEor; System.out.println(\"one is: \" + eor + \" other is: \" + otherEor); } ③数组长度为N，可存放（0~2^n-1）个数。 public static class BitMap { private long[] bits; public BitMap(int max) { //假设max为170 则存在 bits[2] 中的第42位置上 //(max+64)&gt;&gt;6 --&gt; (max+64)/64 bits = new long[(max + 64) &gt;&gt; 6]; } public void add(int num) { //举例: 0000 0010 | 0000 1000 = 0000 1010 //(num&amp;63) --&gt; num%64 bits[num &gt;&gt; 6] |= (1L &lt;&lt; (num &amp; 63)); } public void delete(int num) { //举例: 0000 1010 &amp; (~0000 1000) = 0000 0010 bits[num &gt;&gt; 6] &amp;= ~(1L &lt;&lt; (num &amp; 63)); } public boolean contains(int num) { //举例: 0000 0010 &amp; 0000 1000 = 0 则8这个数字不存在。 return (bits[num &gt;&gt; 6] &amp; (1L &lt;&lt; (num &amp; 63))) != 0; } } 计算机里的加减乘除//a+b==无符号进位＋进位信息 public static int add(int a, int b) { int res = a; while (b != 0) { res = a ^ b; b = (a &amp; b) &lt;&lt; 1; //进位信息 b-&gt;b' (!需要与完左移一位) a = res; //无符号进位 a-&gt;a' } return res; } //减：变相的加 public static int minus(int a, int b) { // a-b=a+(-b)=a+(~b+1) return add(a, add(~b, 1)); } //乘：b的最低位&amp;1不为0 则总和加上a，之后a左移 b不带符号右移，再重复前面的操作。 public static int multi(int a, int b) { int res = 0; while (b != 0) { if ((b &amp; 1) != 0) { res = add(res, a); } a = a &lt;&lt; 1; b &gt;&gt;&gt;= 1; } return res; } 不简单的—除//判断是否为负数 是返回true public static boolean isNeg(int a) { return a &lt; 0; } //取得相反数 public static int negNum(int a) { // a-&gt; -a -&gt; ~a+1 return add(~a, 1); } //除：y左移找接近x的值，相减、 public static int div(int a, int b) { int res = 0; int x = isNeg(a) ? negNum(a) : a; int y = isNeg(b) ? negNum(b) : b; for (int i = 30; i &gt;= 0; i = minus(i, 1)) { if ((x &gt;&gt; i) &gt;= y) { res |= (1 &lt;&lt; i); x = minus(x, y &lt;&lt; i); } } // != --&gt; ^ return isNeg(a) ^ isNeg(b) ? negNum(res) : res; } //返回除 包含特殊情况 public static int divide(int a, int b) { //a-最小 b-最小 //a-不是最小 b-最小 //a-最小 b-随便 //a-bushi最小 b-不是最小 if (a == Integer.MIN_VALUE &amp;&amp; b == Integer.MIN_VALUE) return 1; else if (b == Integer.MIN_VALUE) return 0; else if (a == Integer.MIN_VALUE) { if (b == negNum(1)) return Integer.MAX_VALUE; else { // (a+1)/b=c // 计算补长： d=a-b*c // e=d/b // c+e int c = div(add(a, 1), b); return add(c, div(minus(a, multi(b, c)), b)); } } else return div(a, b); } 前缀树和矩阵求和： 前缀和数组：适用于原数组较大， 矩阵求和：查询次数多，占用空间大 需要根据实际情况做出选择~ //矩阵求和 public static class RangeSum1 { int[] arr; public RangeSum1(int[] arr) { this.arr = arr; } public int rangeSum(int L, int R) { int sum = 0; for (int i = L; i &lt;= R; i++) { sum += arr[i]; } return sum; } } //前缀和数组 public static class RangeSum2 { int[] presum; public RangeSum2(int[] arrary) { int N = arrary.length; presum = new int[N]; presum[0] = arrary[0]; for (int i = 1; i &lt; N; i++) { presum[i] = presum[i - 1] + arrary[i]; } } public int rangeSum(int L, int R) { return L == 0 ? presum[R] : presum[R] - presum[L - 1]; } 计算机里随机概率 Math.random() -&gt; double -&gt; [0，1) Math.random()*n -&gt; double -&gt; [0，n) (int)(Math.random()*k) -&gt; int -&gt; [0，k-1] //k为整数 0,1等概率法 ① 选定数组（越简单越好） ②确定0，1的概率 ③将所求的的范围 改成[0,x],其中x为二进制位所在的长度，如17~56.则范围改为[0,63] ④将所求的的范围 改成[0,范围差] ⑤ 上一步加起始值 ①求到17~56的等概率 public static int h1(){ return (int)(Math.random()*4+1); //返回[1,4] } public static int h2(){ int ans=h1(); return ans&lt;3 ?0:1; } //求得000000~111111 0~63的等概率 public static int h3(){ return (h2()&lt;&lt;5)+(h2()&lt;&lt;4)+(h2()&lt;&lt;3)+(h2()&lt;&lt;2)+(h2()&lt;&lt;1)+h2(); } public static int h4(){ int ans=0; do { ans=h3(); }while (ans&gt;39); return ans; } public static int hh(){ return h4()+17; } 哈希表 and 有序表 哈希表（k,v）表： 1，无论曾删改查 时间复杂度都是O（1），但要比数组的查找慢； 2，基础类型（Integer，String,Char,等）按值查询，只看值，不看地址 3，非基础类型（自定义类型）按引用传递，看地址。 4，基础类型所占的字节数 map&lt; Integer,String &gt;,为其Integer和String的内容所占字节和 5，自定义类型字节数 map&lt; Node,Node &gt;,其一个Node为一个地址，所占8字节。 常用功能： put ,get, remove, containsKey 有序表 TreeMap 相较于哈希表新增如下功能： 0，时间复杂度O（logN），且暂不支持自定义类型，除非类型已排序。 1，查找key最小 -&gt; treeMap.firstKey() 2，查找key最大 -&gt; treeMap.lastKey() 3，查找&lt;=num最近的key -&gt; treeMap.floorKey(num) 4，查找&gt;=num最近的key -&gt; treeMap.ceilingKey(num) 排序：选择排序：public static void selectSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; int N = arr.length; for (int i = 0; i &lt; N; i++) { int minValueIndex = i; for (int j = i + 1; j &lt; N; j++) { minValueIndex = arr[j] &lt; arr[minValueIndex] ? j : minValueIndex; } swap(arr, minValueIndex, i); } } 冒泡排序：public static void bubbleSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; int N = arr.length; //0~n-1,0~n-2,0~end for (int end = N - 1; end &gt;= 0; end--) { //0 1 1 2 2 3 3 4 end-1 end for (int j = 1; j &lt;= end; j++) { if (arr[j - 1] &gt; arr[j]) { swap(arr, j - 1, j); } } } } 插入排序：public static void insertSort(int[] arr) { if (arr == null &amp;&amp; arr.length &lt; 2) return; int N = arr.length; for (int end = 1; end &lt; N; end++) { for (int pre = end - 1; pre &gt;= 0 &amp;&amp; arr[pre] &gt; arr[pre + 1]; pre--) { swap(arr, pre, pre + 1); } } } 归并排序：public static void mergerSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; process(arr, 0, arr.length - 1); } // 2*T(N/2)+O(N) -&gt; log22==1 -&gt; O(N*log22)==O(NlogN) public static void process(int[] arr, int L, int R) { if (L == R) { return; } int mid = L + ((R - L) &gt;&gt; 1); process(arr, L, mid); process(arr, mid + 1, R); merge(arr, L, mid, R); } // 求左小和 public static void merge(int[] arr, int L, int mid, int R) { int[] help = new int[R - L + 1]; int i = 0; int left = L; int right = mid + 1; while (left &lt;= mid &amp;&amp; right &lt;= R) { help[i++] = arr[left] &lt;= arr[right] ? arr[left++] : arr[right++]; } while (left &lt;= mid) { help[i++] = arr[left++]; } while (right &lt;= R) { help[i++] = arr[right++]; } for (int j = 0; j &lt; help.length; j++) { arr[L + j] = help[j]; } } 随机快排：// 目标：&lt;= &gt; 区间内无序 public static int partition(int[] arr, int L, int R) { if (L == R) { return L; } else if (L &gt; R) return -1; int minR = L - 1; int i = 0; int num = arr.length - 1; while (i &lt; R) { if (arr[i] &lt; arr[num]) { swap(arr, i, ++minR); } i++; } swap(arr, num, ++minR); //！切勿忘记将最后一位的排序 return minR; } //荷兰国旗问题 目标：&lt; = &gt; ,返回相等区域的下标的范围 public static int[] netherlandsFlag(int[] arr, int L, int R) { if (L == R) { return new int[]{L, R}; } else if (L &gt; R) { return new int[]{-1, -1}; } int minR = L - 1; //小于区 int maxL = R; //大于区 int index = L; while (index &lt; maxL) { //i不撞上大于区，继续 if (arr[index] &lt; arr[R]) { swap(arr, index++, ++minR); } else if (arr[index] &gt; arr[R]) { swap(arr, index, --maxL); } else { index++; } } swap(arr, R, maxL);//！切勿忘记将最后一位的排序 return new int[]{minR + 1, maxL}; } //荷兰国旗解快排的改善 O(NlogN) public static void quickSort3(int[] arr) { if (arr == null || arr.length &lt; 2) return; process3(arr, 0, arr.length - 1); } public static void process3(int[] arr, int L, int R) { if (L &gt;= R) return; swap(arr, R, L + (int) (Math.random() * (R - L + 1))); int[] M = netherlandsFlag(arr, L, R); //随机选值放到R位置上 process3(arr, L, M[0] - 1); process3(arr, M[1] + 1, R); } public static void swap(int[] arr, int i, int j) { int t = arr[i]; arr[i] = arr[j]; arr[j] = t; } 堆排序：//↑ public static void heapInset(int[] arr,int index){ while (arr[index]&gt;arr[(index-1)/2]){ swap(arr,index,(index-1)/2); index=(index-1)/2; } } //↓ public static void heapify(int[] arr,int index,int heapSize){ int leftChild=2*index+1; while (leftChild&lt;heapSize){ int largest=leftChild+1&lt;heapSize &amp;&amp; arr[leftChild]&lt;arr[leftChild+1]?leftChild+1:leftChild; largest=arr[largest]&lt;arr[index]?index:largest; if (largest==index) break; swap(arr,largest,index); index=largest; leftChild=2*index+1; } } //堆排序 额外空间复杂度 O(1) public static void heapSort(int[] arr ){ if (arr==null ||arr.length&lt;2) return; /* 可以一次次加入值到数组中 O(N*logN) for (int i = 0; i &lt; arr.length; i++) { //O(N) heapInset(arr,i); //O(logN) } */ int heapSize=arr.length; // 前提: 一次性给全要排序的数组 // 目的: 让数组变为堆 // 从下往上调整每个节点下都为大根堆，优化后O(N) for (int i = arr.length-1;i&gt;=0; i--) { heapify(arr,i,heapSize); } swap(arr,0,--heapSize); //O(1) //log(N*logN) while (heapSize&gt;0){ //O(N) heapify(arr,0,heapSize); //O(logN) swap(arr,0,--heapSize); //O(1) } } public static void swap(int[] arr, int a, int b) { int t = arr[a]; arr[a] = arr[b]; arr[b] = t; } 计数排序：可实现value的值为0~200；实质是加入了max个桶public static void countSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; arr.length; i++) { max = Math.max(max, arr[i]); } int[] bucket = new int[max + 1]; for (int i = 0; i &lt; arr.length; i++) { bucket[arr[i]]++; } for (int i = 0, Index = 0; i &lt; bucket.length; i++) { while (bucket[i]-- &gt; 0) { arr[Index++] = i; } } 基数排序：实质：从个位开始逐步比较public static void radixSort(int[] arr) { if (arr == null || arr.length &lt; 2) return; radixSort(arr, 0, arr.length - 1, maxbits(arr)); } //计算最大数的位数 public static int maxbits(int[] arr) { int max = Integer.MIN_VALUE; for (int i = 0; i &lt; arr.length; i++) { max = Math.max(max, arr[i]); } int res = 0; while (max != 0) { max /= 10; res++; } return res; } //digit: 为L...R上最大值的位数。 public static void radixSort(int[] arr, int L, int R, int digit) { final int radix = 10; //桶的个数 int[] count; int bb = 0; //bb为count的下标 int[] help = new int[R-L+1]; for (int d = 1; d &lt;= digit; d++) { count = new int[radix]; // 取个位/十位上的数 for (int i = L; i &lt;= R; i++) { bb = getDigit(arr[i], d); count[bb]++; } // 求前缀和 for (int i = 1; i &lt; radix; i++) { count[i] = count[i] + count[i - 1]; } //从右往左满足条件弹出 for (int i = R; i &gt;= L; i--) { bb = getDigit(arr[i], d); help[count[bb]-1] = arr[i]; count[bb]--; } //加入到arr中 for (int index = 0; index &lt; help.length; index++) { arr[index] = help[index]; } } } public static int getDigit(int value, int digit) { return (int) (value / Math.pow(10, digit - 1)) % 10; } 排序总结： 排序 时间复杂度 额外空间复杂度 稳定性 选择排序 O(N^2) O(1) 无 冒泡排序 O(N^2) O(1) 有 插入排序 O(N^2) O(1) yeah 归并排序 O(NlogN) O(N) yeah 随机快排 O(NlogN) O(logN) none 堆排序 O(NlogN) O(1) none 计数排序 O(N) O(M) 有 基数排序 O(N) O(N) 有 二分：①在有序数组中查找num是否存在 //arr保证有序 public static boolean find(int[] arr, int num) { if (arr == null || arr.length == 0) { return false; } int L = 0; int R = arr.length - 1; while (L &lt;= R) { int mid = (L + R) / 2; if (arr[mid] == num) return true; else if (arr[mid] &lt; num) { L = mid + 1; } else R = mid - 1; } return false; } ②arr有序，查大于等value最左位置 public static int nearestIndex(int[] arr, int value) { if (arr == null || arr.length == 0) return -1; int N = arr.length; int L = 0; //左边界 int R = N - 1; //右边界 int index = -1; // 大于等于value的位置。 while (L &lt;= R) { int mid = (L + R) / 2; if (arr[mid] &gt;= value) { R = mid - 1; index = mid; } else L = mid + 1; } return index; } ③arr无序，相邻不等，求局部最小值 public static int oneMinIndex(int[] arr) { if (arr == null || arr.length == 0) { return -1; } if (arr.length == 1) { return 0; } //最左为小值 if (arr[0] &lt; arr[1]) { return 0; } //最右为小值 if (arr[arr.length - 1] &lt; arr[arr.length - 2]) { return arr.length - 1; } int left = 0; int right = arr.length - 1; while (left &lt; right - 1) {//保证有三个数在比较 int mid = (left + right) / 2; //4种情况 ① 左&gt;我 我&lt;右 (符合) //2,左&gt;我 我&gt;右 （砍大） //3,左&lt;我 我&lt;右 //4,左&lt;我 我&gt;右 if (arr[mid] &lt; arr[mid - 1] &amp;&amp; arr[mid] &lt; arr[mid + 1]) { return mid; } else { if (arr[mid - 1] &lt; arr[mid]) right = mid - 1; else left = mid + 1; } } return arr[left] &lt; arr[right] ? left : right; } 最后晚安","categories":[{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"}]},{"title":"MYSQL-锁","slug":"MYSQL-锁","date":"2022-11-03T06:56:19.000Z","updated":"2023-07-08T07:57:00.000Z","comments":true,"path":"posts/7.html","link":"","permalink":"http://example.com/posts/7.html","excerpt":"","text":"s 和随便写点，反正没人会看高达屹立在大地之上","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"}]},{"title":"悉达多读书笔记","slug":"悉达多","date":"2022-11-02T14:22:22.000Z","updated":"2022-11-02T15:39:06.000Z","comments":true,"path":"posts/5.html","link":"","permalink":"http://example.com/posts/5.html","excerpt":"","text":"悉达多 —赫尔曼·黑塞 作者黑塞一生过着漂泊、孤独、隐逸的生活，被称为德国浪漫派最后一个骑士，在1946年获得诺贝尔文学奖。 他的作品中每个都探索了个人对现实，自我和理想的方式。 悉达多（Siddhartha）这个词由梵语中的两个词组成，siddha (实现) + artha(被搜索的)，这在一起意味着 “找到了意义的人” 或者 “达到目标的人”。事实上，佛陀自己的名字，在他放弃之前，是悉达多·乔达玛。在这本书中，佛陀被称为 “Gotama” 第一部—你的灵魂即是整个世界故事发生在古代印度。尽管吸收了父亲的所有教义，并遵循了种姓的所有宗教仪式，悉达多并不满足，决定离开家，希望获得精神照明通过成为一个苦行僧流浪乞丐。有他最好的朋友加入乔文达，悉达多斋戒，通过受苦，战胜饥饿和疲惫，走向克己，将自己融入到陌生的万物，在月光下又回归的“我” ，也是从此时开始，悉达多开始选择不在逃避“我”，认定了此次求学是必要的。最终寻求并亲自与著名的佛陀交谈。之后，悉达多和乔文达都承认佛陀教义的优雅。尽管乔文达匆忙加入了佛陀的门下，但悉达多没有遵循，声称佛陀的哲学尽管是明智的，但并没有说明每个人都是必然不同的经历。他认为，个人寻求一种绝对独特的个人意义，而这种意义是佛陀无法呈现给他的。因此，他更坚实决心独自继续去探寻新的奥义 文章摘要 惟愿我也有这般目光及微笑，能如此行走及端坐。如此自由 神圣 隐晦，又如此坦率。如孩童，又饱含秘密。只&gt; 有潜入自己最深处的人才能有这般诚挚的目光和步伐。无疑，我也将潜入自己最深处探索（悉达多和佛陀第一次交谈后思想的一次成熟） 意义和本质绝非隐藏在事物背后，它就在事物当中，在一切事物当中。 如果一个人要在一本书中探寻意义，他便会逐字逐句去阅读它，研习它，爱它；他不会忽略每一个词，每一个 字，把它们看作表象，看作偶然和毫无关系的皮毛。可我呢，有意研读世界之书。自我存在之书的人，却预先爱上了一个臆想的意义。我忽视书中词语。我把现实世界看作虚无。我视眼目所见，唇齿所尝的仅为没有价值而表面的偶然之物。不，这些都已经过去。我已苏生。 （悉达多在自我中的探索完成了第一次苏醒，虽然本鱼是无神论者，也会被他的这番话所震撼，此刻我已苏醒！） 感悟不知你是否曾经面对着如奇异秀峰般叹为观止的景色，绝望，孤独的强风袭击了你的脸。你试图保持稳定，闭上眼睛然后脸上露出不由自主的微笑？这就是《悉达多》的魅力 晚安。","categories":[{"name":"悉达多","slug":"悉达多","permalink":"http://example.com/categories/%E6%82%89%E8%BE%BE%E5%A4%9A/"}],"tags":[{"name":"课外阅读","slug":"课外阅读","permalink":"http://example.com/tags/%E8%AF%BE%E5%A4%96%E9%98%85%E8%AF%BB/"}]},{"title":"javaee 课程笔记","slug":"javaee-课程笔记","date":"2022-10-31T14:22:22.000Z","updated":"2022-11-03T06:51:18.000Z","comments":true,"path":"posts/4.html","link":"","permalink":"http://example.com/posts/4.html","excerpt":"","text":"Listener编程基本概念概念：监听javaee Web 中的主要容器对对象的变化 容器的对象分为 容器自身的生命周期变化：创建，销毁 容器对象的数据变化：增，删，改数据 使用场合： 监听servlet的启动和停止 监听用户登录和注销 监听客户点击（请求） Servlet API（JavaEE 中容器类型）都采用 一站点一对象 1，ServletContext（Application对象）application的生命 周期： 创建：Web站点启动时创建 销毁：站点停止 servlet的获取： ServletContext application = request.getServletContext(); //多请求共享一个 ServletContext application = session.getServletContext(); //多请求多个 JSP获取 ：内置application对象，不用定义 &lt;% application.setAttribute(\"count\",0)%&gt; 2，Session对象作用：由于存储单个用户信息，每个客户端对应一个session对象 session的生命周期： ​ 1. 创建 ：客户端浏览器首次访问jsp时 首次访问servlet 要使用 HttpSession session = request.getSession(); ​ 2. 销毁： ​ ①主动销毁 ：session.invalidate() ​ ②被动销魂 情况一 ：访问超时 ​ ③被动销毁 情况二 ：停服务器 Servlet获取： HttpSession session = request.getSession(); String userid = (String)session.getAttribute(\"userid\"); session.removeAttribute(\"userid\"); //删除键名为userid的会话信息 jsp获取： &lt;% session.setAttribute(\"存储名\",\"值\")%&gt; 注：存储名必须为字符串；值为object类型 3，request 对象响应完销毁 作用：由于存储Servlet （Controller），给jsp（view）传递数据 request生命周期： 创建 ：每次请求Web组件（JSP ，Servlet）时，server自动创建请求对象 销毁 ：当server发响应给客户端，结束请求时，request对象被销毁 request 获取： Servlet请求对象：doGet，doPost自动接收请求对象，由servlet创建，并注入doGet ，doPost方法 JSP获取： 内置request 监听器类型 行号 监听器类型 监听器接口 监听事件APT 1 servletContext生命周期监听器 ServletContextListener ServletContextEvent 2 servletContext数据变化监听器 ServletContextAttributeListener ServletContexAttributeEvent 3 session对象生命周期监听器 HttpSessionListener HttpSessionEvent 4 session数据变化监听器 HttpSessionAttributeListener ==HttpSessionBindingEvent== 5 request对象生命周期监听器 ServletRequestListener ServletRequestEvent 6 request数据变化监听器 ServletRequestAttribute ServletRequestAttributeEvent Filter过滤器一. 核心功能 对指定请求地址的登录检查 对指定的请求地址进行权限检查 对请求数据进行汉字转码 对所有响应发送响应头，如跨域访问允许响应头，用于REST API服务。 过滤请求数据。 过滤响应数据。 filter采用AOP的横向切面机制，将分散在各个方法中的重复代码提取出来，在程序需要编译或运行时再将抽取的代码放到合适的位置。 二. Filter API（接口 类） public void init (FilterConfig config){} public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain){} public void destory () {} 多个filter构成一条链，由 filterChain决定是否放过到下一个 过滤器链的执行顺序：根据类名的排名先后 三. 过滤器配置 @WebFilter( urlPatterns = {“受保护文件的相对路径”}, initParams = {@WebInitParam(name = “名”,value = “值”)}) 实验题一，对象获取编程 要求：1，类LoginProcessController （1） 取得ID和密码 （2） 如果ID或密码为空，重定向到login.jsp页面。 （3） ID写入session. （4） 使用ServletContext对象存储所有登录用户ID列表。 (5) 转发到主页Serlvet，映射地址是：/home.do package com.fish.servler.lesson06; @WebServlet( urlPatterns = { \"/lesson06/login.do\" }, ) public class LoginProcessController extends HttpServlet { private static final long serialVersionUID = 1L; private Connection cn=null; public void init(ServletConfig config) throws ServletException { String driver=config.getInitParameter(\"driver\"); String url=config.getInitParameter(\"url\"); String user=config.getInitParameter(\"user\"); String password=config.getInitParameter(\"password\"); try { Class.forName(driver); cn=DriverManager.getConnection(url, user,password); } catch(Exception e) { e.printStackTrace(); } } public void destroy() { try { cn.close(); } catch(Exception e) { e.printStackTrace(); } } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { request.setCharacterEncoding(\"utf-8\"); response.setContentType(\"text/html;charset=utf-8\"); PrintWriter out = response.getWriter(); String userid= request.getParameter(\"name\"); String pwd = request.getParameter(\"pwd\"); if(userid.isEmpty()){ out.print(\"&lt;script&gt;alert('信息缺失');location.href = 'login.jsp';&lt;/script&gt;\"); }else { String sql = \"select * from hwl_admin where admin_name ='\" + userid + \"'and admin_password = \" + pwd; try { //取得session对象 HttpSession session = request.getSession(); //存贮用户ID session.setAttribute(\"userid\", userid); ServletContext application = request.getServletContext();//取得application对象 ArrayList List = (ArrayList) application.getAttribute(\"list\"); if (List == null) { List = new ArrayList(); application.setAttribute(\"list\", List); } List.add(userid); //重定向到主Servlet out.print(\"&lt;script&gt;location.href = 'main.do';&lt;/script&gt;\"); } catch (Exception e) { throw new RuntimeException(e); } } } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } } 2． 类 HomeMainController （1） 取得保存在session对象中的登录帐号。 （2） 取得ServletContext中保存的在线用户ID列表，并显示。 （3） 显示到注销的超链接。 （4） 点击注销链接到注销处理Servlet。 package com.fish.servler.lesson06; @WebServlet(\"/lesson06/main.do\") public class HomMainController extends HttpServlet { private static final long serialVersionUID = 1L; public void init(ServletConfig config) throws ServletException { // TODO Auto-generated method stub } public void destroy() { // TODO Auto-generated method stub } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\"text/html\"); response.setCharacterEncoding(\"UTF-8\"); PrintWriter out=response.getWriter(); out.print(\"&lt;h1&gt;OA系统主页&lt;/h1&gt;\"); out.println(\"&lt;hr&gt;\"); //取得session会话对象中存贮的用户信息 HttpSession session=request.getSession(); String userid01=(String)session.getAttribute(\"userid\"); out.println(\"&lt;h2&gt;登录账号：\"+userid01+\"&lt;/br&gt;\"); ServletContext application = request.getServletContext();//取得application对象 ArrayList&lt;String&gt;list = (ArrayList&lt;String&gt;)application.getAttribute(\"list\"); Iterator&lt;String&gt;it = list.iterator(); while (it.hasNext()){ out.println(it.next()+\"&lt;br/&gt;\"); } out.println(\"&lt;hr&gt;\"); out.println(\"&lt;a href='logout.do'&gt;注销用户&lt;/a&gt;\"); out.flush(); out.close(); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // TODO Auto-generated method stub doGet(request, response); } } 3．类LogoutController； (1) 从ServletContext中取出登录ID列表，删除当前登录ID，剩余登录ID列表保存回ServletContext中； (2) 销毁会话对象； (3) 重定向到登录页面/login.jsp。 package com.fish.servler.lesson06; @WebServlet(\"/lesson06/logout.do\") public class LogoutController extends HttpServlet { private static final long serialVersionUID = 1L; public void init(ServletConfig config) throws ServletException { } public void destroy() { } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { HttpSession session=request.getSession(); String userid01=(String)session.getAttribute(\"userid\"); ServletContext application = request.getServletContext();//取得application对象 ArrayList List = (ArrayList)application.getAttribute(\"list\"); List.remove(userid01); response.sendRedirect(\"login.jsp\"); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } } 4.login页面 &lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt; &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div style =\"text-align:center\"&gt; &lt;h1&gt;登录用户&lt;/h1&gt; &lt;form action=\"login.do\" method=\"post\"&gt; 账号：&lt;input type=\"text\" name=\"name\"&gt;&lt;/br&gt; 密码：&lt;input type=\"password\" name=\"pwd\"&gt;&lt;/br&gt; &lt;input type=\"submit\" value=\"提交\"&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 二，监听器编程 编写IP地址阻拦过滤器，过滤器映射过滤地址：/*；但不阻拦而是直接放过对/error.jsp的访问. 类loginCheckFilter package com.fish.lesson007; @WebFilter( urlPatterns = {\"/lesson007/main.jsp\"}, initParams = {@WebInitParam(name = \"ip\",value = \"11.11.11.11;192.168.43.71\") }) public class loginCheckFilter implements Filter{ String notAllowIP; @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\"过滤器初始化\"); notAllowIP = filterConfig.getInitParameter(\"ip\"); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\"登录检查过滤器\"); HttpServletRequest request=(HttpServletRequest) servletRequest; HttpServletResponse response=(HttpServletResponse) servletResponse; //获取相对路径 String uri = request.getRequestURI(); if (uri.equals(\"/lesson007/login.jsp\")||uri.equals(\"/lesson007/login.do\")){ filterChain.doFilter(request,response); }else { HttpSession session = request.getSession(); if (session.getAttribute(\"user\")==null) { response.sendRedirect(\"/LESSON05/lesson007/login.jsp\"); //这里一定是绝对路径 }else { String ip=request.getRemoteAddr(); if (notAllowIP.contains(ip)){ response.sendRedirect(\"/LESSON05/lesson007/error.jsp\"); }else { filterChain.doFilter(request, response); } } } } @Override public void destroy() { } } login主页面同上 error.jsp和main.jsp随便写写就好了 最后祝你晚安","categories":[{"name":"javaEE笔记","slug":"javaEE笔记","permalink":"http://example.com/categories/javaEE%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"javaEE笔记","slug":"javaEE笔记","permalink":"http://example.com/tags/javaEE%E7%AC%94%E8%AE%B0/"}]},{"title":"ted 英语笔记（一）","slug":"ted-英语笔记（一）","date":"2022-10-27T14:22:22.000Z","updated":"2022-10-27T15:59:38.000Z","comments":true,"path":"posts/3.html","link":"","permalink":"http://example.com/posts/3.html","excerpt":"","text":"Rishi Sunak Delivers First Speech As U.K. Prime Minister英国新首相就职演讲 ​ 点下面让我们开始一起学习吧~ Good morning.I have just been to Buckingham Palace and accepted His Majesty The King’s invitation to form a government in his name.上午好。我刚刚去了白金汉宫，接受了国王陛下的邀请，以他的名义组建政府。 It is only right to explain why I am standing here as your new Prime Minister. Right now our country is facing a ==profound== economic crisis.有必要解释一下我为什么站在这里担任你们的新首相。现在，我们的国家正面临着一场深刻的经济危机 The aftermath of Covid still lingers. Putin’s war in Ukraine has destabilised energy markets and supply chains the world over.新冠疫情影响仍然挥之不去。普京在乌克兰的战争破坏了全世界的能源市场和供应链的稳定。 I want to ==pay tribute to== my predecessor Liz Truss she was not wrong to want to improve growth in this country it is a noble aim.我想向我的前任利兹-特拉斯表示敬意，她想改善这个国家的增长，这并没有错，这是一个崇高的目标。 And I admired her ==restlessness== to create change. But some mistakes were made. Not born of ill will or bad intentions.我很钦佩她为创造变革所做的不懈努力。但也犯了一些错误，虽并非出于恶意或坏心。 Quite the opposite, in fact. But mistakes nonetheless. And I have been elected as leader of my party, and your Prime Minister, in part, to fix them.事实上，恰恰相反。但是，还是犯了错误。我被选为我的政党领袖，以及你们的首相，部分原因是为了纠正这些错误。 And that work begins immediately. I will place economic stability and confidence ==at the heart of== this government’s agenda.这项工作立即开始。我将把经济稳定和信心作为本届政府议程的核心 This will mean difficult decisions to come. But you saw me ==during Covid==, doing everything I could, to protect people and businesses, with schemes like furlough.这将意味着要做出艰难的决定。但是，你们看到我在疫情期间，尽我所能，通过休假等计划来保护人民和企业。 There are always limits, more so now than ever, but I promise you this I will bring that same compassion to the challenges we face today.总有一些限制，现在比以往任何时候都更多，但我向你们保证，我将以同样的同情心对待我们今天面临的挑战。 The government I lead will not leave the next generation your children and grandchildren with a debt to settle that we were too weak to pay ourselves.我所领导的政府不会给你们的子孙后代留下我们自己都无力偿还的债务。 I will unite our country, not with words, but with action. I will work day in and day out to deliver for you. This government will have ==integrity, professionalism and accountability== at every level.我将团结我们的国家，不是用语言，而是用行动。我将日复一日地工作，为你们提供服务。这个政府将在每个层面都有诚信、专业性和问责制。 Trust is earned. And I will earn yours. I will always be grateful to Boris Johnson for his incredible achievements as Prime Minister, and I treasure his ==warmth== and ==generosity of spirit==.信任是可以赢得的。而我将赢得你们的信任。我将永远感谢鲍里斯-约翰逊作为首相所取得的令人难以置信的成就，我珍惜他的热情和慷慨的精神。 And I know he would agree that the mandate my party earned in 2019 is not the sole property of any one individual it is a ==mandate== that belongs to and unites all of us.我知道他也会同意，我的政党在2019年赢得的授权不是任何一个人的专属财产，它是属于我们所有人的授权，并将我们所有人团结起来。 And the heart of that mandate is our ==manifesto==. I will deliver on its promise. A stronger NHS. Better schools. Safer streets. Control of our borders.而这一授权的核心是我们的宣言。我将兑现其承诺。一个更强大的国家卫生系统。更好的学校。更安全的街道。控制我们的边界。 Protecting our environment. Supporting our armed forces. Levelling up and building an economy that ==embraces the opportunities of== Brexit, where businesses invest, innovate, and create jobs.保护我们的环境。支持我们的武装力量。提高水平，建立一个拥抱英国脱欧机遇的经济，企业投资、创新和创造就业有了机会。 I know the high office I have accepted and I hope to live up to its demands. But when the opportunity to serve comes along, you cannot question the moment, only your willingness.我知道我所接受的高职位，我希望不辜负它的要求。但是，当服务的机会出现时，你不能质疑这个时刻，只能质疑你的意愿。 So I stand here before you ready to lead our country into the future. To put your needs above politics. To ==reach out== and build a government that represents the very best traditions of my party.因此，我站在你们面前，准备带领我们的国家走向未来。将你们的需求置于政治之上。伸出援手，建立一个代表我党最佳传统的政府。 Together we can achieve incredible things. We will create a future worthy of the sacrifices so many have made and fill tomorrow, and everyday thereafter with hope. Thank you！我们一起可以实现不可思议的事情。我们将创造一个无愧于许多人所做出的牺牲的未来，让明天和以后的每一天都充满希望。谢谢！ 小结： 单词/词组 含义 pay tribute to sb/sth 称赞… a noble aim 一个崇高的目标 restless 不安的 at the heart of agenda 作为事程的核心 integrity 团结性 accountability 责任心 property 财产 mandate 授权 reach out 伸出援手 quit the opposite in the fact that… ​ 事实上恰恰相反… treasure his warmth and generosity of spirit ​ 珍惜他的热情和慷慨的精神 the dislocation that caused in the midst of a war. ​ 在战争中造成的混乱(宾语从句 看着很帅就先记下了) ​ 自己写文章的话可以改成 👇 the different experience that activated in the midst of the reading. ​ 在阅读中激活不同的人生经历 when the opportunity to serve comes along, you cannot question the moment, only your willingness.当机会来临时，你不能质疑这个时刻，只能质疑你的意愿 （太爱了 ，机会和努力一样重要，父亲常说的一句话 ，只有努力做下去了才可能看见胜利的旗帜，不努力是永远不会取得成功。趁着还年轻，想做什么就去做吧 ） 最后 祝您晚安","categories":[{"name":"外语","slug":"外语","permalink":"http://example.com/categories/%E5%A4%96%E8%AF%AD/"}],"tags":[{"name":"英语阅读 ted","slug":"英语阅读-ted","permalink":"http://example.com/tags/%E8%8B%B1%E8%AF%AD%E9%98%85%E8%AF%BB-ted/"}]},{"title":"MYSQL 事务","slug":"MYSQL-事务","date":"2022-10-26T01:07:41.000Z","updated":"2022-11-03T06:57:46.000Z","comments":true,"path":"posts/2.html","link":"","permalink":"http://example.com/posts/2.html","excerpt":"","text":"小鱼的Mysql的自学笔记（事务篇 –隔离性）什么是事务：数据库事务（database transaction 简称 事务）是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列组成。 关于为什么会用到事务：最简单的例子是 小鱼同学去银行取200元，一定会经历的是 ​ ·第一步需要先检查卡的余额里是否有200元 ​ ·第二步需要从卡里扣除200元 ​ ·第三步银行系统里新增200元收入 以上三步必须打包在一个事务中，没有执行完这三步，任何一方都会得到相应的损失。理解了事务的用途我们接下来继续学习吧 事务的ACID特性提起事务就不得不得不说其ACID特性，太重要了！ 原子性：事务的所有操作在数据库中要么做要么什么都不做 一致性：一个事务独立执行的结果 会保持数据的一致性，即数据不会因事务执行而受到破坏。用前面的例子来说，小鱼突然有其他事要忙，暂时不想取钱，决定退出当前正在加载的页面，此整个事务没有执行完，有了 一致性的特性小鱼退出后卡里的余额和银行的余额都将 不会发生改变 隔离性：一条事务执行不能被其他事务干扰。并发事务在执行的过程中可能会对同一数据经行操作，有了隔离性 这些事务的操作将不受影响 持久性：事务一旦提交，它对数据库的改变是永久的 ，即使系统出现故障也是如此 解决的问题：·脏读：读其他提交但还未提交的事务 ·不可重复读：同一行，两次读到的数据 不同（发生在update和delete中） ·幻读：两次执行同一个sql语句，得到的结果集不同（发生在insert之后） 隔离级别（InnoDB）看名字就猜到了肯定是针对隔离性来定义的 ·读未提交 READ UNCOMMITTED：直接返回最新值，无视图的概念 ·读提交 READ COMMITTED（RC）:解决脏读问题。在sql语句开始执行时创建 ·可重复读 REPEATABLE READ（RR）：解决不可重复读问题，以及大部分幻读。视图是在事务启动时创建。innoDB的默认级别 ·串行化 SERIALIZABLE：同时只能运行一个事务，完全解决了幻读。实现原理 ：加锁 还需要注意的是，隔离级被越高，解决的问题越多，并行能力越差 事务的启动方式1，begin 或start transaction 提交 commit ​ 回滚 rollback 2， 线程自动提交关闭，直至 $\\textcolor{Orange}{主动} $ 执行commit 或rollback 或者断开连接 set autocommit=0 建议使用 set autocommit=1 commit work and chain //提交当前事务 自动启动下一事务 3，相关代码 session 只对当前会话有效 global 对非当前会话的新会话有效 相当于全局声明 select @@global.tx_isolation; //查看系统隔离级别 select @@session.tx_isolation; //查看当前会话隔离级别 适用5.0以上 select @@session.transaction_isolation; //查看会话隔离级别8.0以上 set session transaction isolation level repeatable read; 或 set session.transaction_isolation=\"REPEATABLE-READ\"; //修改同一用户的account值 select @@session.transaction_isolation; //窗口1 start transaction; select *from user; update user set account=200 where user_name=\"fish\"; COMMIT select @@session.transaction_isolation; //窗口2 start transaction; select *from user; update user set account=200 where user_id=1; COMMIT 若执行完后没有更新 则可能为产生了死锁 原因是窗口1中未建立索引，直接将表锁住 第一步解锁： SHOW PROCESSLIST //查看数据库中的表是否被锁； kill id 或 UNLOCK TABLES; 在为窗体1添加索引 select @@session.transaction_isolation; create index index_username on user(user_name); create index index_userid on user(user_id); //窗口1 start transaction; select *from user; update user set account=200 where user_name=\"fish\"; COMMIT 事务隔离的实现多版本并发控制（MVCC），是通过保存数据在某个时间点的视图（就是快照）实现的。每条记录在更新时都会同时记录一条回滚记录，同一条记录在系统中可能 存在多个版本，这就是多版本并发控制 事务在启动时会拍一个快照，这个快照是针对整个库的，如果要修改库中某个表或添加索引 是无法实现的。要恢复数据库是最先要求删除快照。 快照读与当前读 快照读：读一个快读（某时间点的视图）的数据，也称为一致性读 当前读：读最新提交的消息，基于锁的并发读操作 ​ select …for update ​ select …lock in share mode ​ insert,update,delete都会按最新提交的数据操作 关于为什么RR可以实现可重复读而RC不行： ①在快照读的基础下，RR不能更新事务内的up-limit-id(事务启动后已提交的最大事务ID) 而RC每次会把up-limit-id更新为快读之前更新已提交的事务的transaction id（每个事务都有的ID ，严格递增），所以RC不能实现可重复读。 ②当前读的情况下，RR利用的是record lock+gap lock（记录锁和间隙锁） 来实现。 ​ RC没有gap 所以RC不能实现可重复读 innoDB支持READ COMMITTED(RC) 和RR隔离级别的实现的是用的是一致性视图（consistent read view） 在SERIALIZABLE隔离级别下 select变成了当前读，即加共享读锁 幻读产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。 为了解决幻读问题，InnoDB只好引入间隙锁(Gap Lock)同时配合行锁（next-key lock）使用。 现在有个问题是：任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据， 代码如下： begin; select * from t where id=N for update; /*如果行不存在*/ insert into t values(N,N,N); /*如果行存在*/ update t set d=N set id=N; commit; 如图模拟，两session的间隙锁造成了死锁。innoDB会让sessionA的insert语句报错返回。 存在有多个唯一键的时候，insert … on duplicate key update不能适用。可使用读提交隔离级别加binlog_format=row组合 所以间隙锁的引入，可能会导致同样的语句锁住更大的范围，会影响并发度 在解决可能出现的数据和日志不一致问题，需要把用 inlog_format=row 思考题出自丁奇老师的《数据库45讲》 问： 查询语句在mydb库的user表中持续时间超过60秒的事务 select * from mydb.user where TIME_TO_SEC(timediff(now(),user_started))&gt;60; 问：事务隔离如何实现 读未提交，性能最好，没有加锁，实际一般很少使用。 读提交和和可重复读 ，比较复杂 在视图的基础上实现并行 串行化 在读的时候加共享锁，其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发读写哦 问：回滚日志何时删除 当系统没有事务会用到回滚日志的时候，回滚日志会被删除 问：什么时候不需要 当系统无比这回滚日志更早的 read-view时 问：为什么尽量不适用长事务 长事务意味着系统存在很老的事务视图，在事务提交前，回滚日志都要保留，会占用大量存储空间。此外长事务还会占用锁资源，可能会拖垮数据库 问：如何避免长事务 在开发时尽量减少长事务，如果无法避免，保证逻辑日志空间足够，并支持日志空间动态增长，监控长事件所用到的表，发现长事务报警 写到后面：今天是第一次正式发文如有错误还望指出，内容不多但小鱼会继续努力，有问题大家可以点我主页或者加我qq一起来聊天呀。期待我们下一次再会","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"事务","slug":"事务","permalink":"http://example.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"MY First Blog","slug":"MY-First-Blog","date":"2022-10-25T14:27:37.000Z","updated":"2022-10-25T14:42:42.000Z","comments":true,"path":"posts/1.html","link":"","permalink":"http://example.com/posts/1.html","excerpt":"","text":"欢迎你的到来 在这里一起沉浸知识的海洋吧 关于为什么建博客就是玩，会编程还会管理自己的网页，拜托超级酷的哎 展望博客及规划在这里我会整理一些个人所学的知识（目前刚接触后端，有一定的数据库基础）希望在这里能和大家一起学习 关于博主大家可以叫我鱼，百分比不是条咸鱼（真的）。喜欢沉浸在知识的海洋，也会在这里为大家分享我喜欢的书和音乐。let’s enjoy it~ 《幸福一日致秋天的花楸树》 ​ 我无限的热爱着新的一日 ​ 今天的太阳今天的马今天的花楸树 ​ 使我健康富足拥有一生 ​ 从黎明到黄昏 ​ 阳光充足 ​ 胜过一切过去的诗 ​ 幸福找到我 ​ 幸福说：“瞧这个诗人 ​ 他比我本人还要幸福” ​ 在劈开了我的秋天 ​ 在劈开了我的骨头的秋天 ​ 我爱你，花楸树 ​ 我爱你，海那边的友人 溜溜球啦！","categories":[{"name":"生活","slug":"生活","permalink":"http://example.com/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"闲谈","slug":"闲谈","permalink":"http://example.com/tags/%E9%97%B2%E8%B0%88/"}]}],"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"},{"name":"redis","slug":"redis","permalink":"http://example.com/categories/redis/"},{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"spring框架","slug":"spring框架","permalink":"http://example.com/categories/spring%E6%A1%86%E6%9E%B6/"},{"name":"spring","slug":"spring","permalink":"http://example.com/categories/spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"http://example.com/categories/Mybatis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JVM(Java虚拟机)","slug":"JVM-Java虚拟机","permalink":"http://example.com/categories/JVM-Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"},{"name":"悉达多","slug":"悉达多","permalink":"http://example.com/categories/%E6%82%89%E8%BE%BE%E5%A4%9A/"},{"name":"javaEE笔记","slug":"javaEE笔记","permalink":"http://example.com/categories/javaEE%E7%AC%94%E8%AE%B0/"},{"name":"外语","slug":"外语","permalink":"http://example.com/categories/%E5%A4%96%E8%AF%AD/"},{"name":"生活","slug":"生活","permalink":"http://example.com/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"springCloud","slug":"springCloud","permalink":"http://example.com/tags/springCloud/"},{"name":"spring springboot","slug":"spring-springboot","permalink":"http://example.com/tags/spring-springboot/"},{"name":"Mybatis","slug":"Mybatis","permalink":"http://example.com/tags/Mybatis/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JVM java","slug":"JVM-java","permalink":"http://example.com/tags/JVM-java/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"java 并发","slug":"java-并发","permalink":"http://example.com/tags/java-%E5%B9%B6%E5%8F%91/"},{"name":"算法 java","slug":"算法-java","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95-java/"},{"name":"flutter","slug":"flutter","permalink":"http://example.com/tags/flutter/"},{"name":"算法and数据结构","slug":"算法and数据结构","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95and%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"},{"name":"课外阅读","slug":"课外阅读","permalink":"http://example.com/tags/%E8%AF%BE%E5%A4%96%E9%98%85%E8%AF%BB/"},{"name":"javaEE笔记","slug":"javaEE笔记","permalink":"http://example.com/tags/javaEE%E7%AC%94%E8%AE%B0/"},{"name":"英语阅读 ted","slug":"英语阅读-ted","permalink":"http://example.com/tags/%E8%8B%B1%E8%AF%AD%E9%98%85%E8%AF%BB-ted/"},{"name":"事务","slug":"事务","permalink":"http://example.com/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"闲谈","slug":"闲谈","permalink":"http://example.com/tags/%E9%97%B2%E8%B0%88/"}]}